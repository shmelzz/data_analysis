{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# HSE 2021: Mathematical Methods for Data Analysis\n",
    "\n",
    "## Homework 4\n",
    "\n",
    "**Warning 1**: You have 2 weeks for this assignemnt.  **it is better to start early (!)**\n",
    "\n",
    "**Warning 2**: it is critical to describe and explain what you are doing and why, use markdown cells\n",
    "\n",
    "\n",
    "### Contents\n",
    "\n",
    "#### Decision Trees - 7 points\n",
    "* [Task 1](#task1) (0.5 points)\n",
    "* [Task 2](#task2) (0.5 points)\n",
    "* [Task 3](#task3) (2 points)\n",
    "* [Task 4](#task4) (0.5 points)\n",
    "* [Task 5](#task5) (0.5 points)\n",
    "* [Task 6](#task6) (2 points)\n",
    "* [Task 7](#task7) (0.5 points)\n",
    "* [Task 8](#task8) (0.5 points)\n",
    "\n",
    "#### Ensembles - 3 points\n",
    "* [Task 1](#task2_1) (1 point)\n",
    "* [Task 2](#task2_2) (0.7 points)\n",
    "* [Task 3](#task2_3) (0.5 points)\n",
    "* [Task 4](#task2_4) (0.7 points)\n",
    "* [Task 5](#task2_5) (0.1 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (11, 5)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Part 1. Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this task you will be implementing decision tree for the regression by hand. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 1 <a id=\"task1\"></a> (0.5 points)\n",
    "\n",
    "Here you should implement the function `H()` which calculates impurity criterion. We will be training regression tree, and will take mean absolute deviation as impurity criterion.\n",
    "\n",
    "* You cannot use loops\n",
    "* If `y` is empty, the function should return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def H(y):\n",
    "    \"\"\"\n",
    "    Calculate impurity criterion\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : np.array\n",
    "        array of objects target values in the node\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    H(R) : float\n",
    "        Impurity in the node (measured by variance)\n",
    "    \"\"\"\n",
    "    if np.size(y) == 0:\n",
    "        return 0.0\n",
    "    return np.sum(np.abs(y - np.mean(y))) / np.size(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Test the function\n",
    "assert np.allclose(H(np.array([4, 2, 2, 2])), 0.75)\n",
    "assert np.allclose(H(np.array([])), 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 2 <a id=\"task2\"></a>  (0.5 points)\n",
    "\n",
    "To find the best split in the node we need to calculate the cost function. Denote: \n",
    "- `R` all the object in the node\n",
    "- `j` index of the feature selected for the split\n",
    "- `t` threshold\n",
    "- `R_l` and `R_r` objects in the left and right child nodes correspondingly\n",
    "\n",
    "We get the following cost function:\n",
    "\n",
    "$$\n",
    "Q(R, j, t) =\\frac{|R_\\ell|}{|R|}H(R_\\ell) + \\frac{|R_r|}{|R|}H(R_r) \\to \\min_{j, t},\n",
    "$$\n",
    "\n",
    "Implement the function `Q`, which should calculate value of the cost function for a given feature and threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def Q(X, y, j, t):\n",
    "    \"\"\"\n",
    "    Calculate cost function\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray\n",
    "        array of objects in the node \n",
    "    y : ndarray\n",
    "        array of target values in the node \n",
    "    j : int\n",
    "        feature index (column in X)\n",
    "    t : float\n",
    "        threshold\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Q : float\n",
    "        Value of the cost function\n",
    "    \"\"\"\n",
    "\n",
    "    x_column = X[:, j]\n",
    "\n",
    "    y_right = y[x_column > t]\n",
    "    y_left = y[x_column <= t]\n",
    "\n",
    "    impurity_left = H(y_left)\n",
    "    impurity_right = H(y_right)\n",
    "\n",
    "    return impurity_left * len(y_left) / len(y) + impurity_right * len(y_right) / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 3 <a id=\"task3\"></a>  (2 points)\n",
    "\n",
    "Now, let's implement `MyDecisionTreeRegressor` class. More specifically, you need to implement the following methods:\n",
    "\n",
    "- `best_split`\n",
    "- `grow_tree`\n",
    "- `get_prediction`\n",
    "\n",
    "Also, please add `min_samples_leaf` parameter to your class\n",
    "\n",
    "Read docstrings for more details. Do not forget to use function `Q` implemented above, when finding the `best_split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    \"\"\"\n",
    "    Class for a decision tree node.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    right : Node() or None\n",
    "        Right child\n",
    "    right : Node() or None\n",
    "        Left child\n",
    "    threshold: float\n",
    "        \n",
    "    column: int\n",
    "        \n",
    "    depth: int\n",
    "        \n",
    "    prediction: float\n",
    "        prediction of the target value in the node \n",
    "        (average values calculated on a train dataset)\n",
    "    is_terminal:bool\n",
    "        indicates whether it is a terminal node (leaf) or not\n",
    "    \"\"\"    \n",
    "    def __init__(self):        \n",
    "        self.right = None\n",
    "        self.left = None\n",
    "        self.threshold = None\n",
    "        self.column = None\n",
    "        self.depth = None\n",
    "        self.is_terminal = False\n",
    "        self.prediction = None\n",
    "        \n",
    "    def __repr__(self):\n",
    "        if self.is_terminal:\n",
    "            node_desc = 'Pred: {:.2f}'.format(self.prediction)\n",
    "        else:\n",
    "            node_desc = 'Col {}, t {:.2f}, Pred: {:.2f}'. \\\n",
    "            format(self.column, self.threshold, self.prediction)\n",
    "        return node_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "\n",
    "\n",
    "class MyDecisionTreeRegressor(RegressorMixin, BaseEstimator):\n",
    "    \"\"\"\n",
    "    Class for a Decision Tree Regressor.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    max_depth : int\n",
    "        Max depth of a decision tree.\n",
    "    min_samples_split : int\n",
    "        Minimal number of samples (objects) in a node to make a split.\n",
    "    min_samples_leaf : int\n",
    "        Minimal number of samples (objects) in a leaf (terminal node).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_depth=3, min_samples_split=2, min_samples_leaf=1):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "\n",
    "    # чтобы не было warning\n",
    "    def _more_tags(self):\n",
    "        return {\n",
    "            'requires_y': False\n",
    "        }\n",
    "\n",
    "    def best_split(self, X, y):\n",
    "        \"\"\"\n",
    "        Find the best split in terms of Q of data in a given decision tree node. \n",
    "        Try all features and thresholds. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_objects, n_features)\n",
    "            Objects in the parent node\n",
    "        y : ndarray, shape (n_objects, )\n",
    "            1D array with the object labels. \n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        best_split_column : int\n",
    "            Index of the best split column\n",
    "        best_threshold : float\n",
    "            The best split condition.\n",
    "        X_left : ndarray, shape (n_objects_l, n_features)\n",
    "            Objects in the left child\n",
    "        y_left : ndarray, shape (n_objects_l, )\n",
    "            Objects labels in the left child. \n",
    "        X_right : ndarray, shape (n_objects_r, n_features)\n",
    "            Objects in the right child\n",
    "        y_right : ndarray, shape (n_objects_r, )\n",
    "            Objects labels in the right child. \n",
    "        \"\"\"\n",
    "\n",
    "        best_split_column = None\n",
    "        best_threshold = None\n",
    "        best_cost = H(y)\n",
    "\n",
    "        # ищем столбец с наименьшим значением loss функции\n",
    "        for split_column in range(X.shape[1]):\n",
    "            x_col = X[:, split_column]\n",
    "            for i_x in range(0, len(x_col)):\n",
    "                threshold = x_col[i_x]\n",
    "\n",
    "                cost = Q(X, y, split_column, threshold)\n",
    "\n",
    "                if cost < best_cost:\n",
    "                    best_split_column = split_column\n",
    "                    best_threshold = threshold\n",
    "                    best_cost = cost\n",
    "\n",
    "        # если разделить не получится\n",
    "        if best_cost == H(y):\n",
    "            return None, None, None, None, None, None\n",
    "\n",
    "        x_column = X[:, best_split_column]\n",
    "        # делим на правого и левого детей\n",
    "        x_left = X[x_column <= best_threshold, :]\n",
    "        y_left = y[x_column <= best_threshold]\n",
    "        x_right = X[x_column > best_threshold, :]\n",
    "        y_right = y[x_column > best_threshold]\n",
    "\n",
    "        return best_split_column, best_threshold, x_left, y_left, x_right, y_right\n",
    "\n",
    "    def is_terminal(self, node, y):\n",
    "        \"\"\"\n",
    "        Check terminality conditions based on `max_depth`, \n",
    "        `min_samples_split` parameters for a given node.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        node : Node, \n",
    "            \n",
    "        y : ndarray, shape (n_objects, )\n",
    "            Object labels. \n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        Is_termial : bool\n",
    "            If True, node is terminal\n",
    "        \"\"\"\n",
    "        if node.depth >= self.max_depth:\n",
    "            return True\n",
    "        if len(y) < self.min_samples_split:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def grow_tree(self, node, X, y):\n",
    "        \"\"\"\n",
    "        Reccurently grow the tree from the `node` using a `X` and `y` as a dataset:\n",
    "         - check terminality conditions\n",
    "         - find best split if node is not terminal\n",
    "         - add child nodes to the node\n",
    "         - call the function recursively for the added child nodes\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        node : Node() object\n",
    "            Current node of the decision tree.\n",
    "        X : ndarray, shape (n_objects, n_features)\n",
    "            Objects \n",
    "        y : ndarray, shape (n_objects)\n",
    "            Labels\n",
    "        \"\"\"\n",
    "\n",
    "        # проверки взяты с семинара\n",
    "        if self.is_terminal(node, y):\n",
    "            node.is_terminal = True\n",
    "            return\n",
    "\n",
    "        if node.depth >= self.max_depth:\n",
    "            node.is_terminal = True\n",
    "            return\n",
    "        if len(X) < self.min_samples_split:\n",
    "            node.is_terminal = True\n",
    "            return\n",
    "        if len(np.unique(y)) == 1:\n",
    "            node.is_terminal = True\n",
    "            return\n",
    "\n",
    "        split_column, threshold, X_left, y_left, X_right, y_right = self.best_split(X, y)\n",
    "\n",
    "        if split_column is None:\n",
    "            node.is_terminal = True\n",
    "            return\n",
    "        if len(X_left) < self.min_samples_leaf or len(X_right) < self.min_samples_leaf:\n",
    "            node.is_terminal = True\n",
    "            return\n",
    "\n",
    "        node.column = split_column\n",
    "        node.threshold = threshold\n",
    "\n",
    "        node.left = Node()\n",
    "        node.left.depth = node.depth + 1\n",
    "        node.left.prediction = np.mean(y_left)\n",
    "\n",
    "        node.right = Node()\n",
    "        node.right.depth = node.depth + 1\n",
    "        node.right.prediction = np.mean(y_right)\n",
    "\n",
    "        self.grow_tree(node.right, X_right, y_right)\n",
    "        self.grow_tree(node.left, X_left, y_left)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the Decision Tree Regressor.\n",
    "            \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_samples, n_features)\n",
    "            The input samples.\n",
    "        y : ndarray, shape (n_samples,) or (n_samples, n_outputs)\n",
    "            The target values.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "\n",
    "        X, y = check_X_y(X, y, accept_sparse=False)\n",
    "        self.is_fitted_ = True\n",
    "        self.n_features_in_ = X.shape[1]\n",
    "\n",
    "        # Initialize the tree (root node)\n",
    "        self.tree_ = Node()\n",
    "        self.tree_.depth = 0\n",
    "        self.tree_.prediction = np.mean(y)\n",
    "\n",
    "        # Grow the tree\n",
    "        self.grow_tree(self.tree_, X, y)\n",
    "        return self\n",
    "\n",
    "    def get_prediction(self, node, x):\n",
    "        \"\"\"\n",
    "        Get prediction for an object `x`\n",
    "            - Return prediction of the `node` if it is terminal\n",
    "            - Otherwise, recursively call the function to get \n",
    "            predictions of the proper child\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        node : Node() object\n",
    "            Current node of the decision tree.\n",
    "        x : ndarray, shape (n_features,)\n",
    "            Array of feature values of one object.\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : float\n",
    "            Prediction for an object x\n",
    "        \"\"\"\n",
    "\n",
    "        if node.is_terminal:\n",
    "            return node.prediction\n",
    "\n",
    "        if x[node.column] > node.threshold:\n",
    "            return self.get_prediction(node.right, x)\n",
    "        else:\n",
    "            return self.get_prediction(node.left, x)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" \n",
    "        Get prediction for each object in X\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_samples, n_features)\n",
    "            The input samples.\n",
    "        Returns\n",
    "        -------\n",
    "        y : ndarray, shape (n_samples,)\n",
    "            Returns predictions.\n",
    "        \"\"\"\n",
    "        # Check input and that `fit` had been called\n",
    "        X = check_array(X, accept_sparse=False)\n",
    "        check_is_fitted(self, 'is_fitted_')\n",
    "\n",
    "        # Get predictions\n",
    "        y_predicted = []\n",
    "        for x in X:\n",
    "            y_curr = self.get_prediction(self.tree_, x)\n",
    "            y_predicted.append(y_curr)\n",
    "        return np.array(y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# check yourself\n",
    "from sklearn.utils.estimator_checks import check_estimator\n",
    "\n",
    "check_estimator(MyDecisionTreeRegressor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 4 <a id=\"task4\"></a>  (0.5 points)\n",
    "\n",
    "Load boston dataset and split it on the train ($75\\%$) and test ($25\\%$). Fit Decision Tree of depth 1 and make the following plot:\n",
    "\n",
    "- Scatter plot of the traning points (selected for split feature on the x-axis, target variable on the y-axis)\n",
    "- Fitted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shmelzzz/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# loading dataset\n",
    "boston_dataset = load_boston()\n",
    "X_boston = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)\n",
    "y_boston = boston_dataset.target"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_boston, y_boston, train_size=0.75, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tree_regr = MyDecisionTreeRegressor(max_depth=1)\n",
    "tree_regr.fit(x_train, y_train)\n",
    "\n",
    "plt.scatter(x_train.iloc[:,tree_regr.tree_.column], y_train)\n",
    "plt.xlabel('split feature')\n",
    "plt.ylabel('target')\n",
    "\n",
    "plt.axvspan(xmin=np.min(x_train.iloc[:, tree_regr.tree_.column]), xmax=tree_regr.tree_.threshold, facecolor='green',\n",
    "            alpha=0.2)\n",
    "plt.axvspan(xmax=np.max(x_train.iloc[:, tree_regr.tree_.column]), xmin=tree_regr.tree_.threshold, facecolor='red',\n",
    "            alpha=0.2)\n",
    "plt.hlines(y=tree_regr.tree_.left.prediction, xmin=np.min(x_train.iloc[:, tree_regr.tree_.column]),\n",
    "           xmax=tree_regr.tree_.threshold, color='r')\n",
    "plt.hlines(y=tree_regr.tree_.right.prediction, xmax=np.max(x_train.iloc[:, tree_regr.tree_.column]),\n",
    "           xmin=tree_regr.tree_.threshold, color='g')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Так как из-за max_depth=1 был всего лишь один сплит, fitted model - это threshold этого сплита и predictions в каждом из детей. Threshold показан разделением фона графика, а предсказания в каждом ребенке - линии.\n",
    "Таким образом, на зеленом фоне - объекты в левом ребенке и его prediction, на красном фоне - объекты в правом ребенке и его prediction."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 5 <a id=\"task5\"></a>  (0.5 points)\n",
    "\n",
    "Keep working with boston dataset. \n",
    "- Use `GridSearchCV` to find the best hyperparameters among [`max_depth`, `min_samples_leaf`] on 5-Fold cross-validation\n",
    "- Train the model with the best set of hyperparameters on the whole train dataset. \n",
    "- Report `MAE` on test dataset and hyperparameters of the best estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'max_depth': 5, 'min_samples_leaf': 2}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import *\n",
    "\n",
    "tree_regression = MyDecisionTreeRegressor()\n",
    "parameters = {'max_depth': [1, 3, 5, 7, 9, 10],\n",
    "              'min_samples_leaf': [1, 2, 3, 5, 7, 10, 15]}\n",
    "tree = GridSearchCV(tree_regression, parameters, scoring='neg_mean_absolute_error', cv=5)\n",
    "tree.fit(x_train, y_train)\n",
    "print(\"Best parameters: \", tree.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "pred = tree.predict(x_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 2.610396592637237\n"
     ]
    }
   ],
   "source": [
    "print('MAE:', (mean_absolute_error(y_test, pred)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 6 <a id=\"task6\"></a>  (2 points)\n",
    "\n",
    "Recall definition of bias and variance:\n",
    "$$\n",
    "\\text{Bias}^2 = \\mathbb{E}_{p(x, y)} \\left[  (f(x) - \\mathbb{E}_{\\mathbb{X}}a_{\\mathbb{X}}(x))^2 \\right] \\\\\n",
    "\\text{Variance} = \\mathbb{E}_{p(x, y)} \\left[  \\mathbb{V}_{\\mathbb{X}}( a_{\\mathbb{X}}(x))  \\right]\n",
    "$$\n",
    "\n",
    "We wil now use use the following algorithm to estimate bias and variance:\n",
    "\n",
    "1. Use bootsrap to create `n_iter` samples from the original dataset: $X_1, \\dots, X_{n_iter}$\n",
    "2. For each bootstrapped sample define out-of-bag (OOB) sample $Z_1, \\dots, Z_{n_iter}$, which contain all the observations, which did not appear in the corresponding boostraped sample\n",
    "3. Fit the model on $X_i$s and compute predictions on $Z_i$s\n",
    "4. For a given *object* $n$:\n",
    "     - bias^2: squared difference between true value $y_n$ and average prediction (average over the algorithms, for which $n$ was in OOB)\n",
    "     - variance: variance of the prediction (predictions of the algorithms, for which $n$ was in OOB)\n",
    "5. Average bias^2 and variance over all the points\n",
    "    \n",
    "**Implement `get_bias_variance` function, using the algorithm above**\n",
    "\n",
    "*Note:*  You can only use 1 loop (for bootsrap iterations). All other operations should be vectorized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_bias_variance(estimator, x, y, n_iter):\n",
    "    \"\"\" \n",
    "    Calculate bias and variance of the `estimator`. \n",
    "    Using a given dataset and bootstrap with `n_iter` samples. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : ndarray, shape (n_samples, n_features)\n",
    "        The input samples.\n",
    "    y : ndarray, shape (n_samples, n_features)\n",
    "        The input samples.\n",
    "    n_iter: int\n",
    "        Number of samples in \n",
    "    Returns\n",
    "    -------\n",
    "    bias2 : float, \n",
    "        Estiamted squared bias\n",
    "    variance : float, \n",
    "        Estiamted variance\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(42)\n",
    "\n",
    "    # массив индексов объектов в датасете\n",
    "    rows_index = np.arange(start=0, stop=x.shape[0], step=1)\n",
    "    rows_count = x.shape[0]\n",
    "\n",
    "    # создадим пустой массив, в который будут добавлятся предсказания для каждого сэмпла\n",
    "    predictions = np.empty(shape=(n_iter, x.shape[0]))\n",
    "    predictions[:] = np.nan\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        # выбираем рандомные индексы объектов из датасета\n",
    "        random_rows_ind = np.random.choice(rows_index, rows_count, replace=True)\n",
    "        # обпределяем, какие объекты не попали в сэмпл\n",
    "        oob_ind = [ind for ind in rows_index if ind not in random_rows_ind]\n",
    "\n",
    "        estimator.fit(x[random_rows_ind,:],y[random_rows_ind])\n",
    "        oob_y_predict = estimator.predict(x[oob_ind,:])\n",
    "        # добавляем предсказания в массив\n",
    "        predictions[i, oob_ind] = oob_y_predict\n",
    "\n",
    "    bias = np.nanmean(np.square(y - np.nanmean(predictions, axis=0)))\n",
    "    variance = np.nanmean(np.square(np.nanstd(predictions, axis=0)))\n",
    "\n",
    "    return bias, variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(18.777375507281608, 10.192201556239318)"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "estimator = MyDecisionTreeRegressor(max_depth=8, min_samples_split=15)\n",
    "\n",
    "get_bias_variance(estimator, x_train.values, y_train, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 7 <a id=\"task7\"></a>  (0.5 points)\n",
    "\n",
    "Compute bias and variance for the trees with different min_samples_split. Plot how bias and variance change as min_samples_split increases. \n",
    "\n",
    "Comment on what you observe, how does your result correspond to theory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bias = []\n",
    "variance = []\n",
    "\n",
    "min_samples_arr = np.arange(start=1, stop=100, step=5)\n",
    "\n",
    "for i in min_samples_arr:\n",
    "    tree_estimator = MyDecisionTreeRegressor(max_depth = 10, min_samples_split=i)\n",
    "    cur_bias, cur_variance = get_bias_variance(tree_estimator, x_train.values, y_train, 10)\n",
    "    bias.append(cur_bias)\n",
    "    variance.append(cur_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "Text(0.5, 0, 'min_samples_split')"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(min_samples_arr, bias)\n",
    "plt.plot(min_samples_arr, variance)\n",
    "plt.legend(['bias', 'variance'])\n",
    "plt.title('min_samples_split effect on bias and variance')\n",
    "plt.xlabel('min_samples_split')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "В теории большой разброс (variance) значит, что модель переобучена; большое смещение (bias) значит, что модель не дообучилась.\n",
    "На графике видно, что чем больше значение минимального разбиения, тем больше становится смещение. Это происходит из-за того, что большое значение min_samples_split не позволяет дереву сильно расзарстись вглубь (max_depth не будет очень большим при большом значении min_samples_split), что ведет к недообучению модели.\n",
    "Также видно, что при увеличении значения min_samples_split уменьшается разброс. Как было помянуто выше, это происходит из-за того, что глубина дерева не может получится очень большой при большом значении min_samples_split. Следовательно, модель на маленьких значениях min_samples_split переобучается.\n",
    "График соответствует теории."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 8 <a id=\"task8\"></a>  (0.5 points)\n",
    "\n",
    "Let's try to reduce variance with bagging. Use `sklearn.ensemble.BaggingRegressor` to get an ensemble and compute its bias and variance. \n",
    "\n",
    "Answer the following questions:\n",
    " - How bagging should affect bias and variance in theory?\n",
    " - How bias and variance change (if they change) compared to an individual tree in you experiments? \n",
    " - Do your results align with the theory? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias:  15.28504787038091\n",
      "Variance:  1.698725158954505\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "tree_estimator_bagging = BaggingRegressor(base_estimator=\n",
    "                                          MyDecisionTreeRegressor(max_depth=10, min_samples_split=10),\n",
    "                                          n_estimators=40, random_state=42)\n",
    "\n",
    "bagging_bias, bagging_variance = get_bias_variance(tree_estimator_bagging, x_train.values, y_train, 10)\n",
    "print('Bias: ', bagging_bias)\n",
    "print('Variance: ', bagging_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "В теории bagging уменьшает разброс, но при этом никак не влияет на смещение.\n",
    "В сравнении со смещением и разбросом в 6-ом задании разброс значительно уменьшился, в то время как смещение тоже изменилось в меньшую сторону, но не намного.\n",
    "Результаты сходятся с теорией"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Part 2. More Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this part we will be working with [Thyroid Disease Data Set](https://archive.ics.uci.edu/ml/datasets/thyroid+disease) to solve a classification task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "    age sex on_thyroxine query_on_thyroxine on_antithyroid_medication sick  \\\n0  41.0   F            f                  f                         f    f   \n1  23.0   F            f                  f                         f    f   \n2  46.0   M            f                  f                         f    f   \n3  70.0   F            t                  f                         f    f   \n4  70.0   F            f                  f                         f    f   \n\n  pregnant thyroid_surgery I131_treatment query_hypothyroid  ...   T3  \\\n0        f               f              f                 f  ...  2.5   \n1        f               f              f                 f  ...  2.0   \n2        f               f              f                 f  ...  NaN   \n3        f               f              f                 f  ...  1.9   \n4        f               f              f                 f  ...  1.2   \n\n  TT4_measured    TT4 T4U_measured   T4U FTI_measured    FTI  TBG_measured  \\\n0            t  125.0            t  1.14            t  109.0             f   \n1            t  102.0            f   NaN            f    NaN             f   \n2            t  109.0            t  0.91            t  120.0             f   \n3            t  175.0            f   NaN            f    NaN             f   \n4            t   61.0            t  0.87            t   70.0             f   \n\n  TBG  referral_source  \n0 NaN             SVHC  \n1 NaN            other  \n2 NaN            other  \n3 NaN            other  \n4 NaN              SVI  \n\n[5 rows x 29 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>sex</th>\n      <th>on_thyroxine</th>\n      <th>query_on_thyroxine</th>\n      <th>on_antithyroid_medication</th>\n      <th>sick</th>\n      <th>pregnant</th>\n      <th>thyroid_surgery</th>\n      <th>I131_treatment</th>\n      <th>query_hypothyroid</th>\n      <th>...</th>\n      <th>T3</th>\n      <th>TT4_measured</th>\n      <th>TT4</th>\n      <th>T4U_measured</th>\n      <th>T4U</th>\n      <th>FTI_measured</th>\n      <th>FTI</th>\n      <th>TBG_measured</th>\n      <th>TBG</th>\n      <th>referral_source</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>41.0</td>\n      <td>F</td>\n      <td>f</td>\n      <td>f</td>\n      <td>f</td>\n      <td>f</td>\n      <td>f</td>\n      <td>f</td>\n      <td>f</td>\n      <td>f</td>\n      <td>...</td>\n      <td>2.5</td>\n      <td>t</td>\n      <td>125.0</td>\n      <td>t</td>\n      <td>1.14</td>\n      <td>t</td>\n      <td>109.0</td>\n      <td>f</td>\n      <td>NaN</td>\n      <td>SVHC</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>23.0</td>\n      <td>F</td>\n      <td>f</td>\n      <td>f</td>\n      <td>f</td>\n      <td>f</td>\n      <td>f</td>\n      <td>f</td>\n      <td>f</td>\n      <td>f</td>\n      <td>...</td>\n      <td>2.0</td>\n      <td>t</td>\n      <td>102.0</td>\n      <td>f</td>\n      <td>NaN</td>\n      <td>f</td>\n      <td>NaN</td>\n      <td>f</td>\n      <td>NaN</td>\n      <td>other</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>46.0</td>\n      <td>M</td>\n      <td>f</td>\n      <td>f</td>\n      <td>f</td>\n      <td>f</td>\n      <td>f</td>\n      <td>f</td>\n      <td>f</td>\n      <td>f</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>t</td>\n      <td>109.0</td>\n      <td>t</td>\n      <td>0.91</td>\n      <td>t</td>\n      <td>120.0</td>\n      <td>f</td>\n      <td>NaN</td>\n      <td>other</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>70.0</td>\n      <td>F</td>\n      <td>t</td>\n      <td>f</td>\n      <td>f</td>\n      <td>f</td>\n      <td>f</td>\n      <td>f</td>\n      <td>f</td>\n      <td>f</td>\n      <td>...</td>\n      <td>1.9</td>\n      <td>t</td>\n      <td>175.0</td>\n      <td>f</td>\n      <td>NaN</td>\n      <td>f</td>\n      <td>NaN</td>\n      <td>f</td>\n      <td>NaN</td>\n      <td>other</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>70.0</td>\n      <td>F</td>\n      <td>f</td>\n      <td>f</td>\n      <td>f</td>\n      <td>f</td>\n      <td>f</td>\n      <td>f</td>\n      <td>f</td>\n      <td>f</td>\n      <td>...</td>\n      <td>1.2</td>\n      <td>t</td>\n      <td>61.0</td>\n      <td>t</td>\n      <td>0.87</td>\n      <td>t</td>\n      <td>70.0</td>\n      <td>f</td>\n      <td>NaN</td>\n      <td>SVI</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 29 columns</p>\n</div>"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df = pd.read_csv('thyroid_disease.csv')\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df['Class'])\n",
    "X = df.drop('Class', axis=1)\n",
    "X.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 1 <a id=\"task2_1\"></a> (1 point)\n",
    "\n",
    "Let's start with data preprocessing. \n",
    "\n",
    "0. Drop columns, which are not usefull (e.g. a lot of missing values). Motivate your choice. \n",
    "1. Split dataset into train and test\n",
    "2. You've probably noticed that we have both categorical and numerical columns. Here is what you need to do with them:\n",
    "    - Categorical: Fill missing values and apply one-hot-encoding\n",
    "    - Numeric: Fill missing values\n",
    "    \n",
    "Use `ColumnTranformer` to define a single transformer for all the columns in the dataset. It takes as input a list of tuples\n",
    "\n",
    "```\n",
    "ColumnTransformer([\n",
    "    ('name1', transform1, column_names1),\n",
    "    ('name2', transform2, column_names2)\n",
    "])\n",
    "```\n",
    "\n",
    "Pay attention to an argument `remainder='passthrough'`. [Here](https://scikit-learn.org/stable/modules/compose.html#column-transformer) you can find some examples of how to use column transformer. \n",
    "    \n",
    "Since we want to apply 2 transformations to categorical feature, it is very convenient to combine them into a `Pipeline`:\n",
    "\n",
    "```\n",
    "double_tranform = make_pipeline(\n",
    "                        transform_1,\n",
    "                        transform_2\n",
    "                        )\n",
    "```\n",
    "\n",
    "P.S. Choose your favourite way to fill missing values. \n",
    "\n",
    "*Hint* Categorical column usually have `dtype = 'object'`. This may help to obtain list of categorical and numerical columns on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape:  (3772, 29)\n",
      "Missing values count:\n",
      " age                             1\n",
      "sex                           150\n",
      "on_thyroxine                    0\n",
      "query_on_thyroxine              0\n",
      "on_antithyroid_medication       0\n",
      "sick                            0\n",
      "pregnant                        0\n",
      "thyroid_surgery                 0\n",
      "I131_treatment                  0\n",
      "query_hypothyroid               0\n",
      "query_hyperthyroid              0\n",
      "lithium                         0\n",
      "goitre                          0\n",
      "tumor                           0\n",
      "hypopituitary                   0\n",
      "psych                           0\n",
      "TSH_measured                    0\n",
      "TSH                           369\n",
      "T3_measured                     0\n",
      "T3                            769\n",
      "TT4_measured                    0\n",
      "TT4                           231\n",
      "T4U_measured                    0\n",
      "T4U                           387\n",
      "FTI_measured                    0\n",
      "FTI                           385\n",
      "TBG_measured                    0\n",
      "TBG                          3772\n",
      "referral_source                 0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Dataset shape: ', X.shape)\n",
    "print('Missing values count:\\n', pd.isnull(X).sum())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Так как в столбце TBG находятся только пустые значения (их кол-во равно размеру датасета), его можно удалить.\n",
    "Так же больше всего значений пропущено в столбцах T3, T4U, FTI. Но так как кол-во пропущенных значений в этих столбцах не превышает 20% от всего датасета, их мы удалять не будем."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "X = X.drop('TBG', axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train_disease, x_test_disease, y_train_disease, y_test_disease = train_test_split(X, y, train_size=0.75, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "categorical = X.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical = X.select_dtypes(exclude=['object']).columns.tolist()\n",
    "\n",
    "categorical_transform = make_pipeline(SimpleImputer(strategy='most_frequent'), OneHotEncoder())\n",
    "numerical_transform = SimpleImputer(strategy='mean')\n",
    "\n",
    "column_transformer = ColumnTransformer([('numerical', numerical_transform, numerical),\n",
    "                                        ('categorical', categorical_transform, categorical)],\n",
    "                                         remainder='passthrough')\n",
    "\n",
    "x_train_disease = column_transformer.fit_transform(x_train_disease)\n",
    "x_test_disease = column_transformer.transform(x_test_disease)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 2 <a id=\"task2_2\"></a> (0.7 points)\n",
    "\n",
    "Fit and compare 5 different models (use sklearn): Gradient Boosting, Random Forest, Decision Tree, SVM, Logitics Regression\n",
    "    \n",
    "* Choose one classification metric and justify your choice .\n",
    "* Compare the models using score on cross validation. Mind the class balance when choosing the cross validation. (You can read more about different CV strategies [here](https://scikit-learn.org/stable/modules/cross_validation.html#stratified-k-fold))\n",
    "* Which model has the best performance? Which models overfit or underfit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0    3541\n1     231\ndtype: int64"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Классы не сбалансированы. Следовательно, нужно использовать StratifiedKFold, который берет примерно равное кол-во объектов разных классов, то есть подходит в случае, когда классы не сбалансированы."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Метрика F1-score подходит для классификации, даже при условии несбалансированности классов."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting: F1-score = 0.8656\n",
      "Random Forest: F1-score = 0.8425\n",
      "Decision Tree: F1-score = 0.8827\n",
      "SVM: F1-score = 0.0000\n",
      "Logistic Regression: F1-score = 0.6807\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "score_GB = cross_val_score(GradientBoostingClassifier(random_state=42, n_estimators=50), x_train_disease, y_train_disease, cv=skf, scoring='f1')\n",
    "print(\"Gradient Boosting: F1-score = %.4f\" % score_GB.mean())\n",
    "\n",
    "score_RF = cross_val_score(RandomForestClassifier(random_state=42, n_estimators=50), x_train_disease, y_train_disease, cv=skf, scoring='f1')\n",
    "print(\"Random Forest: F1-score = %.4f\" % score_RF.mean())\n",
    "\n",
    "score_DT = cross_val_score(DecisionTreeClassifier(random_state=42, max_depth=10), x_train_disease, y_train_disease, cv=skf, scoring='f1')\n",
    "print(\"Decision Tree: F1-score = %.4f\" % score_DT.mean())\n",
    "\n",
    "score_SVM = cross_val_score(SVC(random_state=42), x_train_disease, y_train_disease, cv=skf, scoring='f1')\n",
    "print(\"SVM: F1-score = %.4f\" % score_SVM.mean())\n",
    "\n",
    "score_LR = cross_val_score(LogisticRegression(random_state=42, max_iter=10000), x_train_disease, y_train_disease, cv=skf, scoring='f1')\n",
    "print(\"Logistic Regression: F1-score = %.4f\" % score_LR.mean())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Лучше всего метрики у Decision tree.\n",
    "* SVM для данной задачи классификации не подходит.\n",
    "* Логистическая регрессия уступает деревьям по качеству.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 3 <a id=\"task2_3\"></a> (0.5 points)\n",
    "\n",
    "More Gradient Boosting. You will have to implement one of the three popular boosting implementations (xgboost, lightgbm, catboost). Select hyperparameters (number of trees, learning rate, depth) on cross-validation and compare with the methods from the previous task. \n",
    "\n",
    "To get method that you have to implement, run cell below and input your name in Russian (for example, if you input Андрей, you will see that user with this name should implement xgboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Реализуйте xgboost\n"
     ]
    }
   ],
   "source": [
    "def assign_method():\n",
    "    name = 'Елизавета'\n",
    "    methods = ['xgboost', 'lightgbm', 'catboost']\n",
    "    idx = sum([ord(x) for x in list(name)]) % 3\n",
    "    print('Реализуйте', methods[idx])\n",
    "    \n",
    "assign_method()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 120 candidates, totalling 600 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n             estimator=XGBClassifier(base_score=None, booster=None,\n                                     callbacks=None, colsample_bylevel=None,\n                                     colsample_bynode=None,\n                                     colsample_bytree=None,\n                                     early_stopping_rounds=None,\n                                     enable_categorical=False, eval_metric=None,\n                                     feature_types=None, gamma=None,\n                                     gpu_id=None, grow_policy=None,\n                                     importance_...\n                                     max_cat_to_onehot=None,\n                                     max_delta_step=None, max_depth=None,\n                                     max_leaves=None, min_child_weight=None,\n                                     missing=nan, monotone_constraints=None,\n                                     n_estimators=100, n_jobs=None,\n                                     num_parallel_tree=None, predictor=None,\n                                     random_state=None, ...),\n             param_grid={'learning_rate': [1.0, 0.1, 0.01],\n                         'max_depth': range(2, 10),\n                         'n_estimators': range(50, 300, 50)},\n             scoring='f1', verbose=True)"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "boosting = XGBClassifier(objective='binary:logistic', seed=42)\n",
    "\n",
    "parameters_boosting = {'max_depth': range(2, 10, 1),\n",
    "                       'n_estimators': range(50, 300, 50),\n",
    "                       'learning_rate': [1.0, 0.1, 0.01]}\n",
    "\n",
    "xgboost = GridSearchCV(boosting, parameters_boosting, scoring='f1', cv=StratifiedKFold(n_splits=5), verbose=True)\n",
    "xgboost.fit(x_train_disease, y_train_disease)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 150}\n",
      "F1 on test: 0.8910891089108911\n",
      "F1 on train: 1.0\n"
     ]
    }
   ],
   "source": [
    "print('Best parameters: ', xgboost.best_params_)\n",
    "print('F1 on test:', f1_score(y_test_disease, xgboost.predict(x_test_disease)))\n",
    "print('F1 on train:', f1_score(y_train_disease, xgboost.predict(x_train_disease)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Качество модели сопоставимо с качеством деревьев из предыдущего задания, f1-score увеличился не намного по сравнению с Decision Tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 4 <a id=\"task2_4\"></a> (0.7 points)\n",
    "\n",
    "Now let's train more fancy ensembles:\n",
    "\n",
    "* Bagging with decision trees as base estimators\n",
    "* Bagging with gradient boosting (with large amount of trees, >100) as base estimators\n",
    "* [Voting classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html#sklearn.ensemble.VotingClassifier) \n",
    "* [Stacking Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html#sklearn.ensemble.StackingClassifier) with Logistic Regression as a final model\n",
    "* [Stacking Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html#sklearn.ensemble.StackingClassifier) with Gradeint Boosting as a final model\n",
    "\n",
    "\n",
    "If not stated in the task, feel free to tune / choose hyperparameters and base models.\n",
    "\n",
    "Answer the questions:\n",
    "* Which model has the best performance?\n",
    "* Does bagging reduce overfiting of the gradient boosting with large amount of trees? \n",
    "* What is the difference between voting and staking? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging with decision trees as base estimators:\n",
      "F1 for test: 0.9\n",
      "F1 for train: 0.9944444444444445\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier, VotingClassifier, StackingClassifier\n",
    "\n",
    "decision_tree_bagging = BaggingClassifier(DecisionTreeClassifier(), random_state=42)\n",
    "decision_tree_bagging.fit(x_train_disease, y_train_disease)\n",
    "print('Bagging with decision trees as base estimators:')\n",
    "print('F1 for test:', f1_score(y_test_disease, decision_tree_bagging.predict(x_test_disease)))\n",
    "print('F1 for train:', f1_score(y_train_disease, decision_tree_bagging.predict(x_train_disease)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging with gradient boosting (with large amount of trees, >100) as base estimators: \n",
      "F1 for test: 0.9\n",
      "F1 for train: 0.9831460674157304\n"
     ]
    }
   ],
   "source": [
    "gr_boosting_bagging = BaggingClassifier(GradientBoostingClassifier(n_estimators=200, random_state=42), random_state=42)\n",
    "gr_boosting_bagging.fit(x_train_disease, y_train_disease)\n",
    "print('Bagging with gradient boosting (with large amount of trees, >100) as base estimators: ')\n",
    "print('F1 for test:', f1_score(y_test_disease, gr_boosting_bagging.predict(x_test_disease)))\n",
    "print('F1 for train:', f1_score(y_train_disease, gr_boosting_bagging.predict(x_train_disease)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier:\n",
      "F1 for test: 0.8514851485148515\n",
      "F1 for train: 1.0\n"
     ]
    }
   ],
   "source": [
    "voting_clf = VotingClassifier(estimators=[\n",
    "    ('rf', RandomForestClassifier(random_state=42)),\n",
    "    ('dt', DecisionTreeClassifier(random_state=42)),\n",
    "    ('gb', GradientBoostingClassifier(random_state=42))])\n",
    "voting_clf.fit(x_train_disease, y_train_disease)\n",
    "print('Voting Classifier:')\n",
    "print('F1 for test:', f1_score(y_test_disease, voting_clf.predict(x_test_disease)))\n",
    "print('F1 for train:', f1_score(y_train_disease, voting_clf.predict(x_train_disease)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Classifier with Logistic Regression as a final model:\n",
      "F1 for test: 0.8659793814432989\n",
      "F1 for train: 1.0\n"
     ]
    }
   ],
   "source": [
    "stacking_logit = StackingClassifier(estimators=[\n",
    "    ('rf', RandomForestClassifier(random_state=42)),\n",
    "    ('dt', DecisionTreeClassifier(random_state=42)),\n",
    "    ('gb', GradientBoostingClassifier(random_state=42))],\n",
    "    final_estimator=LogisticRegression())\n",
    "stacking_logit.fit(x_train_disease, y_train_disease)\n",
    "print('Stacking Classifier with Logistic Regression as a final model:')\n",
    "print('F1 for test:', f1_score(y_test_disease, stacking_logit.predict(x_test_disease)))\n",
    "print('F1 for train:', f1_score(y_train_disease, stacking_logit.predict(x_train_disease)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Classifier with Gradient Boosting as a final model:\n",
      "F1 for test: 0.8686868686868686\n",
      "F1 for train: 0.9888888888888889\n"
     ]
    }
   ],
   "source": [
    "stacking_gr_boosting = StackingClassifier(estimators=[\n",
    "    ('rf', RandomForestClassifier(random_state=42)),\n",
    "    ('dt', DecisionTreeClassifier(random_state=42)),\n",
    "    ('gb', GradientBoostingClassifier(random_state=42))],\n",
    "    final_estimator=GradientBoostingClassifier(random_state=42))\n",
    "stacking_gr_boosting.fit(x_train_disease, y_train_disease)\n",
    "print('Stacking Classifier with Gradient Boosting as a final model:')\n",
    "print('F1 for test:', f1_score(y_test_disease, stacking_gr_boosting.predict(x_test_disease)))\n",
    "print('F1 for train:', f1_score(y_train_disease, stacking_gr_boosting.predict(x_train_disease)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 5 <a id=\"task2_5\"></a> (0.1 points)\n",
    "\n",
    "Report the test score for the best model, that you were able to train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging with decision trees as base estimators is best model:\n",
      "F1 for test: 0.9\n"
     ]
    }
   ],
   "source": [
    "print('Bagging with decision trees as base estimators is best model:')\n",
    "print('F1 for test:', f1_score(y_test_disease, decision_tree_bagging.predict(x_test_disease)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}