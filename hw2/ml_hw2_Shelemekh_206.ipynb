{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# HSE 2022: Mathematical Methods for Data Analysis\n",
    "\n",
    "## Homework 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-26T16:48:20.566549Z",
     "start_time": "2020-09-26T16:48:19.893995Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.regression.linear_model import OLSResults\n",
    "from math import sqrt\n",
    "import random\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Data\n",
    "\n",
    "For this homework we use Dataset from seaborn on diamonds prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = sns.load_dataset('diamonds')\n",
    "y = data.price\n",
    "X = data.drop(['price'], axis=1)\n",
    "columns = data.drop(['price'], axis=1).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 0. [0.25 points] Encode categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 53940 entries, 0 to 53939\n",
      "Data columns (total 9 columns):\n",
      " #   Column   Non-Null Count  Dtype   \n",
      "---  ------   --------------  -----   \n",
      " 0   carat    53940 non-null  float64 \n",
      " 1   cut      53940 non-null  category\n",
      " 2   color    53940 non-null  category\n",
      " 3   clarity  53940 non-null  category\n",
      " 4   depth    53940 non-null  float64 \n",
      " 5   table    53940 non-null  float64 \n",
      " 6   x        53940 non-null  float64 \n",
      " 7   y        53940 non-null  float64 \n",
      " 8   z        53940 non-null  float64 \n",
      "dtypes: category(3), float64(6)\n",
      "memory usage: 2.6 MB\n",
      "None\n",
      "Categorical variables:\n",
      "['cut', 'color', 'clarity']\n"
     ]
    },
    {
     "data": {
      "text/plain": "   carat  cut  color  clarity  depth  table     x     y     z\n0   0.23    2      1        3   61.5   55.0  3.95  3.98  2.43\n1   0.21    3      1        2   59.8   61.0  3.89  3.84  2.31\n2   0.23    1      1        4   56.9   65.0  4.05  4.07  2.31\n3   0.29    3      5        5   62.4   58.0  4.20  4.23  2.63\n4   0.31    1      6        3   63.3   58.0  4.34  4.35  2.75",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>carat</th>\n      <th>cut</th>\n      <th>color</th>\n      <th>clarity</th>\n      <th>depth</th>\n      <th>table</th>\n      <th>x</th>\n      <th>y</th>\n      <th>z</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.23</td>\n      <td>2</td>\n      <td>1</td>\n      <td>3</td>\n      <td>61.5</td>\n      <td>55.0</td>\n      <td>3.95</td>\n      <td>3.98</td>\n      <td>2.43</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.21</td>\n      <td>3</td>\n      <td>1</td>\n      <td>2</td>\n      <td>59.8</td>\n      <td>61.0</td>\n      <td>3.89</td>\n      <td>3.84</td>\n      <td>2.31</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.23</td>\n      <td>1</td>\n      <td>1</td>\n      <td>4</td>\n      <td>56.9</td>\n      <td>65.0</td>\n      <td>4.05</td>\n      <td>4.07</td>\n      <td>2.31</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.29</td>\n      <td>3</td>\n      <td>5</td>\n      <td>5</td>\n      <td>62.4</td>\n      <td>58.0</td>\n      <td>4.20</td>\n      <td>4.23</td>\n      <td>2.63</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.31</td>\n      <td>1</td>\n      <td>6</td>\n      <td>3</td>\n      <td>63.3</td>\n      <td>58.0</td>\n      <td>4.34</td>\n      <td>4.35</td>\n      <td>2.75</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here \n",
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "\n",
    "print(X.info())\n",
    "categories = (data.dtypes ==\"category\")\n",
    "object_cols = list(categories[categories].index)\n",
    "print(\"Categorical variables:\")\n",
    "print(object_cols)\n",
    "\n",
    "encoded_data = X.copy()\n",
    "label_encoder = LabelEncoder()\n",
    "for col in object_cols:\n",
    "    encoded_data[col] = label_encoder.fit_transform(encoded_data[col])\n",
    "\n",
    "# check encoded\n",
    "encoded_data.head()\n",
    "\n",
    "# encoded with labelEncoder, maybe to rewrite with OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 1. [0.25 points] Split the data into train and test sets with ratio 80:20 with random_state=17."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# your code here \n",
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "X_train, X_test, y_train, y_test = train_test_split(encoded_data, y, test_size=0.2, random_state=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2. [1 point] Train models on train data using StatsModels library and apply it to the test set; use $RMSE$ and $R^2$ as the quality measure.\n",
    "\n",
    "* [`LinearRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html);\n",
    "* [`Ridge`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) with $\\alpha = 0.01$;\n",
    "* [`Lasso`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) with $\\alpha = 0.01$\n",
    "* [`ElasticNet`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html) with $\\alpha = 0.01$, $l_{1}$_$ratio = 0.6$\n",
    "\n",
    "Don't forget to scale the data before training the models with StandardScaler!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# your code here \n",
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "\n",
    "# scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE = 1347.9933\n",
      "Test RMSE = 1370.9682\n",
      "Train R2 = 0.8853\n",
      "Test R2 = 0.8839\n"
     ]
    }
   ],
   "source": [
    "# for statsmodels\n",
    "X_train = sm.add_constant(X_train)\n",
    "X_test = sm.add_constant(X_test)\n",
    "model= sm.OLS(y_train, X_train)\n",
    "\n",
    "# Linear Regression statsmodels\n",
    "results_lr = model.fit()\n",
    "y_test_predicted = results_lr.predict(X_test)\n",
    "y_train_predicted = results_lr.predict(X_train)\n",
    "\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_test_predicted, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_test_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE = 1365.9920\n",
      "Test RMSE = 1383.9541\n",
      "Train R2 = 0.8822\n",
      "Test R2 = 0.8817\n"
     ]
    }
   ],
   "source": [
    "# Ridge statsmodels\n",
    "results_ridge = model.fit_regularized(L1_wt=0, alpha=0.01)\n",
    "y_test_predicted = results_ridge.predict(X_test)\n",
    "y_train_predicted = results_ridge.predict(X_train)\n",
    "\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_test_predicted, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_test_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE = 1348.6037\n",
      "Test RMSE = 1370.0240\n",
      "Train R2 = 0.8852\n",
      "Test R2 = 0.8841\n"
     ]
    }
   ],
   "source": [
    "# Lasso statsmodels\n",
    "results_lasso = model.fit_regularized(L1_wt=1, alpha=0.01)\n",
    "# results.summary2()\n",
    "y_test_predicted = results_lasso.predict(X_test)\n",
    "y_train_predicted = results_lasso.predict(X_train)\n",
    "\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_test_predicted, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_test_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE = 1353.6044\n",
      "Test RMSE = 1372.8616\n",
      "Train R2 = 0.8844\n",
      "Test R2 = 0.8836\n"
     ]
    }
   ],
   "source": [
    "# elastic statsmodels\n",
    "results_elastic = model.fit_regularized(L1_wt=0.6, alpha=0.01)\n",
    "y_test_predicted = results_elastic.predict(X_test)\n",
    "y_train_predicted = results_elastic.predict(X_train)\n",
    "\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_test_predicted, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_test_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "outputs": [],
   "source": [
    "# # LinearRegression\n",
    "# model_lr = LinearRegression()\n",
    "# model_lr.fit(X_train, y_train)\n",
    "# y_predicted = model_lr.predict(X_test)\n",
    "# y_train_predicted = model_lr.predict(X_train)\n",
    "#\n",
    "# print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "# print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_predicted, squared=False))\n",
    "# print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "# print(\"Test R2 = %.4f\" % r2_score(y_test, y_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "outputs": [],
   "source": [
    "# # ridge\n",
    "# model_ridge = Ridge(alpha=0.01)\n",
    "# model_ridge.fit(X_train, y_train)\n",
    "# y_predicted = model_ridge.predict(X_test)\n",
    "# y_train_predicted = model_ridge.predict(X_train)\n",
    "#\n",
    "# print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "# print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_predicted, squared=False))\n",
    "# print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "# print(\"Test R2 = %.4f\" % r2_score(y_test, y_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "outputs": [],
   "source": [
    "# # Lasso\n",
    "# model_lasso = Lasso(alpha=0.01)\n",
    "# model_lasso.fit(X_train, y_train)\n",
    "# y_predicted = model_lasso.predict(X_test)\n",
    "# y_train_predicted = model_lasso.predict(X_train)\n",
    "#\n",
    "# print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "# print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_predicted, squared=False))\n",
    "# print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "# print(\"Test R2 = %.4f\" % r2_score(y_test, y_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "outputs": [],
   "source": [
    "# # ElasticNet\n",
    "# model_elastic = ElasticNet(alpha=0.01, l1_ratio=0.6)\n",
    "# model_elastic.fit(X_train, y_train)\n",
    "# y_predicted = model_elastic.predict(X_test)\n",
    "# y_train_predicted = model_elastic.predict(X_train)\n",
    "#\n",
    "# print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "# print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_predicted, squared=False))\n",
    "# print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "# print(\"Test R2 = %.4f\" % r2_score(y_test, y_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 3. [1 point] Explore the values of the parameters of the resulting models and compare the number of zero weights in them. Comment on the significance of the coefficients, overal model significance and other related factors from the results table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<class 'statsmodels.iolib.summary2.Summary'>\n\"\"\"\n                  Results: Ordinary least squares\n====================================================================\nModel:              OLS              Adj. R-squared:     0.885      \nDependent Variable: price            AIC:                744418.8263\nDate:               2022-10-13 00:49 BIC:                744505.5512\nNo. Observations:   43152            Log-Likelihood:     -3.7220e+05\nDf Model:           9                F-statistic:        3.701e+04  \nDf Residuals:       43142            Prob (F-statistic): 0.00       \nR-squared:          0.885            Scale:              1.8175e+06 \n---------------------------------------------------------------------\n         Coef.     Std.Err.     t      P>|t|     [0.025      0.975]  \n---------------------------------------------------------------------\nconst   3928.6813    6.4899  605.3537  0.0000   3915.9610   3941.4016\nx1      5257.1453   31.0710  169.1979  0.0000   5196.2456   5318.0450\nx2        76.4610    6.6590   11.4824  0.0000     63.4093     89.5128\nx3      -455.4350    6.8100  -66.8778  0.0000   -468.7826   -442.0874\nx4       491.4240    6.7022   73.3231  0.0000    478.2876    504.5604\nx5      -226.2704    7.9533  -28.4498  0.0000   -241.8590   -210.6818\nx6      -213.2612    6.9923  -30.4996  0.0000   -226.9662   -199.5562\nx7     -1383.2878   48.3537  -28.6077  0.0000  -1478.0619  -1288.5137\nx8        42.1665   29.5137    1.4287  0.1531    -15.6809    100.0138\nx9         3.3540   29.6459    0.1131  0.9099    -54.7524     61.4604\n--------------------------------------------------------------------\nOmnibus:            11265.146      Durbin-Watson:         1.999     \nProb(Omnibus):      0.000          Jarque-Bera (JB):      360275.892\nSkew:               0.611          Prob(JB):              0.000     \nKurtosis:           17.103         Condition No.:         18        \n====================================================================\n\n\"\"\"",
      "text/html": "<table class=\"simpletable\">\n<tr>\n        <td>Model:</td>               <td>OLS</td>         <td>Adj. R-squared:</td>      <td>0.885</td>   \n</tr>\n<tr>\n  <td>Dependent Variable:</td>       <td>price</td>             <td>AIC:</td>         <td>744418.8263</td>\n</tr>\n<tr>\n         <td>Date:</td>        <td>2022-10-13 00:49</td>        <td>BIC:</td>         <td>744505.5512</td>\n</tr>\n<tr>\n   <td>No. Observations:</td>        <td>43152</td>        <td>Log-Likelihood:</td>   <td>-3.7220e+05</td>\n</tr>\n<tr>\n       <td>Df Model:</td>              <td>9</td>           <td>F-statistic:</td>      <td>3.701e+04</td> \n</tr>\n<tr>\n     <td>Df Residuals:</td>          <td>43142</td>      <td>Prob (F-statistic):</td>    <td>0.00</td>    \n</tr>\n<tr>\n      <td>R-squared:</td>            <td>0.885</td>            <td>Scale:</td>        <td>1.8175e+06</td> \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n    <td></td>       <th>Coef.</th>   <th>Std.Err.</th>     <th>t</th>     <th>P>|t|</th>   <th>[0.025</th>     <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th>  <td>3928.6813</td>  <td>6.4899</td>  <td>605.3537</td> <td>0.0000</td>  <td>3915.9610</td>  <td>3941.4016</td>\n</tr>\n<tr>\n  <th>x1</th>     <td>5257.1453</td>  <td>31.0710</td> <td>169.1979</td> <td>0.0000</td>  <td>5196.2456</td>  <td>5318.0450</td>\n</tr>\n<tr>\n  <th>x2</th>      <td>76.4610</td>   <td>6.6590</td>   <td>11.4824</td> <td>0.0000</td>   <td>63.4093</td>    <td>89.5128</td> \n</tr>\n<tr>\n  <th>x3</th>     <td>-455.4350</td>  <td>6.8100</td>  <td>-66.8778</td> <td>0.0000</td>  <td>-468.7826</td>  <td>-442.0874</td>\n</tr>\n<tr>\n  <th>x4</th>     <td>491.4240</td>   <td>6.7022</td>   <td>73.3231</td> <td>0.0000</td>  <td>478.2876</td>   <td>504.5604</td> \n</tr>\n<tr>\n  <th>x5</th>     <td>-226.2704</td>  <td>7.9533</td>  <td>-28.4498</td> <td>0.0000</td>  <td>-241.8590</td>  <td>-210.6818</td>\n</tr>\n<tr>\n  <th>x6</th>     <td>-213.2612</td>  <td>6.9923</td>  <td>-30.4996</td> <td>0.0000</td>  <td>-226.9662</td>  <td>-199.5562</td>\n</tr>\n<tr>\n  <th>x7</th>    <td>-1383.2878</td>  <td>48.3537</td> <td>-28.6077</td> <td>0.0000</td> <td>-1478.0619</td> <td>-1288.5137</td>\n</tr>\n<tr>\n  <th>x8</th>      <td>42.1665</td>   <td>29.5137</td>  <td>1.4287</td>  <td>0.1531</td>  <td>-15.6809</td>   <td>100.0138</td> \n</tr>\n<tr>\n  <th>x9</th>      <td>3.3540</td>    <td>29.6459</td>  <td>0.1131</td>  <td>0.9099</td>  <td>-54.7524</td>    <td>61.4604</td> \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n     <td>Omnibus:</td>    <td>11265.146</td>  <td>Durbin-Watson:</td>      <td>1.999</td>  \n</tr>\n<tr>\n  <td>Prob(Omnibus):</td>   <td>0.000</td>   <td>Jarque-Bera (JB):</td> <td>360275.892</td>\n</tr>\n<tr>\n       <td>Skew:</td>       <td>0.611</td>       <td>Prob(JB):</td>        <td>0.000</td>  \n</tr>\n<tr>\n     <td>Kurtosis:</td>    <td>17.103</td>    <td>Condition No.:</td>       <td>18</td>    \n</tr>\n</table>"
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here \n",
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "\n",
    "# linear reg\n",
    "results_lr.summary2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "outputs": [
    {
     "data": {
      "text/plain": "<class 'statsmodels.iolib.summary2.Summary'>\n\"\"\"\n                  Results: Ordinary least squares\n====================================================================\nModel:              OLS              Adj. R-squared:     0.882      \nDependent Variable: price            AIC:                745563.5541\nDate:               2022-10-13 00:49 BIC:                745650.2790\nNo. Observations:   43152            Log-Likelihood:     -3.7277e+05\nDf Model:           9                F-statistic:        3.591e+04  \nDf Residuals:       43142            Prob (F-statistic): 0.00       \nR-squared:          0.882            Scale:              1.8664e+06 \n---------------------------------------------------------------------\n            Coef.    Std.Err.     t      P>|t|     [0.025     0.975] \n---------------------------------------------------------------------\nconst     3889.7835    6.5765  591.4627  0.0000  3876.8933  3902.6736\nx1        4219.7372   31.4858  134.0201  0.0000  4158.0243  4281.4500\nx2          81.8556    6.7479   12.1306  0.0000    68.6296    95.0816\nx3        -424.7483    6.9009  -61.5498  0.0000  -438.2742  -411.2225\nx4         496.6610    6.7917   73.1280  0.0000   483.3492   509.9728\nx5        -162.6547    8.0595  -20.1817  0.0000  -178.4514  -146.8579\nx6        -203.6562    7.0856  -28.7421  0.0000  -217.5442  -189.7682\nx7        -312.9137   48.9993   -6.3861  0.0000  -408.9533  -216.8741\nx8          21.4162   29.9077    0.7161  0.4739   -37.2035    80.0359\nx9         -39.9802   30.0417   -1.3308  0.1833   -98.8625    18.9021\n--------------------------------------------------------------------\nOmnibus:            13066.102      Durbin-Watson:         1.998     \nProb(Omnibus):      0.000          Jarque-Bera (JB):      183804.270\nSkew:               1.067          Prob(JB):              0.000     \nKurtosis:           12.883         Condition No.:         18        \n====================================================================\n\n\"\"\"",
      "text/html": "<table class=\"simpletable\">\n<tr>\n        <td>Model:</td>               <td>OLS</td>         <td>Adj. R-squared:</td>      <td>0.882</td>   \n</tr>\n<tr>\n  <td>Dependent Variable:</td>       <td>price</td>             <td>AIC:</td>         <td>745563.5541</td>\n</tr>\n<tr>\n         <td>Date:</td>        <td>2022-10-13 00:49</td>        <td>BIC:</td>         <td>745650.2790</td>\n</tr>\n<tr>\n   <td>No. Observations:</td>        <td>43152</td>        <td>Log-Likelihood:</td>   <td>-3.7277e+05</td>\n</tr>\n<tr>\n       <td>Df Model:</td>              <td>9</td>           <td>F-statistic:</td>      <td>3.591e+04</td> \n</tr>\n<tr>\n     <td>Df Residuals:</td>          <td>43142</td>      <td>Prob (F-statistic):</td>    <td>0.00</td>    \n</tr>\n<tr>\n      <td>R-squared:</td>            <td>0.882</td>            <td>Scale:</td>        <td>1.8664e+06</td> \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n    <td></td>      <th>Coef.</th>   <th>Std.Err.</th>     <th>t</th>     <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th> <td>3889.7835</td>  <td>6.5765</td>  <td>591.4627</td> <td>0.0000</td> <td>3876.8933</td> <td>3902.6736</td>\n</tr>\n<tr>\n  <th>x1</th>    <td>4219.7372</td>  <td>31.4858</td> <td>134.0201</td> <td>0.0000</td> <td>4158.0243</td> <td>4281.4500</td>\n</tr>\n<tr>\n  <th>x2</th>     <td>81.8556</td>   <td>6.7479</td>   <td>12.1306</td> <td>0.0000</td>  <td>68.6296</td>   <td>95.0816</td> \n</tr>\n<tr>\n  <th>x3</th>    <td>-424.7483</td>  <td>6.9009</td>  <td>-61.5498</td> <td>0.0000</td> <td>-438.2742</td> <td>-411.2225</td>\n</tr>\n<tr>\n  <th>x4</th>    <td>496.6610</td>   <td>6.7917</td>   <td>73.1280</td> <td>0.0000</td> <td>483.3492</td>  <td>509.9728</td> \n</tr>\n<tr>\n  <th>x5</th>    <td>-162.6547</td>  <td>8.0595</td>  <td>-20.1817</td> <td>0.0000</td> <td>-178.4514</td> <td>-146.8579</td>\n</tr>\n<tr>\n  <th>x6</th>    <td>-203.6562</td>  <td>7.0856</td>  <td>-28.7421</td> <td>0.0000</td> <td>-217.5442</td> <td>-189.7682</td>\n</tr>\n<tr>\n  <th>x7</th>    <td>-312.9137</td>  <td>48.9993</td>  <td>-6.3861</td> <td>0.0000</td> <td>-408.9533</td> <td>-216.8741</td>\n</tr>\n<tr>\n  <th>x8</th>     <td>21.4162</td>   <td>29.9077</td>  <td>0.7161</td>  <td>0.4739</td> <td>-37.2035</td>   <td>80.0359</td> \n</tr>\n<tr>\n  <th>x9</th>    <td>-39.9802</td>   <td>30.0417</td>  <td>-1.3308</td> <td>0.1833</td> <td>-98.8625</td>   <td>18.9021</td> \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n     <td>Omnibus:</td>    <td>13066.102</td>  <td>Durbin-Watson:</td>      <td>1.998</td>  \n</tr>\n<tr>\n  <td>Prob(Omnibus):</td>   <td>0.000</td>   <td>Jarque-Bera (JB):</td> <td>183804.270</td>\n</tr>\n<tr>\n       <td>Skew:</td>       <td>1.067</td>       <td>Prob(JB):</td>        <td>0.000</td>  \n</tr>\n<tr>\n     <td>Kurtosis:</td>    <td>12.883</td>    <td>Condition No.:</td>       <td>18</td>    \n</tr>\n</table>"
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ridge\n",
    "OLSResults(model, results_ridge.params, model.normalized_cov_params).summary2()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "outputs": [
    {
     "data": {
      "text/plain": "<class 'statsmodels.iolib.summary2.Summary'>\n\"\"\"\n                  Results: Ordinary least squares\n====================================================================\nModel:              OLS              Adj. R-squared:     0.885      \nDependent Variable: price            AIC:                744457.9003\nDate:               2022-10-13 00:49 BIC:                744544.6252\nNo. Observations:   43152            Log-Likelihood:     -3.7222e+05\nDf Model:           9                F-statistic:        3.697e+04  \nDf Residuals:       43142            Prob (F-statistic): 0.00       \nR-squared:          0.885            Scale:              1.8192e+06 \n---------------------------------------------------------------------\n         Coef.     Std.Err.     t      P>|t|     [0.025      0.975]  \n---------------------------------------------------------------------\nconst   3928.6713    6.4928  605.0782  0.0000   3915.9452   3941.3974\nx1      5063.8698   31.0850  162.9037  0.0000   5002.9425   5124.7970\nx2        77.5642    6.6620   11.6428  0.0000     64.5066     90.6218\nx3      -453.1266    6.8130  -66.5087  0.0000   -466.4802   -439.7729\nx4       495.2762    6.7052   73.8644  0.0000    482.1339    508.4186\nx5      -214.9442    7.9569  -27.0135  0.0000   -230.5399   -199.3485\nx6      -213.4343    6.9954  -30.5105  0.0000   -227.1455   -199.7231\nx7     -1201.4586   48.3756  -24.8361  0.0000  -1296.2757  -1106.6416\nx8        54.7441   29.5270    1.8540  0.0637     -3.1294    112.6176\nx9        -1.4625   29.6593   -0.0493  0.9607    -59.5953     56.6702\n--------------------------------------------------------------------\nOmnibus:            11656.263      Durbin-Watson:         1.999     \nProb(Omnibus):      0.000          Jarque-Bera (JB):      315053.588\nSkew:               0.713          Prob(JB):              0.000     \nKurtosis:           16.160         Condition No.:         18        \n====================================================================\n\n\"\"\"",
      "text/html": "<table class=\"simpletable\">\n<tr>\n        <td>Model:</td>               <td>OLS</td>         <td>Adj. R-squared:</td>      <td>0.885</td>   \n</tr>\n<tr>\n  <td>Dependent Variable:</td>       <td>price</td>             <td>AIC:</td>         <td>744457.9003</td>\n</tr>\n<tr>\n         <td>Date:</td>        <td>2022-10-13 00:49</td>        <td>BIC:</td>         <td>744544.6252</td>\n</tr>\n<tr>\n   <td>No. Observations:</td>        <td>43152</td>        <td>Log-Likelihood:</td>   <td>-3.7222e+05</td>\n</tr>\n<tr>\n       <td>Df Model:</td>              <td>9</td>           <td>F-statistic:</td>      <td>3.697e+04</td> \n</tr>\n<tr>\n     <td>Df Residuals:</td>          <td>43142</td>      <td>Prob (F-statistic):</td>    <td>0.00</td>    \n</tr>\n<tr>\n      <td>R-squared:</td>            <td>0.885</td>            <td>Scale:</td>        <td>1.8192e+06</td> \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n    <td></td>       <th>Coef.</th>   <th>Std.Err.</th>     <th>t</th>     <th>P>|t|</th>   <th>[0.025</th>     <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th>  <td>3928.6713</td>  <td>6.4928</td>  <td>605.0782</td> <td>0.0000</td>  <td>3915.9452</td>  <td>3941.3974</td>\n</tr>\n<tr>\n  <th>x1</th>     <td>5063.8698</td>  <td>31.0850</td> <td>162.9037</td> <td>0.0000</td>  <td>5002.9425</td>  <td>5124.7970</td>\n</tr>\n<tr>\n  <th>x2</th>      <td>77.5642</td>   <td>6.6620</td>   <td>11.6428</td> <td>0.0000</td>   <td>64.5066</td>    <td>90.6218</td> \n</tr>\n<tr>\n  <th>x3</th>     <td>-453.1266</td>  <td>6.8130</td>  <td>-66.5087</td> <td>0.0000</td>  <td>-466.4802</td>  <td>-439.7729</td>\n</tr>\n<tr>\n  <th>x4</th>     <td>495.2762</td>   <td>6.7052</td>   <td>73.8644</td> <td>0.0000</td>  <td>482.1339</td>   <td>508.4186</td> \n</tr>\n<tr>\n  <th>x5</th>     <td>-214.9442</td>  <td>7.9569</td>  <td>-27.0135</td> <td>0.0000</td>  <td>-230.5399</td>  <td>-199.3485</td>\n</tr>\n<tr>\n  <th>x6</th>     <td>-213.4343</td>  <td>6.9954</td>  <td>-30.5105</td> <td>0.0000</td>  <td>-227.1455</td>  <td>-199.7231</td>\n</tr>\n<tr>\n  <th>x7</th>    <td>-1201.4586</td>  <td>48.3756</td> <td>-24.8361</td> <td>0.0000</td> <td>-1296.2757</td> <td>-1106.6416</td>\n</tr>\n<tr>\n  <th>x8</th>      <td>54.7441</td>   <td>29.5270</td>  <td>1.8540</td>  <td>0.0637</td>   <td>-3.1294</td>   <td>112.6176</td> \n</tr>\n<tr>\n  <th>x9</th>      <td>-1.4625</td>   <td>29.6593</td>  <td>-0.0493</td> <td>0.9607</td>  <td>-59.5953</td>    <td>56.6702</td> \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n     <td>Omnibus:</td>    <td>11656.263</td>  <td>Durbin-Watson:</td>      <td>1.999</td>  \n</tr>\n<tr>\n  <td>Prob(Omnibus):</td>   <td>0.000</td>   <td>Jarque-Bera (JB):</td> <td>315053.588</td>\n</tr>\n<tr>\n       <td>Skew:</td>       <td>0.713</td>       <td>Prob(JB):</td>        <td>0.000</td>  \n</tr>\n<tr>\n     <td>Kurtosis:</td>    <td>16.160</td>    <td>Condition No.:</td>       <td>18</td>    \n</tr>\n</table>"
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lasso\n",
    "OLSResults(model, results_lasso.params, model.normalized_cov_params).summary2()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "outputs": [
    {
     "data": {
      "text/plain": "<class 'statsmodels.iolib.summary2.Summary'>\n\"\"\"\n                  Results: Ordinary least squares\n====================================================================\nModel:              OLS              Adj. R-squared:     0.884      \nDependent Variable: price            AIC:                744777.3304\nDate:               2022-10-13 00:49 BIC:                744864.0552\nNo. Observations:   43152            Log-Likelihood:     -3.7238e+05\nDf Model:           9                F-statistic:        3.666e+04  \nDf Residuals:       43142            Prob (F-statistic): 0.00       \nR-squared:          0.884            Scale:              1.8327e+06 \n---------------------------------------------------------------------\n            Coef.    Std.Err.     t      P>|t|     [0.025     0.975] \n---------------------------------------------------------------------\nconst     3913.0232    6.5169  600.4416  0.0000  3900.2499  3925.7965\nx1        4675.8946   31.2003  149.8669  0.0000  4614.7414  4737.0478\nx2          79.7319    6.6867   11.9240  0.0000    66.6258    92.8379\nx3        -440.5288    6.8383  -64.4208  0.0000  -453.9320  -427.1256\nx4         496.4432    6.7301   73.7649  0.0000   483.2521   509.6342\nx5        -190.1759    7.9864  -23.8124  0.0000  -205.8294  -174.5223\nx6        -209.3948    7.0214  -29.8225  0.0000  -223.1568  -195.6328\nx7        -766.0692   48.5550  -15.7774  0.0000  -861.2378  -670.9005\nx8          20.1115   29.6365    0.6786  0.4974   -37.9767    78.1996\nx9         -26.8075   29.7693   -0.9005  0.3679   -85.1558    31.5408\n--------------------------------------------------------------------\nOmnibus:            12406.161      Durbin-Watson:         1.999     \nProb(Omnibus):      0.000          Jarque-Bera (JB):      244618.648\nSkew:               0.894          Prob(JB):              0.000     \nKurtosis:           14.526         Condition No.:         18        \n====================================================================\n\n\"\"\"",
      "text/html": "<table class=\"simpletable\">\n<tr>\n        <td>Model:</td>               <td>OLS</td>         <td>Adj. R-squared:</td>      <td>0.884</td>   \n</tr>\n<tr>\n  <td>Dependent Variable:</td>       <td>price</td>             <td>AIC:</td>         <td>744777.3304</td>\n</tr>\n<tr>\n         <td>Date:</td>        <td>2022-10-13 00:49</td>        <td>BIC:</td>         <td>744864.0552</td>\n</tr>\n<tr>\n   <td>No. Observations:</td>        <td>43152</td>        <td>Log-Likelihood:</td>   <td>-3.7238e+05</td>\n</tr>\n<tr>\n       <td>Df Model:</td>              <td>9</td>           <td>F-statistic:</td>      <td>3.666e+04</td> \n</tr>\n<tr>\n     <td>Df Residuals:</td>          <td>43142</td>      <td>Prob (F-statistic):</td>    <td>0.00</td>    \n</tr>\n<tr>\n      <td>R-squared:</td>            <td>0.884</td>            <td>Scale:</td>        <td>1.8327e+06</td> \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n    <td></td>      <th>Coef.</th>   <th>Std.Err.</th>     <th>t</th>     <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th> <td>3913.0232</td>  <td>6.5169</td>  <td>600.4416</td> <td>0.0000</td> <td>3900.2499</td> <td>3925.7965</td>\n</tr>\n<tr>\n  <th>x1</th>    <td>4675.8946</td>  <td>31.2003</td> <td>149.8669</td> <td>0.0000</td> <td>4614.7414</td> <td>4737.0478</td>\n</tr>\n<tr>\n  <th>x2</th>     <td>79.7319</td>   <td>6.6867</td>   <td>11.9240</td> <td>0.0000</td>  <td>66.6258</td>   <td>92.8379</td> \n</tr>\n<tr>\n  <th>x3</th>    <td>-440.5288</td>  <td>6.8383</td>  <td>-64.4208</td> <td>0.0000</td> <td>-453.9320</td> <td>-427.1256</td>\n</tr>\n<tr>\n  <th>x4</th>    <td>496.4432</td>   <td>6.7301</td>   <td>73.7649</td> <td>0.0000</td> <td>483.2521</td>  <td>509.6342</td> \n</tr>\n<tr>\n  <th>x5</th>    <td>-190.1759</td>  <td>7.9864</td>  <td>-23.8124</td> <td>0.0000</td> <td>-205.8294</td> <td>-174.5223</td>\n</tr>\n<tr>\n  <th>x6</th>    <td>-209.3948</td>  <td>7.0214</td>  <td>-29.8225</td> <td>0.0000</td> <td>-223.1568</td> <td>-195.6328</td>\n</tr>\n<tr>\n  <th>x7</th>    <td>-766.0692</td>  <td>48.5550</td> <td>-15.7774</td> <td>0.0000</td> <td>-861.2378</td> <td>-670.9005</td>\n</tr>\n<tr>\n  <th>x8</th>     <td>20.1115</td>   <td>29.6365</td>  <td>0.6786</td>  <td>0.4974</td> <td>-37.9767</td>   <td>78.1996</td> \n</tr>\n<tr>\n  <th>x9</th>    <td>-26.8075</td>   <td>29.7693</td>  <td>-0.9005</td> <td>0.3679</td> <td>-85.1558</td>   <td>31.5408</td> \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n     <td>Omnibus:</td>    <td>12406.161</td>  <td>Durbin-Watson:</td>      <td>1.999</td>  \n</tr>\n<tr>\n  <td>Prob(Omnibus):</td>   <td>0.000</td>   <td>Jarque-Bera (JB):</td> <td>244618.648</td>\n</tr>\n<tr>\n       <td>Skew:</td>       <td>0.894</td>       <td>Prob(JB):</td>        <td>0.000</td>  \n</tr>\n<tr>\n     <td>Kurtosis:</td>    <td>14.526</td>    <td>Condition No.:</td>       <td>18</td>    \n</tr>\n</table>"
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# elastic\n",
    "OLSResults(model, results_elastic.params, model.normalized_cov_params).summary2()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 4. [1 point] Implement one of the elimination algorithms that were described in the Seminar_4 (Elimination by P-value, Forward elimination, Backward elimination), make conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# your code here \n",
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 5. [1 point] Find the best (in terms of RMSE) $\\alpha$ for Lasso regression using cross-validation with 4 folds. You must select values from range $[10^{-4}, 10^{3}]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Gradient descent\n",
    "\n",
    "#### 6. [3.5 points] Implement a Ridge regression model for the MSE loss function, trained by gradient descent.\n",
    "\n",
    "All calculations must be vectorized, and python loops can only be used for gradient descent iterations. As a stop criterion, you must use (simultaneously):\n",
    "\n",
    "* checking for the Absolute-value norm of the weight difference on two adjacent iterations (for example, less than some small number of the order of $10^{-6}$, set by the `tolerance` parameter);\n",
    "* reaching the maximum number of iterations (for example, 10000, set by the `max_iter` parameter).\n",
    "\n",
    "You need to implement:\n",
    "\n",
    "* Full gradient descent:\n",
    "\n",
    "$$\n",
    "w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} Q(w_{k}).\n",
    "$$\n",
    "\n",
    "* Stochastic Gradient Descent:\n",
    "\n",
    "$$\n",
    "w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} q_{i_{k}}(w_{k}).\n",
    "$$\n",
    "\n",
    "$\\nabla_{w} q_{i_{k}}(w_{k}) \\, $ is the estimate of the gradient over the batch of objects selected randomly.\n",
    "\n",
    "* Momentum method:\n",
    "\n",
    "$$\n",
    "h_0 = 0, \\\\\n",
    "h_{k + 1} = \\alpha h_{k} + \\eta_k \\nabla_{w} Q(w_{k}), \\\\\n",
    "w_{k + 1} = w_{k} - h_{k + 1}.\n",
    "$$\n",
    "\n",
    "* Adagrad method:\n",
    "\n",
    "$$\n",
    "G_0 = 0, \\\\\n",
    "G_{k + 1} = G_{k} + (\\nabla_{w} Q(w_{k+1}))^2, \\\\\n",
    "w_{k + 1} = w_{k} - \\eta * \\frac{\\nabla_{w} Q(w_{k+1})}{\\sqrt{G_{k+1} + \\epsilon}}.\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "To make sure that the optimization process really converges, we will use the `loss_history` class attribute. After calling the `fit` method, it should contain the values of the loss function for all iterations, starting from the first one (before the first step on the anti-gradient).\n",
    "\n",
    "You need to initialize the weights with a random vector from normal distribution. The following is a template class that needs to contain the code implementing all variations of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class LinReg(BaseEstimator):\n",
    "    def __init__(self, delta=1.0, gd_type='Momentum', \n",
    "                 tolerance=1e-4, max_iter=1000, w0=None, eta=1e-2, alpha=1e-3):\n",
    "        \"\"\"\n",
    "        gd_type: str\n",
    "            'GradientDescent', 'StochasticDescent', 'Momentum', 'Adagrad'\n",
    "        delta: float\n",
    "            proportion of object in a batch (for stochastic GD)\n",
    "        tolerance: float\n",
    "            for stopping gradient descent\n",
    "        max_iter: int\n",
    "            maximum number of steps in gradient descent\n",
    "        w0: np.array of shape (d)\n",
    "            init weights\n",
    "        eta: float\n",
    "            learning rate\n",
    "        alpha: float\n",
    "            momentum coefficient\n",
    "        reg_cf: float\n",
    "            regularization coefficient\n",
    "        epsilon: float\n",
    "            numerical stability\n",
    "        \"\"\"\n",
    "        \n",
    "        self.delta = delta\n",
    "        self.gd_type = gd_type\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iter = max_iter\n",
    "        self.w0 = w0\n",
    "        self.alpha = alpha\n",
    "        self.w = None\n",
    "        self.eta = eta\n",
    "        self.loss_history = None # list of loss function values at each training iteration\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (l, d)\n",
    "        y: np.array of shape (l)\n",
    "        ---\n",
    "        output: self\n",
    "        \"\"\"\n",
    "        self.loss_history = []\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.w is None:\n",
    "            raise Exception('Not trained yet')\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def calc_gradient(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (l, d) (l can be equal to 1 if stochastic)\n",
    "        y: np.array of shape (l)\n",
    "        ---\n",
    "        output: np.array of shape (d)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def calc_loss(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (l, d)\n",
    "        y: np.array of shape (l)\n",
    "        ---\n",
    "        output: float \n",
    "        \"\"\" \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 7. [1 points] Train and validate \"hand-written\" models on the same data, and compare the quality with the Sklearn or StatsModels methods. Investigate the effect of the `max_iter` and `alpha` parameters on the optimization process. Is it consistent with your expectations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# your code here \n",
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 8. [1 points] Plot graphs (on the same picture) of the dependence of the loss function value on the iteration number for Full GD, SGD, Momentum and Adagrad. Draw conclusions about the rate of convergence of various modifications of gradient descent.\n",
    "\n",
    "Don't forget about what *beautiful* graphics should look like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# your code here \n",
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}