{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# HSE 2022: Mathematical Methods for Data Analysis\n",
    "\n",
    "## Homework 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-26T16:48:20.566549Z",
     "start_time": "2020-09-26T16:48:19.893995Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.regression.linear_model import OLSResults\n",
    "from math import sqrt\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Data\n",
    "\n",
    "For this homework we use Dataset from seaborn on diamonds prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = sns.load_dataset('diamonds')\n",
    "y = data.price\n",
    "X = data.drop(['price'], axis=1)\n",
    "columns = data.drop(['price'], axis=1).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 0. [0.25 points] Encode categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "categories = (data.dtypes == \"category\")\n",
    "object_cols = list(categories[categories].index)\n",
    "\n",
    "encoded_data = X.copy()\n",
    "label_encoder = LabelEncoder()\n",
    "for col in object_cols:\n",
    "    encoded_data[col] = label_encoder.fit_transform(encoded_data[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 1. [0.25 points] Split the data into train and test sets with ratio 80:20 with random_state=17."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(encoded_data, y, test_size=0.2, random_state=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2. [1 point] Train models on train data using StatsModels library and apply it to the test set; use $RMSE$ and $R^2$ as the quality measure.\n",
    "\n",
    "* [`LinearRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html);\n",
    "* [`Ridge`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) with $\\alpha = 0.01$;\n",
    "* [`Lasso`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) with $\\alpha = 0.01$\n",
    "* [`ElasticNet`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html) with $\\alpha = 0.01$, $l_{1}$_$ratio = 0.6$\n",
    "\n",
    "Don't forget to scale the data before training the models with StandardScaler!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE = 1347.9933\n",
      "Test RMSE = 1370.9682\n",
      "Train R2 = 0.8853\n",
      "Test R2 = 0.8839\n"
     ]
    }
   ],
   "source": [
    "# for statsmodels\n",
    "X_train = sm.add_constant(X_train)\n",
    "X_test = sm.add_constant(X_test)\n",
    "model = sm.OLS(y_train, X_train)\n",
    "\n",
    "# Linear Regression from statsmodels\n",
    "results_lr = model.fit()\n",
    "y_test_predicted = results_lr.predict(X_test)\n",
    "y_train_predicted = results_lr.predict(X_train)\n",
    "\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_test_predicted, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_test_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE = 1365.9920\n",
      "Test RMSE = 1383.9541\n",
      "Train R2 = 0.8822\n",
      "Test R2 = 0.8817\n"
     ]
    }
   ],
   "source": [
    "# Ridge from statsmodels\n",
    "results_ridge = model.fit_regularized(L1_wt=0, alpha=0.01)\n",
    "y_test_predicted = results_ridge.predict(X_test)\n",
    "y_train_predicted = results_ridge.predict(X_train)\n",
    "\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_test_predicted, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_test_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE = 1348.6037\n",
      "Test RMSE = 1370.0240\n",
      "Train R2 = 0.8852\n",
      "Test R2 = 0.8841\n"
     ]
    }
   ],
   "source": [
    "# Lasso from statsmodels\n",
    "results_lasso = model.fit_regularized(L1_wt=1, alpha=0.01)\n",
    "y_test_predicted = results_lasso.predict(X_test)\n",
    "y_train_predicted = results_lasso.predict(X_train)\n",
    "\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_test_predicted, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_test_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE = 1353.6044\n",
      "Test RMSE = 1372.8616\n",
      "Train R2 = 0.8844\n",
      "Test R2 = 0.8836\n"
     ]
    }
   ],
   "source": [
    "# Elastic net from statsmodels\n",
    "results_elastic = model.fit_regularized(L1_wt=0.6, alpha=0.01)\n",
    "y_test_predicted = results_elastic.predict(X_test)\n",
    "y_train_predicted = results_elastic.predict(X_train)\n",
    "\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_test_predicted, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_test_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The best quality has Linear Regression model because it has the smallest RMSE and the highest R2 score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 3. [1 point] Explore the values of the parameters of the resulting models and compare the number of zero weights in them. Comment on the significance of the coefficients, overal model significance and other related factors from the results table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<class 'statsmodels.iolib.summary2.Summary'>\n\"\"\"\n                  Results: Ordinary least squares\n====================================================================\nModel:              OLS              Adj. R-squared:     0.885      \nDependent Variable: price            AIC:                744418.8263\nDate:               2022-10-16 19:30 BIC:                744505.5512\nNo. Observations:   43152            Log-Likelihood:     -3.7220e+05\nDf Model:           9                F-statistic:        3.701e+04  \nDf Residuals:       43142            Prob (F-statistic): 0.00       \nR-squared:          0.885            Scale:              1.8175e+06 \n--------------------------------------------------------------------\n             Coef.    Std.Err.    t     P>|t|    [0.025     0.975]  \n--------------------------------------------------------------------\nconst       3928.6813   6.4899 605.3537 0.0000  3915.9610  3941.4016\ncarat       5257.1453  31.0710 169.1979 0.0000  5196.2456  5318.0450\ncut           76.4610   6.6590  11.4824 0.0000    63.4093    89.5128\ncolor       -455.4350   6.8100 -66.8778 0.0000  -468.7826  -442.0874\nclarity      491.4240   6.7022  73.3231 0.0000   478.2876   504.5604\ndepth       -226.2704   7.9533 -28.4498 0.0000  -241.8590  -210.6818\ntable       -213.2612   6.9923 -30.4996 0.0000  -226.9662  -199.5562\nx          -1383.2878  48.3537 -28.6077 0.0000 -1478.0619 -1288.5137\ny             42.1665  29.5137   1.4287 0.1531   -15.6809   100.0138\nz              3.3540  29.6459   0.1131 0.9099   -54.7524    61.4604\n--------------------------------------------------------------------\nOmnibus:            11265.146      Durbin-Watson:         1.999     \nProb(Omnibus):      0.000          Jarque-Bera (JB):      360275.892\nSkew:               0.611          Prob(JB):              0.000     \nKurtosis:           17.103         Condition No.:         18        \n====================================================================\n\n\"\"\"",
      "text/html": "<table class=\"simpletable\">\n<tr>\n        <td>Model:</td>               <td>OLS</td>         <td>Adj. R-squared:</td>      <td>0.885</td>   \n</tr>\n<tr>\n  <td>Dependent Variable:</td>       <td>price</td>             <td>AIC:</td>         <td>744418.8263</td>\n</tr>\n<tr>\n         <td>Date:</td>        <td>2022-10-16 19:30</td>        <td>BIC:</td>         <td>744505.5512</td>\n</tr>\n<tr>\n   <td>No. Observations:</td>        <td>43152</td>        <td>Log-Likelihood:</td>   <td>-3.7220e+05</td>\n</tr>\n<tr>\n       <td>Df Model:</td>              <td>9</td>           <td>F-statistic:</td>      <td>3.701e+04</td> \n</tr>\n<tr>\n     <td>Df Residuals:</td>          <td>43142</td>      <td>Prob (F-statistic):</td>    <td>0.00</td>    \n</tr>\n<tr>\n      <td>R-squared:</td>            <td>0.885</td>            <td>Scale:</td>        <td>1.8175e+06</td> \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n     <td></td>        <th>Coef.</th>   <th>Std.Err.</th>     <th>t</th>     <th>P>|t|</th>   <th>[0.025</th>     <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th>    <td>3928.6813</td>  <td>6.4899</td>  <td>605.3537</td> <td>0.0000</td>  <td>3915.9610</td>  <td>3941.4016</td>\n</tr>\n<tr>\n  <th>carat</th>    <td>5257.1453</td>  <td>31.0710</td> <td>169.1979</td> <td>0.0000</td>  <td>5196.2456</td>  <td>5318.0450</td>\n</tr>\n<tr>\n  <th>cut</th>       <td>76.4610</td>   <td>6.6590</td>   <td>11.4824</td> <td>0.0000</td>   <td>63.4093</td>    <td>89.5128</td> \n</tr>\n<tr>\n  <th>color</th>    <td>-455.4350</td>  <td>6.8100</td>  <td>-66.8778</td> <td>0.0000</td>  <td>-468.7826</td>  <td>-442.0874</td>\n</tr>\n<tr>\n  <th>clarity</th>  <td>491.4240</td>   <td>6.7022</td>   <td>73.3231</td> <td>0.0000</td>  <td>478.2876</td>   <td>504.5604</td> \n</tr>\n<tr>\n  <th>depth</th>    <td>-226.2704</td>  <td>7.9533</td>  <td>-28.4498</td> <td>0.0000</td>  <td>-241.8590</td>  <td>-210.6818</td>\n</tr>\n<tr>\n  <th>table</th>    <td>-213.2612</td>  <td>6.9923</td>  <td>-30.4996</td> <td>0.0000</td>  <td>-226.9662</td>  <td>-199.5562</td>\n</tr>\n<tr>\n  <th>x</th>       <td>-1383.2878</td>  <td>48.3537</td> <td>-28.6077</td> <td>0.0000</td> <td>-1478.0619</td> <td>-1288.5137</td>\n</tr>\n<tr>\n  <th>y</th>         <td>42.1665</td>   <td>29.5137</td>  <td>1.4287</td>  <td>0.1531</td>  <td>-15.6809</td>   <td>100.0138</td> \n</tr>\n<tr>\n  <th>z</th>         <td>3.3540</td>    <td>29.6459</td>  <td>0.1131</td>  <td>0.9099</td>  <td>-54.7524</td>    <td>61.4604</td> \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n     <td>Omnibus:</td>    <td>11265.146</td>  <td>Durbin-Watson:</td>      <td>1.999</td>  \n</tr>\n<tr>\n  <td>Prob(Omnibus):</td>   <td>0.000</td>   <td>Jarque-Bera (JB):</td> <td>360275.892</td>\n</tr>\n<tr>\n       <td>Skew:</td>       <td>0.611</td>       <td>Prob(JB):</td>        <td>0.000</td>  \n</tr>\n<tr>\n     <td>Kurtosis:</td>    <td>17.103</td>    <td>Condition No.:</td>       <td>18</td>    \n</tr>\n</table>"
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = list(X.iloc[:, :10].columns)\n",
    "features.insert(0, \"const\")\n",
    "\n",
    "# linear regression\n",
    "results_lr.summary2(xname=features)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The values of the features are in \"coef\" column. There is no zero weights.\n",
    "Linear Regression summary:\n",
    "    We consider significance level = 0.05.\n",
    "   1. P-values of \"y\" is higher than 0.05, so we can call \"y\" coefficient insignificant\n",
    "   2. P-values of \"z\" is higher than 0.05, so we can call \"z\" coefficient insignificant\n",
    "   3. R2_adj show that ~88.5% of the variance is explained, so the model is valuable\n",
    "   4. As F-statistics ia a pretty large number, we can say that the model is significant\n",
    "   5. Durbin-Watson value is nearly 2, so there is no autocorrelation\n",
    "   6. Large Jarque-Bera value indicates that errors are not normally distributed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [
    {
     "data": {
      "text/plain": "<class 'statsmodels.iolib.summary2.Summary'>\n\"\"\"\n                  Results: Ordinary least squares\n====================================================================\nModel:              OLS              Adj. R-squared:     0.882      \nDependent Variable: price            AIC:                745563.5541\nDate:               2022-10-16 19:30 BIC:                745650.2790\nNo. Observations:   43152            Log-Likelihood:     -3.7277e+05\nDf Model:           9                F-statistic:        3.591e+04  \nDf Residuals:       43142            Prob (F-statistic): 0.00       \nR-squared:          0.882            Scale:              1.8664e+06 \n---------------------------------------------------------------------\n            Coef.    Std.Err.     t      P>|t|     [0.025     0.975] \n---------------------------------------------------------------------\nconst     3889.7835    6.5765  591.4627  0.0000  3876.8933  3902.6736\ncarat     4219.7372   31.4858  134.0201  0.0000  4158.0243  4281.4500\ncut         81.8556    6.7479   12.1306  0.0000    68.6296    95.0816\ncolor     -424.7483    6.9009  -61.5498  0.0000  -438.2742  -411.2225\nclarity    496.6610    6.7917   73.1280  0.0000   483.3492   509.9728\ndepth     -162.6547    8.0595  -20.1817  0.0000  -178.4514  -146.8579\ntable     -203.6562    7.0856  -28.7421  0.0000  -217.5442  -189.7682\nx         -312.9137   48.9993   -6.3861  0.0000  -408.9533  -216.8741\ny           21.4162   29.9077    0.7161  0.4739   -37.2035    80.0359\nz          -39.9802   30.0417   -1.3308  0.1833   -98.8625    18.9021\n--------------------------------------------------------------------\nOmnibus:            13066.102      Durbin-Watson:         1.998     \nProb(Omnibus):      0.000          Jarque-Bera (JB):      183804.270\nSkew:               1.067          Prob(JB):              0.000     \nKurtosis:           12.883         Condition No.:         18        \n====================================================================\n\n\"\"\"",
      "text/html": "<table class=\"simpletable\">\n<tr>\n        <td>Model:</td>               <td>OLS</td>         <td>Adj. R-squared:</td>      <td>0.882</td>   \n</tr>\n<tr>\n  <td>Dependent Variable:</td>       <td>price</td>             <td>AIC:</td>         <td>745563.5541</td>\n</tr>\n<tr>\n         <td>Date:</td>        <td>2022-10-16 19:30</td>        <td>BIC:</td>         <td>745650.2790</td>\n</tr>\n<tr>\n   <td>No. Observations:</td>        <td>43152</td>        <td>Log-Likelihood:</td>   <td>-3.7277e+05</td>\n</tr>\n<tr>\n       <td>Df Model:</td>              <td>9</td>           <td>F-statistic:</td>      <td>3.591e+04</td> \n</tr>\n<tr>\n     <td>Df Residuals:</td>          <td>43142</td>      <td>Prob (F-statistic):</td>    <td>0.00</td>    \n</tr>\n<tr>\n      <td>R-squared:</td>            <td>0.882</td>            <td>Scale:</td>        <td>1.8664e+06</td> \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n     <td></td>       <th>Coef.</th>   <th>Std.Err.</th>     <th>t</th>     <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th>   <td>3889.7835</td>  <td>6.5765</td>  <td>591.4627</td> <td>0.0000</td> <td>3876.8933</td> <td>3902.6736</td>\n</tr>\n<tr>\n  <th>carat</th>   <td>4219.7372</td>  <td>31.4858</td> <td>134.0201</td> <td>0.0000</td> <td>4158.0243</td> <td>4281.4500</td>\n</tr>\n<tr>\n  <th>cut</th>      <td>81.8556</td>   <td>6.7479</td>   <td>12.1306</td> <td>0.0000</td>  <td>68.6296</td>   <td>95.0816</td> \n</tr>\n<tr>\n  <th>color</th>   <td>-424.7483</td>  <td>6.9009</td>  <td>-61.5498</td> <td>0.0000</td> <td>-438.2742</td> <td>-411.2225</td>\n</tr>\n<tr>\n  <th>clarity</th> <td>496.6610</td>   <td>6.7917</td>   <td>73.1280</td> <td>0.0000</td> <td>483.3492</td>  <td>509.9728</td> \n</tr>\n<tr>\n  <th>depth</th>   <td>-162.6547</td>  <td>8.0595</td>  <td>-20.1817</td> <td>0.0000</td> <td>-178.4514</td> <td>-146.8579</td>\n</tr>\n<tr>\n  <th>table</th>   <td>-203.6562</td>  <td>7.0856</td>  <td>-28.7421</td> <td>0.0000</td> <td>-217.5442</td> <td>-189.7682</td>\n</tr>\n<tr>\n  <th>x</th>       <td>-312.9137</td>  <td>48.9993</td>  <td>-6.3861</td> <td>0.0000</td> <td>-408.9533</td> <td>-216.8741</td>\n</tr>\n<tr>\n  <th>y</th>        <td>21.4162</td>   <td>29.9077</td>  <td>0.7161</td>  <td>0.4739</td> <td>-37.2035</td>   <td>80.0359</td> \n</tr>\n<tr>\n  <th>z</th>       <td>-39.9802</td>   <td>30.0417</td>  <td>-1.3308</td> <td>0.1833</td> <td>-98.8625</td>   <td>18.9021</td> \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n     <td>Omnibus:</td>    <td>13066.102</td>  <td>Durbin-Watson:</td>      <td>1.998</td>  \n</tr>\n<tr>\n  <td>Prob(Omnibus):</td>   <td>0.000</td>   <td>Jarque-Bera (JB):</td> <td>183804.270</td>\n</tr>\n<tr>\n       <td>Skew:</td>       <td>1.067</td>       <td>Prob(JB):</td>        <td>0.000</td>  \n</tr>\n<tr>\n     <td>Kurtosis:</td>    <td>12.883</td>    <td>Condition No.:</td>       <td>18</td>    \n</tr>\n</table>"
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ridge\n",
    "OLSResults(model, results_ridge.params, model.normalized_cov_params).summary2(xname=features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The values of the features are in \"coef\" column. There is no zero weights.\n",
    "Ridge Regression summary:\n",
    "    We consider significance level = 0.05.\n",
    "   1. P-values of \"y\" is higher than 0.05, so we can call \"y\" coefficient insignificant\n",
    "   2. P-values of \"z\" is higher than 0.05, so we can call \"z\" coefficient insignificant\n",
    "   3. R2_adj show that ~88.2% of the variance is explained, so the model is valuable\n",
    "   4. As F-statistics ia a pretty large number, we can say that the model is significant\n",
    "   5. Durbin-Watson value is nearly 2, so there is no autocorrelation\n",
    "   6. Large Jarque-Bera value indicates that errors are not normally distributed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "outputs": [
    {
     "data": {
      "text/plain": "<class 'statsmodels.iolib.summary2.Summary'>\n\"\"\"\n                  Results: Ordinary least squares\n====================================================================\nModel:              OLS              Adj. R-squared:     0.885      \nDependent Variable: price            AIC:                744457.9003\nDate:               2022-10-16 19:30 BIC:                744544.6252\nNo. Observations:   43152            Log-Likelihood:     -3.7222e+05\nDf Model:           9                F-statistic:        3.697e+04  \nDf Residuals:       43142            Prob (F-statistic): 0.00       \nR-squared:          0.885            Scale:              1.8192e+06 \n--------------------------------------------------------------------\n             Coef.    Std.Err.    t     P>|t|    [0.025     0.975]  \n--------------------------------------------------------------------\nconst       3928.6713   6.4928 605.0782 0.0000  3915.9452  3941.3974\ncarat       5063.8698  31.0850 162.9037 0.0000  5002.9425  5124.7970\ncut           77.5642   6.6620  11.6428 0.0000    64.5066    90.6218\ncolor       -453.1266   6.8130 -66.5087 0.0000  -466.4802  -439.7729\nclarity      495.2762   6.7052  73.8644 0.0000   482.1339   508.4186\ndepth       -214.9442   7.9569 -27.0135 0.0000  -230.5399  -199.3485\ntable       -213.4343   6.9954 -30.5105 0.0000  -227.1455  -199.7231\nx          -1201.4586  48.3756 -24.8361 0.0000 -1296.2757 -1106.6416\ny             54.7441  29.5270   1.8540 0.0637    -3.1294   112.6176\nz             -1.4625  29.6593  -0.0493 0.9607   -59.5953    56.6702\n--------------------------------------------------------------------\nOmnibus:            11656.263      Durbin-Watson:         1.999     \nProb(Omnibus):      0.000          Jarque-Bera (JB):      315053.588\nSkew:               0.713          Prob(JB):              0.000     \nKurtosis:           16.160         Condition No.:         18        \n====================================================================\n\n\"\"\"",
      "text/html": "<table class=\"simpletable\">\n<tr>\n        <td>Model:</td>               <td>OLS</td>         <td>Adj. R-squared:</td>      <td>0.885</td>   \n</tr>\n<tr>\n  <td>Dependent Variable:</td>       <td>price</td>             <td>AIC:</td>         <td>744457.9003</td>\n</tr>\n<tr>\n         <td>Date:</td>        <td>2022-10-16 19:30</td>        <td>BIC:</td>         <td>744544.6252</td>\n</tr>\n<tr>\n   <td>No. Observations:</td>        <td>43152</td>        <td>Log-Likelihood:</td>   <td>-3.7222e+05</td>\n</tr>\n<tr>\n       <td>Df Model:</td>              <td>9</td>           <td>F-statistic:</td>      <td>3.697e+04</td> \n</tr>\n<tr>\n     <td>Df Residuals:</td>          <td>43142</td>      <td>Prob (F-statistic):</td>    <td>0.00</td>    \n</tr>\n<tr>\n      <td>R-squared:</td>            <td>0.885</td>            <td>Scale:</td>        <td>1.8192e+06</td> \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n     <td></td>        <th>Coef.</th>   <th>Std.Err.</th>     <th>t</th>     <th>P>|t|</th>   <th>[0.025</th>     <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th>    <td>3928.6713</td>  <td>6.4928</td>  <td>605.0782</td> <td>0.0000</td>  <td>3915.9452</td>  <td>3941.3974</td>\n</tr>\n<tr>\n  <th>carat</th>    <td>5063.8698</td>  <td>31.0850</td> <td>162.9037</td> <td>0.0000</td>  <td>5002.9425</td>  <td>5124.7970</td>\n</tr>\n<tr>\n  <th>cut</th>       <td>77.5642</td>   <td>6.6620</td>   <td>11.6428</td> <td>0.0000</td>   <td>64.5066</td>    <td>90.6218</td> \n</tr>\n<tr>\n  <th>color</th>    <td>-453.1266</td>  <td>6.8130</td>  <td>-66.5087</td> <td>0.0000</td>  <td>-466.4802</td>  <td>-439.7729</td>\n</tr>\n<tr>\n  <th>clarity</th>  <td>495.2762</td>   <td>6.7052</td>   <td>73.8644</td> <td>0.0000</td>  <td>482.1339</td>   <td>508.4186</td> \n</tr>\n<tr>\n  <th>depth</th>    <td>-214.9442</td>  <td>7.9569</td>  <td>-27.0135</td> <td>0.0000</td>  <td>-230.5399</td>  <td>-199.3485</td>\n</tr>\n<tr>\n  <th>table</th>    <td>-213.4343</td>  <td>6.9954</td>  <td>-30.5105</td> <td>0.0000</td>  <td>-227.1455</td>  <td>-199.7231</td>\n</tr>\n<tr>\n  <th>x</th>       <td>-1201.4586</td>  <td>48.3756</td> <td>-24.8361</td> <td>0.0000</td> <td>-1296.2757</td> <td>-1106.6416</td>\n</tr>\n<tr>\n  <th>y</th>         <td>54.7441</td>   <td>29.5270</td>  <td>1.8540</td>  <td>0.0637</td>   <td>-3.1294</td>   <td>112.6176</td> \n</tr>\n<tr>\n  <th>z</th>         <td>-1.4625</td>   <td>29.6593</td>  <td>-0.0493</td> <td>0.9607</td>  <td>-59.5953</td>    <td>56.6702</td> \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n     <td>Omnibus:</td>    <td>11656.263</td>  <td>Durbin-Watson:</td>      <td>1.999</td>  \n</tr>\n<tr>\n  <td>Prob(Omnibus):</td>   <td>0.000</td>   <td>Jarque-Bera (JB):</td> <td>315053.588</td>\n</tr>\n<tr>\n       <td>Skew:</td>       <td>0.713</td>       <td>Prob(JB):</td>        <td>0.000</td>  \n</tr>\n<tr>\n     <td>Kurtosis:</td>    <td>16.160</td>    <td>Condition No.:</td>       <td>18</td>    \n</tr>\n</table>"
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lasso\n",
    "OLSResults(model, results_lasso.params, model.normalized_cov_params).summary2(xname=features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The values of the features are in \"coef\" column. There is no zero weights.\n",
    "Lasso Regression summary:\n",
    "    We consider significance level = 0.05.\n",
    "   1. P-values of \"y\" is higher than 0.05, so we can call \"y\" coefficient insignificant\n",
    "   2. P-values of \"z\" is higher than 0.05, so we can call \"z\" coefficient insignificant\n",
    "   3. R2_adj show that ~88.5% of the variance is explained, so the model is valuable\n",
    "   4. As F-statistics ia a pretty large number, we can say that the model is significant\n",
    "   5. Durbin-Watson value is nearly 2, so there is no autocorrelation\n",
    "   6. Large Jarque-Bera value indicates that errors are not normally distributed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [
    {
     "data": {
      "text/plain": "<class 'statsmodels.iolib.summary2.Summary'>\n\"\"\"\n                  Results: Ordinary least squares\n====================================================================\nModel:              OLS              Adj. R-squared:     0.884      \nDependent Variable: price            AIC:                744777.3304\nDate:               2022-10-16 19:30 BIC:                744864.0552\nNo. Observations:   43152            Log-Likelihood:     -3.7238e+05\nDf Model:           9                F-statistic:        3.666e+04  \nDf Residuals:       43142            Prob (F-statistic): 0.00       \nR-squared:          0.884            Scale:              1.8327e+06 \n---------------------------------------------------------------------\n            Coef.    Std.Err.     t      P>|t|     [0.025     0.975] \n---------------------------------------------------------------------\nconst     3913.0232    6.5169  600.4416  0.0000  3900.2499  3925.7965\ncarat     4675.8946   31.2003  149.8669  0.0000  4614.7414  4737.0478\ncut         79.7319    6.6867   11.9240  0.0000    66.6258    92.8379\ncolor     -440.5288    6.8383  -64.4208  0.0000  -453.9320  -427.1256\nclarity    496.4432    6.7301   73.7649  0.0000   483.2521   509.6342\ndepth     -190.1759    7.9864  -23.8124  0.0000  -205.8294  -174.5223\ntable     -209.3948    7.0214  -29.8225  0.0000  -223.1568  -195.6328\nx         -766.0692   48.5550  -15.7774  0.0000  -861.2378  -670.9005\ny           20.1115   29.6365    0.6786  0.4974   -37.9767    78.1996\nz          -26.8075   29.7693   -0.9005  0.3679   -85.1558    31.5408\n--------------------------------------------------------------------\nOmnibus:            12406.161      Durbin-Watson:         1.999     \nProb(Omnibus):      0.000          Jarque-Bera (JB):      244618.648\nSkew:               0.894          Prob(JB):              0.000     \nKurtosis:           14.526         Condition No.:         18        \n====================================================================\n\n\"\"\"",
      "text/html": "<table class=\"simpletable\">\n<tr>\n        <td>Model:</td>               <td>OLS</td>         <td>Adj. R-squared:</td>      <td>0.884</td>   \n</tr>\n<tr>\n  <td>Dependent Variable:</td>       <td>price</td>             <td>AIC:</td>         <td>744777.3304</td>\n</tr>\n<tr>\n         <td>Date:</td>        <td>2022-10-16 19:30</td>        <td>BIC:</td>         <td>744864.0552</td>\n</tr>\n<tr>\n   <td>No. Observations:</td>        <td>43152</td>        <td>Log-Likelihood:</td>   <td>-3.7238e+05</td>\n</tr>\n<tr>\n       <td>Df Model:</td>              <td>9</td>           <td>F-statistic:</td>      <td>3.666e+04</td> \n</tr>\n<tr>\n     <td>Df Residuals:</td>          <td>43142</td>      <td>Prob (F-statistic):</td>    <td>0.00</td>    \n</tr>\n<tr>\n      <td>R-squared:</td>            <td>0.884</td>            <td>Scale:</td>        <td>1.8327e+06</td> \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n     <td></td>       <th>Coef.</th>   <th>Std.Err.</th>     <th>t</th>     <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th>   <td>3913.0232</td>  <td>6.5169</td>  <td>600.4416</td> <td>0.0000</td> <td>3900.2499</td> <td>3925.7965</td>\n</tr>\n<tr>\n  <th>carat</th>   <td>4675.8946</td>  <td>31.2003</td> <td>149.8669</td> <td>0.0000</td> <td>4614.7414</td> <td>4737.0478</td>\n</tr>\n<tr>\n  <th>cut</th>      <td>79.7319</td>   <td>6.6867</td>   <td>11.9240</td> <td>0.0000</td>  <td>66.6258</td>   <td>92.8379</td> \n</tr>\n<tr>\n  <th>color</th>   <td>-440.5288</td>  <td>6.8383</td>  <td>-64.4208</td> <td>0.0000</td> <td>-453.9320</td> <td>-427.1256</td>\n</tr>\n<tr>\n  <th>clarity</th> <td>496.4432</td>   <td>6.7301</td>   <td>73.7649</td> <td>0.0000</td> <td>483.2521</td>  <td>509.6342</td> \n</tr>\n<tr>\n  <th>depth</th>   <td>-190.1759</td>  <td>7.9864</td>  <td>-23.8124</td> <td>0.0000</td> <td>-205.8294</td> <td>-174.5223</td>\n</tr>\n<tr>\n  <th>table</th>   <td>-209.3948</td>  <td>7.0214</td>  <td>-29.8225</td> <td>0.0000</td> <td>-223.1568</td> <td>-195.6328</td>\n</tr>\n<tr>\n  <th>x</th>       <td>-766.0692</td>  <td>48.5550</td> <td>-15.7774</td> <td>0.0000</td> <td>-861.2378</td> <td>-670.9005</td>\n</tr>\n<tr>\n  <th>y</th>        <td>20.1115</td>   <td>29.6365</td>  <td>0.6786</td>  <td>0.4974</td> <td>-37.9767</td>   <td>78.1996</td> \n</tr>\n<tr>\n  <th>z</th>       <td>-26.8075</td>   <td>29.7693</td>  <td>-0.9005</td> <td>0.3679</td> <td>-85.1558</td>   <td>31.5408</td> \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n     <td>Omnibus:</td>    <td>12406.161</td>  <td>Durbin-Watson:</td>      <td>1.999</td>  \n</tr>\n<tr>\n  <td>Prob(Omnibus):</td>   <td>0.000</td>   <td>Jarque-Bera (JB):</td> <td>244618.648</td>\n</tr>\n<tr>\n       <td>Skew:</td>       <td>0.894</td>       <td>Prob(JB):</td>        <td>0.000</td>  \n</tr>\n<tr>\n     <td>Kurtosis:</td>    <td>14.526</td>    <td>Condition No.:</td>       <td>18</td>    \n</tr>\n</table>"
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# elastic\n",
    "OLSResults(model, results_elastic.params, model.normalized_cov_params).summary2(xname=features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The values of the features are in \"coef\" column. There is no zero weights.\n",
    "Elastic Net Regression summary:\n",
    "    We consider significance level = 0.05.\n",
    "   1. P-values of \"y\" is higher than 0.05, so we can call \"y\" coefficient insignificant\n",
    "   2. P-values of \"z\" is higher than 0.05, so we can call \"z\" coefficient insignificant\n",
    "   3. R2_adj show that ~88.4% of the variance is explained, so the model is valuable\n",
    "   4. As F-statistics ia a pretty large number, we can say that the model is significant\n",
    "   5. Durbin-Watson value is nearly 2, so there is no autocorrelation\n",
    "   6. Large Jarque-Bera value indicates that errors are not normally distributed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<bold> Summary: <bold>\n",
    "1. As Linear Regression model has the smallest AIC value, it should be selected.\n",
    "2. Linear Regression model also has the smallest BIC value\n",
    "3. In all models \"y\" and \"z\" are insignificant\n",
    "4. The Ridge model is the most skewed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 4. [1 point] Implement one of the elimination algorithms that were described in the Seminar_4 (Elimination by P-value, Forward elimination, Backward elimination), make conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<bold>We consider significance level = 0.05.<bold>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start from model with all features:\n",
      "R2_adj = 0.4483662805\n",
      "RMSE = 4153.5072579449\n",
      "AIC = 841537.8310855594\n",
      "\n",
      "Feature \"y\" was eliminated.\n",
      "Current:\n",
      "R2_adj = 0.4483790491\n",
      "RMSE = 4153.5073228418\n",
      "AIC = 841535.8324340256\n",
      "\n",
      "Feature \"x\" was eliminated.\n",
      "Current:\n",
      "R2_adj = 0.4483889897\n",
      "RMSE = 4153.5180325655\n",
      "AIC = 841534.0549666309\n",
      "\n",
      "Features after p-value elimination algorithm:\n",
      "['const' 'carat' 'cut' 'color' 'clarity' 'depth' 'table' 'z']\n"
     ]
    }
   ],
   "source": [
    "# p-value elimination for linear regression model\n",
    "model = sm.OLS(y_train, X_train)\n",
    "results = model.fit()\n",
    "sign_level = 0.05\n",
    "\n",
    "features = list(X.iloc[:, :10].columns)\n",
    "features.insert(0, \"const\")\n",
    "features = np.array(features)\n",
    "\n",
    "print(\"Start from model with all features:\")\n",
    "print(\"R2_adj = %.10f\" % results.rsquared_adj)\n",
    "print(\"RMSE = %.10f\" % mean_squared_error(y_train, results.predict(X_train), squared=False))\n",
    "print(\"AIC = %.10f\\n\" % results.aic)\n",
    "p_values = np.array(results.pvalues)\n",
    "X_eliminated = X_train.copy()\n",
    "\n",
    "while np.max(p_values) > sign_level:\n",
    "    insign_feature_index = np.argmax(p_values)\n",
    "    X_eliminated = np.delete(X_eliminated, insign_feature_index, axis=1)\n",
    "    print(f'Feature \\\"{features[insign_feature_index]}\\\" was eliminated.')\n",
    "    features = np.delete(features, insign_feature_index)\n",
    "    model = sm.OLS(y_train, X_eliminated)\n",
    "    results = model.fit()\n",
    "    p_values = np.array(results.pvalues)\n",
    "    print(\"Current:\\nR2_adj = %.10f\" % results.rsquared_adj)\n",
    "    print(\"RMSE = %.10f\" % mean_squared_error(y_train, results.predict(X_eliminated), squared=False))\n",
    "    print(\"AIC = %.10f\\n\" % results.aic)\n",
    "\n",
    "print(\"Features after p-value elimination algorithm:\")\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Summary: the elimination of the \"z\" increased model's quality, so we can remove it."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 5. [1 point] Find the best (in terms of RMSE) $\\alpha$ for Lasso regression using cross-validation with 4 folds. You must select values from range $[10^{-4}, 10^{3}]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.682695795279722\n"
     ]
    }
   ],
   "source": [
    "parameters = {'alpha': np.logspace(-4, 3)}\n",
    "search = GridSearchCV(Lasso(), parameters, cv=4, scoring='neg_root_mean_squared_error')\n",
    "search.fit(X_train, y_train)\n",
    "print(search.best_params_['alpha'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Gradient descent\n",
    "\n",
    "#### 6. [3.5 points] Implement a Ridge regression model for the MSE loss function, trained by gradient descent.\n",
    "\n",
    "All calculations must be vectorized, and python loops can only be used for gradient descent iterations. As a stop criterion, you must use (simultaneously):\n",
    "\n",
    "* checking for the Absolute-value norm of the weight difference on two adjacent iterations (for example, less than some small number of the order of $10^{-6}$, set by the `tolerance` parameter);\n",
    "* reaching the maximum number of iterations (for example, 10000, set by the `max_iter` parameter).\n",
    "\n",
    "You need to implement:\n",
    "\n",
    "* Full gradient descent:\n",
    "\n",
    "$$\n",
    "w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} Q(w_{k}).\n",
    "$$\n",
    "\n",
    "* Stochastic Gradient Descent:\n",
    "\n",
    "$$\n",
    "w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} q_{i_{k}}(w_{k}).\n",
    "$$\n",
    "\n",
    "$\\nabla_{w} q_{i_{k}}(w_{k}) \\, $ is the estimate of the gradient over the batch of objects selected randomly.\n",
    "\n",
    "* Momentum method:\n",
    "\n",
    "$$\n",
    "h_0 = 0, \\\\\n",
    "h_{k + 1} = \\alpha h_{k} + \\eta_k \\nabla_{w} Q(w_{k}), \\\\\n",
    "w_{k + 1} = w_{k} - h_{k + 1}.\n",
    "$$\n",
    "\n",
    "* Adagrad method:\n",
    "\n",
    "$$\n",
    "G_0 = 0, \\\\\n",
    "G_{k + 1} = G_{k} + (\\nabla_{w} Q(w_{k+1}))^2, \\\\\n",
    "w_{k + 1} = w_{k} - \\eta * \\frac{\\nabla_{w} Q(w_{k+1})}{\\sqrt{G_{k+1} + \\epsilon}}.\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "To make sure that the optimization process really converges, we will use the `loss_history` class attribute. After calling the `fit` method, it should contain the values of the loss function for all iterations, starting from the first one (before the first step on the anti-gradient).\n",
    "\n",
    "You need to initialize the weights with a random vector from normal distribution. The following is a template class that needs to contain the code implementing all variations of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from numpy import linalg as la\n",
    "\n",
    "\n",
    "class LinReg(BaseEstimator):\n",
    "    def __init__(self, delta=1.0, gd_type='Momentum',\n",
    "                 tolerance=1e-4, max_iter=1000, w0=None, eta=1e-2, alpha=1e-3, reg_cf=1.0, epsilon=1e-8):\n",
    "        \"\"\"\n",
    "        gd_type: str\n",
    "            'GradientDescent', 'StochasticDescent', 'Momentum', 'Adagrad'\n",
    "        delta: float\n",
    "            proportion of object in a batch (for stochastic GD)\n",
    "        tolerance: float\n",
    "            for stopping gradient descent\n",
    "        max_iter: int\n",
    "            maximum number of steps in gradient descent\n",
    "        w0: np.array of shape (d)\n",
    "            init weights\n",
    "        eta: float\n",
    "            learning rate\n",
    "        alpha: float\n",
    "            momentum coefficient\n",
    "        reg_cf: float\n",
    "            regularization coefficient\n",
    "        epsilon: float\n",
    "            numerical stability\n",
    "        \"\"\"\n",
    "\n",
    "        self.delta = delta\n",
    "        self.gd_type = gd_type\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iter = max_iter\n",
    "        self.w0 = w0\n",
    "        self.alpha = alpha\n",
    "        self.w = None\n",
    "        self.eta = eta\n",
    "        self.loss_history = None  # list of loss function values at each training iteration\n",
    "        self.epsilon = epsilon\n",
    "        self.reg_cf = reg_cf\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (l, d)\n",
    "        y: np.array of shape (l)\n",
    "        ---\n",
    "        output: self\n",
    "        \"\"\"\n",
    "        self.loss_history = []\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "        if self.gd_type == 'GradientDescent':\n",
    "            self.w = self.full_grad_descent_calc(X, y)\n",
    "        if self.gd_type == 'Momentum':\n",
    "            self.w = self.momentum_descent_calc(X, y)\n",
    "        if self.gd_type == 'Adagrad':\n",
    "            self.w = self.adagrad_descent_calc(X, y)\n",
    "        if self.gd_type == 'StochasticDescent':\n",
    "            self.w = self.stochastic_grad_descent_calc(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.w is None:\n",
    "            raise Exception('Not trained yet')\n",
    "        return np.dot(np.insert(X, 0, 1, axis=1), self.w)\n",
    "\n",
    "    def calc_gradient(self, X, y, w):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (l, d) (l can be equal to 1 if stochastic)\n",
    "        y: np.array of shape (l)\n",
    "        ---\n",
    "        output: np.array of shape (d)\n",
    "        \"\"\"\n",
    "        return 2 * np.dot(X.T, np.dot(X, w) - y) / np.shape(X)[0] + 2 * self.reg_cf * w / np.shape(X)[0]\n",
    "\n",
    "    def full_grad_descent_calc(self, X, y):\n",
    "        w = np.random.normal(0, 1, np.shape(X)[1])\n",
    "        self.w = w.copy()\n",
    "        iter_num = 0\n",
    "        while iter_num < self.max_iter and abs(la.norm(w - self.w) - self.tolerance) > 0:\n",
    "            gradient = self.calc_gradient(X, y, w)\n",
    "            self.loss_history = np.append(self.loss_history, self.calc_loss(X, y))\n",
    "            self.w = w\n",
    "            w -= self.eta * gradient\n",
    "            iter_num += 1\n",
    "        return w\n",
    "\n",
    "    def momentum_descent_calc(self, X, y):\n",
    "        w = np.random.normal(0, 1, np.shape(X)[1])\n",
    "        self.w = w.copy()\n",
    "        iter_num = 0\n",
    "        h = np.zeros(np.shape(X)[1])\n",
    "        while iter_num < self.max_iter and abs(la.norm(w - self.w) - self.tolerance) > 0:\n",
    "            gradient = self.calc_gradient(X, y, w)\n",
    "            self.loss_history = np.append(self.loss_history, self.calc_loss(X, y))\n",
    "            h = self.alpha * h + self.eta * gradient\n",
    "            self.w = w\n",
    "            w -= h\n",
    "            iter_num += 1\n",
    "        return w\n",
    "\n",
    "    def adagrad_descent_calc(self, X, y):\n",
    "        w = np.random.normal(0, 1, np.shape(X)[1])\n",
    "        self.w = w.copy()\n",
    "        iter_num = 0\n",
    "        g = np.zeros(np.shape(X)[1])\n",
    "        while iter_num < self.max_iter and abs(la.norm(w - self.w) - self.tolerance) > 0:\n",
    "            gradient = self.calc_gradient(X, y, w)\n",
    "            self.loss_history = np.append(self.loss_history, self.calc_loss(X, y))\n",
    "            g += np.square(gradient)\n",
    "            self.w = w\n",
    "            w -= self.eta / (np.sqrt(g + self.epsilon)) * gradient\n",
    "            iter_num += 1\n",
    "        return w\n",
    "\n",
    "    def stochastic_grad_descent_calc(self, X, y):\n",
    "        np.random.seed(42)\n",
    "        w = np.random.normal(0, 1, np.shape(X)[1])\n",
    "        self.w = w.copy()\n",
    "        iter_num = 0\n",
    "        batch_size = int(np.round(np.shape(X)[0] * self.delta))\n",
    "        while iter_num < self.max_iter and abs(la.norm(w - self.w) - self.tolerance) > 0:\n",
    "            # get samples from data randomly\n",
    "            batch_indices = np.random.randint(X.shape[0], size=batch_size)\n",
    "            X_batch = X[batch_indices, :]\n",
    "            y_batch = y.iloc[batch_indices]\n",
    "            gradient = self.calc_gradient(X_batch, y_batch, w)\n",
    "            self.w = w\n",
    "            w -= self.eta * gradient\n",
    "\n",
    "            # if we want to run code with epochs (than max_iter = number of epochs)\n",
    "            # for start in range(0, np.shape(X)[0], batch_size):\n",
    "            #     stop = start + batch_size\n",
    "            #     X_batch = X[start:stop]\n",
    "            #     y_batch = y[start:stop]\n",
    "            #     gradient = self.calc_gradient(X_batch, y_batch, w)\n",
    "            #     self.w = w\n",
    "            #     w -= self.eta * gradient\n",
    "\n",
    "            self.loss_history = np.append(self.loss_history, self.calc_loss(X, y))\n",
    "            iter_num += 1\n",
    "        return w\n",
    "\n",
    "    def calc_loss(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (l, d)\n",
    "        y: np.array of shape (l)\n",
    "        ---\n",
    "        output: float \n",
    "        \"\"\"\n",
    "        return np.sum(np.square(np.dot(X, self.w) - y)) / np.shape(X)[0] + self.reg_cf * np.sum(np.square(self.w)) / np.shape(X)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 7. [1 points] Train and validate \"hand-written\" models on the same data, and compare the quality with the Sklearn or StatsModels methods. Investigate the effect of the `max_iter` and `alpha` parameters on the optimization process. Is it consistent with your expectations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full gradient descent (max 1000 iterations, learning rate = 0.01): \n",
      "Train RMSE = 1418.2352\n",
      "Train R2 = 0.8731\n",
      "Test RMSE = 1436.7290\n",
      "Test R2 = 0.8725\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(encoded_data, y, test_size=0.2, random_state=17)\n",
    "# scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "# Full gradient descent\n",
    "model_full_gr = LinReg(gd_type='GradientDescent')\n",
    "model_full_gr.fit(X_train, y_train)\n",
    "y_train_predicted = model_full_gr.predict(X_train)\n",
    "y_test_predicted = model_full_gr.predict(X_test)\n",
    "print(\"Full gradient descent (max 1000 iterations, learning rate = 0.01): \")\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_test_predicted, squared=False))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_test_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full gradient descent (max 500 iterations): \n",
      "Train RMSE = 1488.9875\n",
      "Train R2 = 0.8601\n",
      "Test RMSE = 1513.3263\n",
      "Test R2 = 0.8586\n"
     ]
    }
   ],
   "source": [
    "model_full_gr_iter_test = LinReg(gd_type='GradientDescent', max_iter=500)\n",
    "model_full_gr_iter_test.fit(X_train, y_train)\n",
    "y_train_predicted = model_full_gr_iter_test.predict(X_train)\n",
    "y_test_predicted = model_full_gr_iter_test.predict(X_test)\n",
    "print(\"Full gradient descent (max 500 iterations): \")\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_test_predicted, squared=False))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_test_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full gradient descent (max 1000 iterations, learning rate = 0.001): \n",
      "Train RMSE = 1698.7561\n",
      "Train R2 = 0.8179\n",
      "Test RMSE = 1736.3958\n",
      "Test R2 = 0.8138\n"
     ]
    }
   ],
   "source": [
    "model_full_gr_iter_test = LinReg(gd_type='GradientDescent', eta=0.001)\n",
    "model_full_gr_iter_test.fit(X_train, y_train)\n",
    "y_train_predicted = model_full_gr_iter_test.predict(X_train)\n",
    "y_test_predicted = model_full_gr_iter_test.predict(X_test)\n",
    "print(\"Full gradient descent (max 1000 iterations, learning rate = 0.001): \")\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_test_predicted, squared=False))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_test_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Summary:\n",
    "1. При увеличении max_iter качество модели улучшается, при уменьшении max_iter качество модели ухудшается, как и ожидалось."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Momentum (max 1000 iterations, momentum coefficient = 0.001): \n",
      "Train RMSE = 1418.1097\n",
      "Train R2 = 0.8731\n",
      "Test RMSE = 1436.5942\n",
      "Test R2 = 0.8726\n"
     ]
    }
   ],
   "source": [
    "# Momentum\n",
    "model_momentum = LinReg(gd_type='Momentum')\n",
    "model_momentum.fit(X_train, y_train)\n",
    "y_train_predicted = model_momentum.predict(X_train)\n",
    "y_test_predicted = model_momentum.predict(X_test)\n",
    "print(\"Momentum (max 1000 iterations, momentum coefficient = 0.001): \")\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_test_predicted, squared=False))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_test_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Momentum (max 500 iterations): \n",
      "Train RMSE = 1488.8866\n",
      "Train R2 = 0.8601\n",
      "Test RMSE = 1513.2316\n",
      "Test R2 = 0.8586\n"
     ]
    }
   ],
   "source": [
    "model_momentum_test = LinReg(gd_type='Momentum', max_iter=500)\n",
    "model_momentum_test.fit(X_train, y_train)\n",
    "y_train_predicted = model_momentum_test.predict(X_train)\n",
    "y_test_predicted = model_momentum_test.predict(X_test)\n",
    "print(\"Momentum (max 500 iterations): \")\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_test_predicted, squared=False))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_test_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Momentum (max 1000 iterations, momentum coefficient = 0.1): \n",
      "Train RMSE = 1408.7429\n",
      "Train R2 = 0.8748\n",
      "Test RMSE = 1426.5514\n",
      "Test R2 = 0.8743\n"
     ]
    }
   ],
   "source": [
    "model_momentum_test = LinReg(gd_type='Momentum', alpha=0.1)\n",
    "model_momentum_test.fit(X_train, y_train)\n",
    "y_train_predicted = model_momentum_test.predict(X_train)\n",
    "y_test_predicted = model_momentum_test.predict(X_test)\n",
    "print(\"Momentum (max 1000 iterations, momentum coefficient = 0.1): \")\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_test_predicted, squared=False))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_test_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Summary:\n",
    "1. При увеличении max_iter качество модели улучшается, при уменьшении max_iter качество модели ухудшается, как и ожидалось.\n",
    "2. При увеличении momentum coefficient качество модели улучшается, при уменьшении momentum coefficient качество модели ухудшается."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic (max 1000 iterations, delta = 0.8): \n",
      "Train RMSE = 1418.4459\n",
      "Train R2 = 0.8730\n",
      "Test RMSE = 1437.0572\n",
      "Test R2 = 0.8725\n"
     ]
    }
   ],
   "source": [
    "# Stochastic\n",
    "model_stochastic = LinReg(gd_type='StochasticDescent', delta=0.8)\n",
    "model_stochastic.fit(X_train, y_train)\n",
    "y_train_predicted = model_stochastic.predict(X_train)\n",
    "y_test_predicted = model_stochastic.predict(X_test)\n",
    "print(\"Stochastic (max 1000 iterations, delta = 0.8): \")\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_test_predicted, squared=False))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_test_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic (max 1000 iterations, delta = 0.3: \n",
      "Train RMSE = 1418.5434\n",
      "Train R2 = 0.8730\n",
      "Test RMSE = 1437.1287\n",
      "Test R2 = 0.8725\n"
     ]
    }
   ],
   "source": [
    "model_stochastic_test = LinReg(gd_type='StochasticDescent', delta=0.3)\n",
    "model_stochastic_test.fit(X_train, y_train)\n",
    "y_train_predicted = model_stochastic_test.predict(X_train)\n",
    "y_test_predicted = model_stochastic_test.predict(X_test)\n",
    "print(\"Stochastic (max 1000 iterations, delta = 0.3: \")\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_test_predicted, squared=False))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_test_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic (max 500 iterations, delta = 0.8: \n",
      "Train RMSE = 1488.9311\n",
      "Train R2 = 0.8601\n",
      "Test RMSE = 1513.3074\n",
      "Test R2 = 0.8586\n"
     ]
    }
   ],
   "source": [
    "model_stochastic_test = LinReg(gd_type='StochasticDescent', delta=0.8, max_iter=500)\n",
    "model_stochastic_test.fit(X_train, y_train)\n",
    "y_train_predicted = model_stochastic_test.predict(X_train)\n",
    "y_test_predicted = model_stochastic_test.predict(X_test)\n",
    "print(\"Stochastic (max 500 iterations, delta = 0.8: \")\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_test_predicted, squared=False))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_test_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Summary:\n",
    "1. При увеличении max_iter качество модели улучшается, при уменьшении max_iter качество модели ухудшается, как и ожидалось.\n",
    "2. При увеличении размера батча качество модели улучшается, при уменьшении размера батча качество модели ухудшается, как и ожидалось."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Adagrad\n",
    "# searching the best learning rate for adagrad modification\n",
    "parameters = {'eta': np.logspace(3,4)}\n",
    "search = GridSearchCV(LinReg(gd_type='Adagrad'), parameters, cv=2, scoring='neg_root_mean_squared_error')\n",
    "search.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adagrad: \n",
      "Train RMSE = 1488.9311\n",
      "Train R2 = 0.8601\n",
      "Test RMSE = 1513.3074\n",
      "Test R2 = 0.8586\n"
     ]
    }
   ],
   "source": [
    "# best_eta = search.best_params_['eta']\n",
    "best_eta = 1000\n",
    "model_adagrad = LinReg(gd_type='Adagrad', eta=best_eta)\n",
    "model_adagrad.fit(X_train, y_train)\n",
    "print(\"Adagrad: \")\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_test_predicted, squared=False))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_test_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For Adagrad modification we have to choose a higher learning rate, because of the dynamic learning rate calculations.\n",
    "This modification has the best equality, but this is mostly because of the GridSearchCV for the best learning rate."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge from Sklearn: \n",
      "Train RMSE = 1347.9935\n",
      "Train R2 = 0.8853\n",
      "Test RMSE = 1370.9384\n",
      "Test R2 = 0.8839\n"
     ]
    }
   ],
   "source": [
    "# Sklearn model\n",
    "model = Ridge()\n",
    "model.fit(X_train, y_train)\n",
    "y_train_predicted = model.predict(X_train)\n",
    "y_test_predicted = model.predict(X_test)\n",
    "print(\"Ridge from Sklearn: \")\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_test_predicted, squared=False))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_test_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<bold>Comparing hand-written models with model from Sklearn:<bold>\n",
    "Quality of Sklearn model is a little bit better than the hand-written one, because:\n",
    "   1. Sklearn model's RMSE is less than others.\n",
    "   2. R2 is higher than R2 of other models.\n",
    "\n",
    "But hand-written models still have good quality. Sklearn model explained ~89% of the data (R2). In the same time hand-written models explained ~87% of the data, which is very close to library model. Same about RMSE values.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 8. [1 points] Plot graphs (on the same picture) of the dependence of the loss function value on the iteration number for Full GD, SGD, Momentum and Adagrad. Draw conclusions about the rate of convergence of various modifications of gradient descent.\n",
    "\n",
    "Don't forget about what *beautiful* graphics should look like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Text(0.5, 1.0, 'Loss function value and iteration number dependence')"
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 720x720 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1, figsize=(10,10))\n",
    "plt.plot(model_full_gr.loss_history)\n",
    "plt.plot(model_momentum.loss_history)\n",
    "plt.plot(model_adagrad.loss_history)\n",
    "plt.plot(model_stochastic.loss_history)\n",
    "plt.legend([\"Full Gradient Descent\", \"Momentum\", \"Adagrad\", \"Stochastic GD\"])\n",
    "plt.xlabel(\"iteration number\")\n",
    "plt.ylabel(\"loss function value\")\n",
    "plt.title(\"Loss function value and iteration number dependence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "***Графики накладываются друг на друга\n",
    "Значение функции потерь с каждой итерацией уменьшается для всех модификаций градиентного спуска.\n",
    "Значение резко уменьшается в районе 0-100 итераций."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}