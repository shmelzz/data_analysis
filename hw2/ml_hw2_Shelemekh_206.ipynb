{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# HSE 2022: Mathematical Methods for Data Analysis\n",
    "\n",
    "## Homework 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-26T16:48:20.566549Z",
     "start_time": "2020-09-26T16:48:19.893995Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.regression.linear_model import OLSResults\n",
    "from math import sqrt\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Data\n",
    "\n",
    "For this homework we use Dataset from seaborn on diamonds prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = sns.load_dataset('diamonds')\n",
    "y = data.price\n",
    "X = data.drop(['price'], axis=1)\n",
    "columns = data.drop(['price'], axis=1).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 0. [0.25 points] Encode categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "categories = (data.dtypes == \"category\")\n",
    "object_cols = list(categories[categories].index)\n",
    "\n",
    "encoded_data = X.copy()\n",
    "label_encoder = LabelEncoder()\n",
    "for col in object_cols:\n",
    "    encoded_data[col] = label_encoder.fit_transform(encoded_data[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 1. [0.25 points] Split the data into train and test sets with ratio 80:20 with random_state=17."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(encoded_data, y, test_size=0.2, random_state=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2. [1 point] Train models on train data using StatsModels library and apply it to the test set; use $RMSE$ and $R^2$ as the quality measure.\n",
    "\n",
    "* [`LinearRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html);\n",
    "* [`Ridge`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) with $\\alpha = 0.01$;\n",
    "* [`Lasso`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) with $\\alpha = 0.01$\n",
    "* [`ElasticNet`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html) with $\\alpha = 0.01$, $l_{1}$_$ratio = 0.6$\n",
    "\n",
    "Don't forget to scale the data before training the models with StandardScaler!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for statsmodels\n",
    "X_train = sm.add_constant(X_train)\n",
    "X_test = sm.add_constant(X_test)\n",
    "model = sm.OLS(y_train, X_train)\n",
    "\n",
    "# Linear Regression from statsmodels\n",
    "results_lr = model.fit()\n",
    "y_test_predicted = results_lr.predict(X_test)\n",
    "y_train_predicted = results_lr.predict(X_train)\n",
    "\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_test_predicted, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_test_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Ridge from statsmodels\n",
    "results_ridge = model.fit_regularized(L1_wt=0, alpha=0.01)\n",
    "y_test_predicted = results_ridge.predict(X_test)\n",
    "y_train_predicted = results_ridge.predict(X_train)\n",
    "\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_test_predicted, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_test_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Lasso from statsmodels\n",
    "results_lasso = model.fit_regularized(L1_wt=1, alpha=0.01)\n",
    "y_test_predicted = results_lasso.predict(X_test)\n",
    "y_train_predicted = results_lasso.predict(X_train)\n",
    "\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_test_predicted, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_test_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Elastic net from statsmodels\n",
    "results_elastic = model.fit_regularized(L1_wt=0.6, alpha=0.01)\n",
    "y_test_predicted = results_elastic.predict(X_test)\n",
    "y_train_predicted = results_elastic.predict(X_train)\n",
    "\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_test_predicted, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_test_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The best quality has Linear Regression model because it has the smallest RMSE and the highest R2 score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 3. [1 point] Explore the values of the parameters of the resulting models and compare the number of zero weights in them. Comment on the significance of the coefficients, overal model significance and other related factors from the results table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "features = list(X.iloc[:, :10].columns)\n",
    "features.insert(0, \"const\")\n",
    "\n",
    "# linear regression\n",
    "results_lr.summary2(xname=features)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The values of the features are in \"coef\" column. There is no zero weights.\n",
    "Linear Regression summary:\n",
    "    We consider significance level = 0.05.\n",
    "   1. P-values of \"y\" is higher than 0.05, so we can call \"y\" coefficient insignificant\n",
    "   2. P-values of \"z\" is higher than 0.05, so we can call \"z\" coefficient insignificant\n",
    "   3. R2_adj show that ~88.5% of the variance is explained, so the model is valuable\n",
    "   4. As F-statistics ia a pretty large number, we can say that the model is significant\n",
    "   5. Durbin-Watson value is nearly 2, so there is no autocorrelation\n",
    "   6. Large Jarque-Bera value indicates that errors are not normally distributed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ridge\n",
    "OLSResults(model, results_ridge.params, model.normalized_cov_params).summary2(xname=features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The values of the features are in \"coef\" column. There is no zero weights.\n",
    "Ridge Regression summary:\n",
    "    We consider significance level = 0.05.\n",
    "   1. P-values of \"y\" is higher than 0.05, so we can call \"y\" coefficient insignificant\n",
    "   2. P-values of \"z\" is higher than 0.05, so we can call \"z\" coefficient insignificant\n",
    "   3. R2_adj show that ~88.2% of the variance is explained, so the model is valuable\n",
    "   4. As F-statistics ia a pretty large number, we can say that the model is significant\n",
    "   5. Durbin-Watson value is nearly 2, so there is no autocorrelation\n",
    "   6. Large Jarque-Bera value indicates that errors are not normally distributed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# lasso\n",
    "OLSResults(model, results_lasso.params, model.normalized_cov_params).summary2(xname=features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The values of the features are in \"coef\" column. There is no zero weights.\n",
    "Lasso Regression summary:\n",
    "    We consider significance level = 0.05.\n",
    "   1. P-values of \"y\" is higher than 0.05, so we can call \"y\" coefficient insignificant\n",
    "   2. P-values of \"z\" is higher than 0.05, so we can call \"z\" coefficient insignificant\n",
    "   3. R2_adj show that ~88.5% of the variance is explained, so the model is valuable\n",
    "   4. As F-statistics ia a pretty large number, we can say that the model is significant\n",
    "   5. Durbin-Watson value is nearly 2, so there is no autocorrelation\n",
    "   6. Large Jarque-Bera value indicates that errors are not normally distributed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# elastic\n",
    "OLSResults(model, results_elastic.params, model.normalized_cov_params).summary2(xname=features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The values of the features are in \"coef\" column. There is no zero weights.\n",
    "Elastic Net Regression summary:\n",
    "    We consider significance level = 0.05.\n",
    "   1. P-values of \"y\" is higher than 0.05, so we can call \"y\" coefficient insignificant\n",
    "   2. P-values of \"z\" is higher than 0.05, so we can call \"z\" coefficient insignificant\n",
    "   3. R2_adj show that ~88.4% of the variance is explained, so the model is valuable\n",
    "   4. As F-statistics ia a pretty large number, we can say that the model is significant\n",
    "   5. Durbin-Watson value is nearly 2, so there is no autocorrelation\n",
    "   6. Large Jarque-Bera value indicates that errors are not normally distributed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<bold> Summary: <bold>\n",
    "1. As Linear Regression model has the smallest AIC value, it should be selected.\n",
    "2. Linear Regression model also has the smallest BIC value\n",
    "3. In all models \"y\" and \"z\" are insignificant\n",
    "4. The Ridge model is the most skewed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 4. [1 point] Implement one of the elimination algorithms that were described in the Seminar_4 (Elimination by P-value, Forward elimination, Backward elimination), make conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<bold>We consider significance level = 0.05.<bold>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# p-value elimination for linear regression model\n",
    "model = sm.OLS(y_train, X_train)\n",
    "results = model.fit()\n",
    "sign_level = 0.05\n",
    "\n",
    "features = list(X.iloc[:, :10].columns)\n",
    "features.insert(0, \"const\")\n",
    "features = np.array(features)\n",
    "\n",
    "print(\"Start from model with all features:\")\n",
    "print(\"R2_adj = %.10f\" % results.rsquared_adj)\n",
    "print(\"RMSE = %.10f\" % mean_squared_error(y_train, results.predict(X_train), squared=False))\n",
    "print(\"AIC = %.10f\\n\" % results.aic)\n",
    "p_values = np.array(results.pvalues)\n",
    "X_eliminated = X_train\n",
    "\n",
    "while np.max(p_values) > sign_level:\n",
    "    insign_feature_index = np.argmax(p_values)\n",
    "    X_eliminated = np.delete(X_eliminated, insign_feature_index, axis=1)\n",
    "    print(f'Feature \\\"{features[insign_feature_index]}\\\" was eliminated.')\n",
    "    features = np.delete(features, insign_feature_index)\n",
    "    model = sm.OLS(y_train, X_eliminated)\n",
    "    results = model.fit()\n",
    "    p_values = np.array(results.pvalues)\n",
    "    print(\"Current:\\nR2_adj = %.10f\" % results.rsquared_adj)\n",
    "    print(\"RMSE = %.10f\" % mean_squared_error(y_train, results.predict(X_eliminated), squared=False))\n",
    "    print(\"AIC = %.10f\\n\" % results.aic)\n",
    "\n",
    "print(\"Features after p-value elimination algorithm:\")\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Summary: the elimination of the \"z\" increased model's quality, so we can remove it."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 5. [1 point] Find the best (in terms of RMSE) $\\alpha$ for Lasso regression using cross-validation with 4 folds. You must select values from range $[10^{-4}, 10^{3}]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "parameters = {'alpha': np.logspace(-4, 3)}\n",
    "search = GridSearchCV(Lasso(), parameters, cv=4, scoring='neg_root_mean_squared_error')\n",
    "search.fit(X_train, y_train)\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Gradient descent\n",
    "\n",
    "#### 6. [3.5 points] Implement a Ridge regression model for the MSE loss function, trained by gradient descent.\n",
    "\n",
    "All calculations must be vectorized, and python loops can only be used for gradient descent iterations. As a stop criterion, you must use (simultaneously):\n",
    "\n",
    "* checking for the Absolute-value norm of the weight difference on two adjacent iterations (for example, less than some small number of the order of $10^{-6}$, set by the `tolerance` parameter);\n",
    "* reaching the maximum number of iterations (for example, 10000, set by the `max_iter` parameter).\n",
    "\n",
    "You need to implement:\n",
    "\n",
    "* Full gradient descent:\n",
    "\n",
    "$$\n",
    "w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} Q(w_{k}).\n",
    "$$\n",
    "\n",
    "* Stochastic Gradient Descent:\n",
    "\n",
    "$$\n",
    "w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} q_{i_{k}}(w_{k}).\n",
    "$$\n",
    "\n",
    "$\\nabla_{w} q_{i_{k}}(w_{k}) \\, $ is the estimate of the gradient over the batch of objects selected randomly.\n",
    "\n",
    "* Momentum method:\n",
    "\n",
    "$$\n",
    "h_0 = 0, \\\\\n",
    "h_{k + 1} = \\alpha h_{k} + \\eta_k \\nabla_{w} Q(w_{k}), \\\\\n",
    "w_{k + 1} = w_{k} - h_{k + 1}.\n",
    "$$\n",
    "\n",
    "* Adagrad method:\n",
    "\n",
    "$$\n",
    "G_0 = 0, \\\\\n",
    "G_{k + 1} = G_{k} + (\\nabla_{w} Q(w_{k+1}))^2, \\\\\n",
    "w_{k + 1} = w_{k} - \\eta * \\frac{\\nabla_{w} Q(w_{k+1})}{\\sqrt{G_{k+1} + \\epsilon}}.\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "To make sure that the optimization process really converges, we will use the `loss_history` class attribute. After calling the `fit` method, it should contain the values of the loss function for all iterations, starting from the first one (before the first step on the anti-gradient).\n",
    "\n",
    "You need to initialize the weights with a random vector from normal distribution. The following is a template class that needs to contain the code implementing all variations of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from numpy import linalg as la\n",
    "\n",
    "\n",
    "class LinReg(BaseEstimator):\n",
    "    def __init__(self, delta=1.0, gd_type='Momentum',\n",
    "                 tolerance=1e-4, max_iter=1000, w0=None, eta=1e-2, alpha=1e-3, reg_cf=1.0, epsilon=10e-8):\n",
    "        \"\"\"\n",
    "        gd_type: str\n",
    "            'GradientDescent', 'StochasticDescent', 'Momentum', 'Adagrad'\n",
    "        delta: float\n",
    "            proportion of object in a batch (for stochastic GD)\n",
    "        tolerance: float\n",
    "            for stopping gradient descent\n",
    "        max_iter: int\n",
    "            maximum number of steps in gradient descent\n",
    "        w0: np.array of shape (d)\n",
    "            init weights\n",
    "        eta: float\n",
    "            learning rate\n",
    "        alpha: float\n",
    "            momentum coefficient\n",
    "        reg_cf: float\n",
    "            regularization coefficient\n",
    "        epsilon: float\n",
    "            numerical stability\n",
    "        \"\"\"\n",
    "\n",
    "        self.delta = delta\n",
    "        self.gd_type = gd_type\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iter = max_iter\n",
    "        self.w0 = w0\n",
    "        self.alpha = alpha\n",
    "        self.w = None\n",
    "        self.eta = eta\n",
    "        self.loss_history = None  # list of loss function values at each training iteration\n",
    "        self.epsilon = epsilon\n",
    "        self.reg_cf = reg_cf\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (l, d)\n",
    "        y: np.array of shape (l)\n",
    "        ---\n",
    "        output: self\n",
    "        \"\"\"\n",
    "        self.loss_history = []\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "        if self.gd_type == 'GradientDescent':\n",
    "            self.w = self.full_grad_descent_calc(X, y)\n",
    "        if self.gd_type == 'Momentum':\n",
    "            self.w = self.momentum_descent_calc(X, y)\n",
    "        if self.gd_type == 'Adagrad':\n",
    "            self.w = self.adagrad_descent_calc(X, y)\n",
    "        if self.gd_type == 'StochasticDescent':\n",
    "            self.w = self.stochastic_grad_descent_calc(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.w is None:\n",
    "            raise Exception('Not trained yet')\n",
    "        return np.dot(np.insert(X, 0, 1, axis=1), self.w)\n",
    "\n",
    "    def calc_gradient(self, X, y, w):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (l, d) (l can be equal to 1 if stochastic)\n",
    "        y: np.array of shape (l)\n",
    "        ---\n",
    "        output: np.array of shape (d)\n",
    "        \"\"\"\n",
    "        return 2 * np.dot(X.T, np.dot(X, w) - y) / np.shape(X)[0] + 2 * self.reg_cf * w / np.shape(X)[0]\n",
    "\n",
    "    def full_grad_descent_calc(self, X, y):\n",
    "        w = np.random.normal(0, 1, np.shape(X)[1])\n",
    "        self.w = w.copy()\n",
    "        iter_num = 0\n",
    "        while iter_num < self.max_iter and abs(la.norm(w - self.w) - self.tolerance) > 0:\n",
    "            gradient = self.calc_gradient(X, y, w)\n",
    "            self.loss_history = np.append(self.loss_history, self.calc_loss(X, y))\n",
    "            self.w = w\n",
    "            w -= self.eta * gradient\n",
    "            iter_num += 1\n",
    "        return w\n",
    "\n",
    "    def momentum_descent_calc(self, X, y):\n",
    "        w = np.random.normal(0, 1, np.shape(X)[1])\n",
    "        self.w = w.copy()\n",
    "        iter_num = 0\n",
    "        h = np.zeros(np.shape(X)[1])\n",
    "        while iter_num < self.max_iter and abs(la.norm(w - self.w) - self.tolerance) > 0:\n",
    "            gradient = self.calc_gradient(X, y, w)\n",
    "            self.loss_history = np.append(self.loss_history, self.calc_loss(X, y))\n",
    "            h = self.alpha * h + self.eta * gradient\n",
    "            self.w = w\n",
    "            w -= h\n",
    "            iter_num += 1\n",
    "        return w\n",
    "\n",
    "    def adagrad_descent_calc(self, X, y):\n",
    "        w = np.random.normal(0, 1, np.shape(X)[1])\n",
    "        self.w = w.copy()\n",
    "        iter_num = 0\n",
    "        g = np.zeros(np.shape(X)[1])\n",
    "        while iter_num < self.max_iter and abs(la.norm(w - self.w) - self.tolerance) > 0:\n",
    "            gradient = self.calc_gradient(X, y, w)\n",
    "            self.loss_history = np.append(self.loss_history, self.calc_loss(X, y))\n",
    "            g += np.square(gradient)\n",
    "            self.w = w\n",
    "            w -= self.eta * np.divide(gradient, (np.sqrt(g + self.epsilon)))\n",
    "            iter_num += 1\n",
    "        return w\n",
    "\n",
    "    def stochastic_grad_descent_calc(self, X, y):\n",
    "        np.random.seed(42)\n",
    "        w = np.random.normal(0, 1, np.shape(X)[1])\n",
    "        self.w = w.copy()\n",
    "        iter_num = 0\n",
    "        batch_size = int(np.round(np.shape(X)[0] * self.delta))\n",
    "        while iter_num < self.max_iter and abs(la.norm(w - self.w) - self.tolerance) > 0:\n",
    "            # get samples from data randomly\n",
    "            batch_indices = np.random.randint(X.shape[0], size=batch_size)\n",
    "            X_batch = X[batch_indices, :]\n",
    "            y_batch = y.iloc[batch_indices]\n",
    "            gradient = self.calc_gradient(X_batch, y_batch, w)\n",
    "            self.w = w\n",
    "            w -= self.eta * gradient\n",
    "\n",
    "            # if we want to run code with epochs (than max_iter = number of epochs)\n",
    "            # for start in range(0, np.shape(X)[0], batch_size):\n",
    "            #     stop = start + batch_size\n",
    "            #     X_batch = X[start:stop]\n",
    "            #     y_batch = y[start:stop]\n",
    "            #     gradient = self.calc_gradient(X_batch, y_batch, w)\n",
    "            #     self.w = w\n",
    "            #     w -= self.eta * gradient\n",
    "\n",
    "            self.loss_history = np.append(self.loss_history, self.calc_loss(X, y))\n",
    "            iter_num += 1\n",
    "        return w\n",
    "\n",
    "    def calc_loss(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (l, d)\n",
    "        y: np.array of shape (l)\n",
    "        ---\n",
    "        output: float \n",
    "        \"\"\"\n",
    "        return np.sum(np.square(np.dot(X, self.w) - y)) / np.shape(X)[0] + self.reg_cf * np.sum(np.square(self.w)) / np.shape(X)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 7. [1 points] Train and validate \"hand-written\" models on the same data, and compare the quality with the Sklearn or StatsModels methods. Investigate the effect of the `max_iter` and `alpha` parameters on the optimization process. Is it consistent with your expectations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(encoded_data, y, test_size=0.2, random_state=17)\n",
    "# scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "# Full gradient descent\n",
    "model_full_gr = LinReg(gd_type='GradientDescent')\n",
    "model_full_gr.fit(X_train, y_train)\n",
    "y_train_predicted = model_full_gr.predict(X_train)\n",
    "y_test_predicted = model_full_gr.predict(X_test)\n",
    "print(\"Full gradient descent (max 1000 iterations, learning rate = 0.01): \")\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_test_predicted, squared=False))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_test_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_full_gr_iter_test = LinReg(gd_type='GradientDescent', max_iter=500)\n",
    "model_full_gr_iter_test.fit(X_train, y_train)\n",
    "y_train_predicted = model_full_gr_iter_test.predict(X_train)\n",
    "y_test_predicted = model_full_gr_iter_test.predict(X_test)\n",
    "print(\"Full gradient descent (max 500 iterations): \")\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_test_predicted, squared=False))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_test_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_full_gr_iter_test = LinReg(gd_type='GradientDescent', eta=0.001)\n",
    "model_full_gr_iter_test.fit(X_train, y_train)\n",
    "y_train_predicted = model_full_gr_iter_test.predict(X_train)\n",
    "y_test_predicted = model_full_gr_iter_test.predict(X_test)\n",
    "print(\"Full gradient descent (max 1000 iterations, learning rate = 0.001): \")\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_test_predicted, squared=False))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_test_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Summary:\n",
    "1. При увеличении max_iter качество модели улучшается, при уменьшении max_iter качество модели ухудшается."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Momentum\n",
    "model_momentum = LinReg(gd_type='Momentum')\n",
    "model_momentum.fit(X_train, y_train)\n",
    "y_train_predicted = model_momentum.predict(X_train)\n",
    "y_test_predicted = model_momentum.predict(X_test)\n",
    "print(\"Momentum: \")\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_test_predicted, squared=False))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_test_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Stochastic\n",
    "model_stochastic = LinReg(gd_type='StochasticDescent', delta=0.5)\n",
    "model_stochastic.fit(X_train, y_train)\n",
    "y_train_predicted = model_stochastic.predict(X_train)\n",
    "y_test_predicted = model_stochastic.predict(X_test)\n",
    "print(\"Stochastic: \")\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_test_predicted, squared=False))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_test_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Adagrad\n",
    "model_adagrad = LinReg(gd_type='Adagrad')\n",
    "model_adagrad.fit(X_train, y_train)\n",
    "y_train_predicted = model_adagrad.predict(X_train)\n",
    "y_test_predicted = model_adagrad.predict(X_test)\n",
    "print(\"Adagrad: \")\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_test_predicted, squared=False))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_test_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Sklearn model\n",
    "model = Ridge()\n",
    "model.fit(X_train, y_train)\n",
    "y_train_predicted = model.predict(X_train)\n",
    "y_test_predicted = model.predict(X_test)\n",
    "print(\"Ridge from Sklearn: \")\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_test_predicted, squared=False))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_test_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<bold>Comparing hand-written models with model from Sklearn:<bold>\n",
    "Quality of Sklearn model is a little bit better than the hand-written one, because:\n",
    "   1. Sklearn model's RMSE is less than others.\n",
    "   2. R2 is higher than R2 of other models.\n",
    "\n",
    "But hand-written models still have good quality. Sklearn model explained ~89% of the data (R2). In the same time hand-written models explained ~87% of the data, which is very close to library model. Same about RMSE values.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 8. [1 points] Plot graphs (on the same picture) of the dependence of the loss function value on the iteration number for Full GD, SGD, Momentum and Adagrad. Draw conclusions about the rate of convergence of various modifications of gradient descent.\n",
    "\n",
    "Don't forget about what *beautiful* graphics should look like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(10,10))\n",
    "plt.plot(model_full_gr.loss_history)\n",
    "plt.plot(model_momentum.loss_history)\n",
    "plt.plot(model_adagrad.loss_history)\n",
    "plt.plot(model_stochastic.loss_history)\n",
    "plt.legend([\"Full Gradient Descent\", \"Momentum\", \"Adagrad\", \"Stochastic GD\"])\n",
    "plt.xlabel(\"iteration number\")\n",
    "plt.ylabel(\"loss function value\")\n",
    "plt.title(\"Loss function value and iteration number dependence\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}