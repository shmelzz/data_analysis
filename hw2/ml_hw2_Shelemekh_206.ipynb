{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# HSE 2022: Mathematical Methods for Data Analysis\n",
    "\n",
    "## Homework 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-26T16:48:20.566549Z",
     "start_time": "2020-09-26T16:48:19.893995Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.regression.linear_model import OLSResults\n",
    "from math import sqrt\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Data\n",
    "\n",
    "For this homework we use Dataset from seaborn on diamonds prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = sns.load_dataset('diamonds')\n",
    "y = data.price\n",
    "X = data.drop(['price'], axis=1)\n",
    "columns = data.drop(['price'], axis=1).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 0. [0.25 points] Encode categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "categories = (data.dtypes == \"category\")\n",
    "object_cols = list(categories[categories].index)\n",
    "\n",
    "encoded_data = X.copy()\n",
    "label_encoder = LabelEncoder()\n",
    "for col in object_cols:\n",
    "    encoded_data[col] = label_encoder.fit_transform(encoded_data[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 1. [0.25 points] Split the data into train and test sets with ratio 80:20 with random_state=17."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(encoded_data, y, test_size=0.2, random_state=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2. [1 point] Train models on train data using StatsModels library and apply it to the test set; use $RMSE$ and $R^2$ as the quality measure.\n",
    "\n",
    "* [`LinearRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html);\n",
    "* [`Ridge`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) with $\\alpha = 0.01$;\n",
    "* [`Lasso`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) with $\\alpha = 0.01$\n",
    "* [`ElasticNet`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html) with $\\alpha = 0.01$, $l_{1}$_$ratio = 0.6$\n",
    "\n",
    "Don't forget to scale the data before training the models with StandardScaler!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE = 1347.9933\n",
      "Test RMSE = 1370.9682\n",
      "Train R2 = 0.8853\n",
      "Test R2 = 0.8839\n"
     ]
    }
   ],
   "source": [
    "# for statsmodels\n",
    "X_train = sm.add_constant(X_train)\n",
    "X_test = sm.add_constant(X_test)\n",
    "model = sm.OLS(y_train, X_train)\n",
    "\n",
    "# Linear Regression from statsmodels\n",
    "results_lr = model.fit()\n",
    "y_test_predicted = results_lr.predict(X_test)\n",
    "y_train_predicted = results_lr.predict(X_train)\n",
    "\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_test_predicted, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_test_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE = 1365.9920\n",
      "Test RMSE = 1383.9541\n",
      "Train R2 = 0.8822\n",
      "Test R2 = 0.8817\n"
     ]
    }
   ],
   "source": [
    "# Ridge from statsmodels\n",
    "results_ridge = model.fit_regularized(L1_wt=0, alpha=0.01)\n",
    "y_test_predicted = results_ridge.predict(X_test)\n",
    "y_train_predicted = results_ridge.predict(X_train)\n",
    "\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_test_predicted, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_test_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE = 1348.6037\n",
      "Test RMSE = 1370.0240\n",
      "Train R2 = 0.8852\n",
      "Test R2 = 0.8841\n"
     ]
    }
   ],
   "source": [
    "# Lasso from statsmodels\n",
    "results_lasso = model.fit_regularized(L1_wt=1, alpha=0.01)\n",
    "y_test_predicted = results_lasso.predict(X_test)\n",
    "y_train_predicted = results_lasso.predict(X_train)\n",
    "\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_test_predicted, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_test_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE = 1353.6044\n",
      "Test RMSE = 1372.8616\n",
      "Train R2 = 0.8844\n",
      "Test R2 = 0.8836\n"
     ]
    }
   ],
   "source": [
    "# Elastic net from statsmodels\n",
    "results_elastic = model.fit_regularized(L1_wt=0.6, alpha=0.01)\n",
    "y_test_predicted = results_elastic.predict(X_test)\n",
    "y_train_predicted = results_elastic.predict(X_train)\n",
    "\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_test_predicted, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_test_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The best quality has Linear Regression model because it has the smallest RMSE and the highest R2 score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 3. [1 point] Explore the values of the parameters of the resulting models and compare the number of zero weights in them. Comment on the significance of the coefficients, overal model significance and other related factors from the results table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<class 'statsmodels.iolib.summary2.Summary'>\n\"\"\"\n                  Results: Ordinary least squares\n====================================================================\nModel:              OLS              Adj. R-squared:     0.885      \nDependent Variable: price            AIC:                744418.8263\nDate:               2022-10-16 17:58 BIC:                744505.5512\nNo. Observations:   43152            Log-Likelihood:     -3.7220e+05\nDf Model:           9                F-statistic:        3.701e+04  \nDf Residuals:       43142            Prob (F-statistic): 0.00       \nR-squared:          0.885            Scale:              1.8175e+06 \n--------------------------------------------------------------------\n             Coef.    Std.Err.    t     P>|t|    [0.025     0.975]  \n--------------------------------------------------------------------\nconst       3928.6813   6.4899 605.3537 0.0000  3915.9610  3941.4016\ncarat       5257.1453  31.0710 169.1979 0.0000  5196.2456  5318.0450\ncut           76.4610   6.6590  11.4824 0.0000    63.4093    89.5128\ncolor       -455.4350   6.8100 -66.8778 0.0000  -468.7826  -442.0874\nclarity      491.4240   6.7022  73.3231 0.0000   478.2876   504.5604\ndepth       -226.2704   7.9533 -28.4498 0.0000  -241.8590  -210.6818\ntable       -213.2612   6.9923 -30.4996 0.0000  -226.9662  -199.5562\nx          -1383.2878  48.3537 -28.6077 0.0000 -1478.0619 -1288.5137\ny             42.1665  29.5137   1.4287 0.1531   -15.6809   100.0138\nz              3.3540  29.6459   0.1131 0.9099   -54.7524    61.4604\n--------------------------------------------------------------------\nOmnibus:            11265.146      Durbin-Watson:         1.999     \nProb(Omnibus):      0.000          Jarque-Bera (JB):      360275.892\nSkew:               0.611          Prob(JB):              0.000     \nKurtosis:           17.103         Condition No.:         18        \n====================================================================\n\n\"\"\"",
      "text/html": "<table class=\"simpletable\">\n<tr>\n        <td>Model:</td>               <td>OLS</td>         <td>Adj. R-squared:</td>      <td>0.885</td>   \n</tr>\n<tr>\n  <td>Dependent Variable:</td>       <td>price</td>             <td>AIC:</td>         <td>744418.8263</td>\n</tr>\n<tr>\n         <td>Date:</td>        <td>2022-10-16 17:58</td>        <td>BIC:</td>         <td>744505.5512</td>\n</tr>\n<tr>\n   <td>No. Observations:</td>        <td>43152</td>        <td>Log-Likelihood:</td>   <td>-3.7220e+05</td>\n</tr>\n<tr>\n       <td>Df Model:</td>              <td>9</td>           <td>F-statistic:</td>      <td>3.701e+04</td> \n</tr>\n<tr>\n     <td>Df Residuals:</td>          <td>43142</td>      <td>Prob (F-statistic):</td>    <td>0.00</td>    \n</tr>\n<tr>\n      <td>R-squared:</td>            <td>0.885</td>            <td>Scale:</td>        <td>1.8175e+06</td> \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n     <td></td>        <th>Coef.</th>   <th>Std.Err.</th>     <th>t</th>     <th>P>|t|</th>   <th>[0.025</th>     <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th>    <td>3928.6813</td>  <td>6.4899</td>  <td>605.3537</td> <td>0.0000</td>  <td>3915.9610</td>  <td>3941.4016</td>\n</tr>\n<tr>\n  <th>carat</th>    <td>5257.1453</td>  <td>31.0710</td> <td>169.1979</td> <td>0.0000</td>  <td>5196.2456</td>  <td>5318.0450</td>\n</tr>\n<tr>\n  <th>cut</th>       <td>76.4610</td>   <td>6.6590</td>   <td>11.4824</td> <td>0.0000</td>   <td>63.4093</td>    <td>89.5128</td> \n</tr>\n<tr>\n  <th>color</th>    <td>-455.4350</td>  <td>6.8100</td>  <td>-66.8778</td> <td>0.0000</td>  <td>-468.7826</td>  <td>-442.0874</td>\n</tr>\n<tr>\n  <th>clarity</th>  <td>491.4240</td>   <td>6.7022</td>   <td>73.3231</td> <td>0.0000</td>  <td>478.2876</td>   <td>504.5604</td> \n</tr>\n<tr>\n  <th>depth</th>    <td>-226.2704</td>  <td>7.9533</td>  <td>-28.4498</td> <td>0.0000</td>  <td>-241.8590</td>  <td>-210.6818</td>\n</tr>\n<tr>\n  <th>table</th>    <td>-213.2612</td>  <td>6.9923</td>  <td>-30.4996</td> <td>0.0000</td>  <td>-226.9662</td>  <td>-199.5562</td>\n</tr>\n<tr>\n  <th>x</th>       <td>-1383.2878</td>  <td>48.3537</td> <td>-28.6077</td> <td>0.0000</td> <td>-1478.0619</td> <td>-1288.5137</td>\n</tr>\n<tr>\n  <th>y</th>         <td>42.1665</td>   <td>29.5137</td>  <td>1.4287</td>  <td>0.1531</td>  <td>-15.6809</td>   <td>100.0138</td> \n</tr>\n<tr>\n  <th>z</th>         <td>3.3540</td>    <td>29.6459</td>  <td>0.1131</td>  <td>0.9099</td>  <td>-54.7524</td>    <td>61.4604</td> \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n     <td>Omnibus:</td>    <td>11265.146</td>  <td>Durbin-Watson:</td>      <td>1.999</td>  \n</tr>\n<tr>\n  <td>Prob(Omnibus):</td>   <td>0.000</td>   <td>Jarque-Bera (JB):</td> <td>360275.892</td>\n</tr>\n<tr>\n       <td>Skew:</td>       <td>0.611</td>       <td>Prob(JB):</td>        <td>0.000</td>  \n</tr>\n<tr>\n     <td>Kurtosis:</td>    <td>17.103</td>    <td>Condition No.:</td>       <td>18</td>    \n</tr>\n</table>"
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = list(X.iloc[:, :10].columns)\n",
    "features.insert(0, \"const\")\n",
    "\n",
    "# linear regression\n",
    "results_lr.summary2(xname=features)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The values of the features are in \"coef\" column. There is no zero weights.\n",
    "Linear Regression summary:\n",
    "    We consider significance level = 0.05.\n",
    "   1. P-values of \"y\" is higher than 0.05, so we can call \"y\" coefficient insignificant\n",
    "   2. P-values of \"z\" is higher than 0.05, so we can call \"z\" coefficient insignificant\n",
    "   3. R2_adj show that ~88.5% of the variance is explained, so the model is valuable\n",
    "   4. As F-statistics ia a pretty large number, we can say that the model is significant\n",
    "   5. Durbin-Watson value is nearly 2, so there is no autocorrelation\n",
    "   6. Large Jarque-Bera value indicates that errors are not normally distributed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "data": {
      "text/plain": "<class 'statsmodels.iolib.summary2.Summary'>\n\"\"\"\n                  Results: Ordinary least squares\n====================================================================\nModel:              OLS              Adj. R-squared:     0.882      \nDependent Variable: price            AIC:                745563.5541\nDate:               2022-10-16 17:58 BIC:                745650.2790\nNo. Observations:   43152            Log-Likelihood:     -3.7277e+05\nDf Model:           9                F-statistic:        3.591e+04  \nDf Residuals:       43142            Prob (F-statistic): 0.00       \nR-squared:          0.882            Scale:              1.8664e+06 \n---------------------------------------------------------------------\n            Coef.    Std.Err.     t      P>|t|     [0.025     0.975] \n---------------------------------------------------------------------\nconst     3889.7835    6.5765  591.4627  0.0000  3876.8933  3902.6736\ncarat     4219.7372   31.4858  134.0201  0.0000  4158.0243  4281.4500\ncut         81.8556    6.7479   12.1306  0.0000    68.6296    95.0816\ncolor     -424.7483    6.9009  -61.5498  0.0000  -438.2742  -411.2225\nclarity    496.6610    6.7917   73.1280  0.0000   483.3492   509.9728\ndepth     -162.6547    8.0595  -20.1817  0.0000  -178.4514  -146.8579\ntable     -203.6562    7.0856  -28.7421  0.0000  -217.5442  -189.7682\nx         -312.9137   48.9993   -6.3861  0.0000  -408.9533  -216.8741\ny           21.4162   29.9077    0.7161  0.4739   -37.2035    80.0359\nz          -39.9802   30.0417   -1.3308  0.1833   -98.8625    18.9021\n--------------------------------------------------------------------\nOmnibus:            13066.102      Durbin-Watson:         1.998     \nProb(Omnibus):      0.000          Jarque-Bera (JB):      183804.270\nSkew:               1.067          Prob(JB):              0.000     \nKurtosis:           12.883         Condition No.:         18        \n====================================================================\n\n\"\"\"",
      "text/html": "<table class=\"simpletable\">\n<tr>\n        <td>Model:</td>               <td>OLS</td>         <td>Adj. R-squared:</td>      <td>0.882</td>   \n</tr>\n<tr>\n  <td>Dependent Variable:</td>       <td>price</td>             <td>AIC:</td>         <td>745563.5541</td>\n</tr>\n<tr>\n         <td>Date:</td>        <td>2022-10-16 17:58</td>        <td>BIC:</td>         <td>745650.2790</td>\n</tr>\n<tr>\n   <td>No. Observations:</td>        <td>43152</td>        <td>Log-Likelihood:</td>   <td>-3.7277e+05</td>\n</tr>\n<tr>\n       <td>Df Model:</td>              <td>9</td>           <td>F-statistic:</td>      <td>3.591e+04</td> \n</tr>\n<tr>\n     <td>Df Residuals:</td>          <td>43142</td>      <td>Prob (F-statistic):</td>    <td>0.00</td>    \n</tr>\n<tr>\n      <td>R-squared:</td>            <td>0.882</td>            <td>Scale:</td>        <td>1.8664e+06</td> \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n     <td></td>       <th>Coef.</th>   <th>Std.Err.</th>     <th>t</th>     <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th>   <td>3889.7835</td>  <td>6.5765</td>  <td>591.4627</td> <td>0.0000</td> <td>3876.8933</td> <td>3902.6736</td>\n</tr>\n<tr>\n  <th>carat</th>   <td>4219.7372</td>  <td>31.4858</td> <td>134.0201</td> <td>0.0000</td> <td>4158.0243</td> <td>4281.4500</td>\n</tr>\n<tr>\n  <th>cut</th>      <td>81.8556</td>   <td>6.7479</td>   <td>12.1306</td> <td>0.0000</td>  <td>68.6296</td>   <td>95.0816</td> \n</tr>\n<tr>\n  <th>color</th>   <td>-424.7483</td>  <td>6.9009</td>  <td>-61.5498</td> <td>0.0000</td> <td>-438.2742</td> <td>-411.2225</td>\n</tr>\n<tr>\n  <th>clarity</th> <td>496.6610</td>   <td>6.7917</td>   <td>73.1280</td> <td>0.0000</td> <td>483.3492</td>  <td>509.9728</td> \n</tr>\n<tr>\n  <th>depth</th>   <td>-162.6547</td>  <td>8.0595</td>  <td>-20.1817</td> <td>0.0000</td> <td>-178.4514</td> <td>-146.8579</td>\n</tr>\n<tr>\n  <th>table</th>   <td>-203.6562</td>  <td>7.0856</td>  <td>-28.7421</td> <td>0.0000</td> <td>-217.5442</td> <td>-189.7682</td>\n</tr>\n<tr>\n  <th>x</th>       <td>-312.9137</td>  <td>48.9993</td>  <td>-6.3861</td> <td>0.0000</td> <td>-408.9533</td> <td>-216.8741</td>\n</tr>\n<tr>\n  <th>y</th>        <td>21.4162</td>   <td>29.9077</td>  <td>0.7161</td>  <td>0.4739</td> <td>-37.2035</td>   <td>80.0359</td> \n</tr>\n<tr>\n  <th>z</th>       <td>-39.9802</td>   <td>30.0417</td>  <td>-1.3308</td> <td>0.1833</td> <td>-98.8625</td>   <td>18.9021</td> \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n     <td>Omnibus:</td>    <td>13066.102</td>  <td>Durbin-Watson:</td>      <td>1.998</td>  \n</tr>\n<tr>\n  <td>Prob(Omnibus):</td>   <td>0.000</td>   <td>Jarque-Bera (JB):</td> <td>183804.270</td>\n</tr>\n<tr>\n       <td>Skew:</td>       <td>1.067</td>       <td>Prob(JB):</td>        <td>0.000</td>  \n</tr>\n<tr>\n     <td>Kurtosis:</td>    <td>12.883</td>    <td>Condition No.:</td>       <td>18</td>    \n</tr>\n</table>"
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ridge\n",
    "OLSResults(model, results_ridge.params, model.normalized_cov_params).summary2(xname=features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The values of the features are in \"coef\" column. There is no zero weights.\n",
    "Ridge Regression summary:\n",
    "    We consider significance level = 0.05.\n",
    "   1. P-values of \"y\" is higher than 0.05, so we can call \"y\" coefficient insignificant\n",
    "   2. P-values of \"z\" is higher than 0.05, so we can call \"z\" coefficient insignificant\n",
    "   3. R2_adj show that ~88.2% of the variance is explained, so the model is valuable\n",
    "   4. As F-statistics ia a pretty large number, we can say that the model is significant\n",
    "   5. Durbin-Watson value is nearly 2, so there is no autocorrelation\n",
    "   6. Large Jarque-Bera value indicates that errors are not normally distributed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "data": {
      "text/plain": "<class 'statsmodels.iolib.summary2.Summary'>\n\"\"\"\n                  Results: Ordinary least squares\n====================================================================\nModel:              OLS              Adj. R-squared:     0.885      \nDependent Variable: price            AIC:                744457.9003\nDate:               2022-10-16 17:58 BIC:                744544.6252\nNo. Observations:   43152            Log-Likelihood:     -3.7222e+05\nDf Model:           9                F-statistic:        3.697e+04  \nDf Residuals:       43142            Prob (F-statistic): 0.00       \nR-squared:          0.885            Scale:              1.8192e+06 \n--------------------------------------------------------------------\n             Coef.    Std.Err.    t     P>|t|    [0.025     0.975]  \n--------------------------------------------------------------------\nconst       3928.6713   6.4928 605.0782 0.0000  3915.9452  3941.3974\ncarat       5063.8698  31.0850 162.9037 0.0000  5002.9425  5124.7970\ncut           77.5642   6.6620  11.6428 0.0000    64.5066    90.6218\ncolor       -453.1266   6.8130 -66.5087 0.0000  -466.4802  -439.7729\nclarity      495.2762   6.7052  73.8644 0.0000   482.1339   508.4186\ndepth       -214.9442   7.9569 -27.0135 0.0000  -230.5399  -199.3485\ntable       -213.4343   6.9954 -30.5105 0.0000  -227.1455  -199.7231\nx          -1201.4586  48.3756 -24.8361 0.0000 -1296.2757 -1106.6416\ny             54.7441  29.5270   1.8540 0.0637    -3.1294   112.6176\nz             -1.4625  29.6593  -0.0493 0.9607   -59.5953    56.6702\n--------------------------------------------------------------------\nOmnibus:            11656.263      Durbin-Watson:         1.999     \nProb(Omnibus):      0.000          Jarque-Bera (JB):      315053.588\nSkew:               0.713          Prob(JB):              0.000     \nKurtosis:           16.160         Condition No.:         18        \n====================================================================\n\n\"\"\"",
      "text/html": "<table class=\"simpletable\">\n<tr>\n        <td>Model:</td>               <td>OLS</td>         <td>Adj. R-squared:</td>      <td>0.885</td>   \n</tr>\n<tr>\n  <td>Dependent Variable:</td>       <td>price</td>             <td>AIC:</td>         <td>744457.9003</td>\n</tr>\n<tr>\n         <td>Date:</td>        <td>2022-10-16 17:58</td>        <td>BIC:</td>         <td>744544.6252</td>\n</tr>\n<tr>\n   <td>No. Observations:</td>        <td>43152</td>        <td>Log-Likelihood:</td>   <td>-3.7222e+05</td>\n</tr>\n<tr>\n       <td>Df Model:</td>              <td>9</td>           <td>F-statistic:</td>      <td>3.697e+04</td> \n</tr>\n<tr>\n     <td>Df Residuals:</td>          <td>43142</td>      <td>Prob (F-statistic):</td>    <td>0.00</td>    \n</tr>\n<tr>\n      <td>R-squared:</td>            <td>0.885</td>            <td>Scale:</td>        <td>1.8192e+06</td> \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n     <td></td>        <th>Coef.</th>   <th>Std.Err.</th>     <th>t</th>     <th>P>|t|</th>   <th>[0.025</th>     <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th>    <td>3928.6713</td>  <td>6.4928</td>  <td>605.0782</td> <td>0.0000</td>  <td>3915.9452</td>  <td>3941.3974</td>\n</tr>\n<tr>\n  <th>carat</th>    <td>5063.8698</td>  <td>31.0850</td> <td>162.9037</td> <td>0.0000</td>  <td>5002.9425</td>  <td>5124.7970</td>\n</tr>\n<tr>\n  <th>cut</th>       <td>77.5642</td>   <td>6.6620</td>   <td>11.6428</td> <td>0.0000</td>   <td>64.5066</td>    <td>90.6218</td> \n</tr>\n<tr>\n  <th>color</th>    <td>-453.1266</td>  <td>6.8130</td>  <td>-66.5087</td> <td>0.0000</td>  <td>-466.4802</td>  <td>-439.7729</td>\n</tr>\n<tr>\n  <th>clarity</th>  <td>495.2762</td>   <td>6.7052</td>   <td>73.8644</td> <td>0.0000</td>  <td>482.1339</td>   <td>508.4186</td> \n</tr>\n<tr>\n  <th>depth</th>    <td>-214.9442</td>  <td>7.9569</td>  <td>-27.0135</td> <td>0.0000</td>  <td>-230.5399</td>  <td>-199.3485</td>\n</tr>\n<tr>\n  <th>table</th>    <td>-213.4343</td>  <td>6.9954</td>  <td>-30.5105</td> <td>0.0000</td>  <td>-227.1455</td>  <td>-199.7231</td>\n</tr>\n<tr>\n  <th>x</th>       <td>-1201.4586</td>  <td>48.3756</td> <td>-24.8361</td> <td>0.0000</td> <td>-1296.2757</td> <td>-1106.6416</td>\n</tr>\n<tr>\n  <th>y</th>         <td>54.7441</td>   <td>29.5270</td>  <td>1.8540</td>  <td>0.0637</td>   <td>-3.1294</td>   <td>112.6176</td> \n</tr>\n<tr>\n  <th>z</th>         <td>-1.4625</td>   <td>29.6593</td>  <td>-0.0493</td> <td>0.9607</td>  <td>-59.5953</td>    <td>56.6702</td> \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n     <td>Omnibus:</td>    <td>11656.263</td>  <td>Durbin-Watson:</td>      <td>1.999</td>  \n</tr>\n<tr>\n  <td>Prob(Omnibus):</td>   <td>0.000</td>   <td>Jarque-Bera (JB):</td> <td>315053.588</td>\n</tr>\n<tr>\n       <td>Skew:</td>       <td>0.713</td>       <td>Prob(JB):</td>        <td>0.000</td>  \n</tr>\n<tr>\n     <td>Kurtosis:</td>    <td>16.160</td>    <td>Condition No.:</td>       <td>18</td>    \n</tr>\n</table>"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lasso\n",
    "OLSResults(model, results_lasso.params, model.normalized_cov_params).summary2(xname=features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The values of the features are in \"coef\" column. There is no zero weights.\n",
    "Lasso Regression summary:\n",
    "    We consider significance level = 0.05.\n",
    "   1. P-values of \"y\" is higher than 0.05, so we can call \"y\" coefficient insignificant\n",
    "   2. P-values of \"z\" is higher than 0.05, so we can call \"z\" coefficient insignificant\n",
    "   3. R2_adj show that ~88.5% of the variance is explained, so the model is valuable\n",
    "   4. As F-statistics ia a pretty large number, we can say that the model is significant\n",
    "   5. Durbin-Watson value is nearly 2, so there is no autocorrelation\n",
    "   6. Large Jarque-Bera value indicates that errors are not normally distributed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "data": {
      "text/plain": "<class 'statsmodels.iolib.summary2.Summary'>\n\"\"\"\n                  Results: Ordinary least squares\n====================================================================\nModel:              OLS              Adj. R-squared:     0.884      \nDependent Variable: price            AIC:                744777.3304\nDate:               2022-10-16 17:58 BIC:                744864.0552\nNo. Observations:   43152            Log-Likelihood:     -3.7238e+05\nDf Model:           9                F-statistic:        3.666e+04  \nDf Residuals:       43142            Prob (F-statistic): 0.00       \nR-squared:          0.884            Scale:              1.8327e+06 \n---------------------------------------------------------------------\n            Coef.    Std.Err.     t      P>|t|     [0.025     0.975] \n---------------------------------------------------------------------\nconst     3913.0232    6.5169  600.4416  0.0000  3900.2499  3925.7965\ncarat     4675.8946   31.2003  149.8669  0.0000  4614.7414  4737.0478\ncut         79.7319    6.6867   11.9240  0.0000    66.6258    92.8379\ncolor     -440.5288    6.8383  -64.4208  0.0000  -453.9320  -427.1256\nclarity    496.4432    6.7301   73.7649  0.0000   483.2521   509.6342\ndepth     -190.1759    7.9864  -23.8124  0.0000  -205.8294  -174.5223\ntable     -209.3948    7.0214  -29.8225  0.0000  -223.1568  -195.6328\nx         -766.0692   48.5550  -15.7774  0.0000  -861.2378  -670.9005\ny           20.1115   29.6365    0.6786  0.4974   -37.9767    78.1996\nz          -26.8075   29.7693   -0.9005  0.3679   -85.1558    31.5408\n--------------------------------------------------------------------\nOmnibus:            12406.161      Durbin-Watson:         1.999     \nProb(Omnibus):      0.000          Jarque-Bera (JB):      244618.648\nSkew:               0.894          Prob(JB):              0.000     \nKurtosis:           14.526         Condition No.:         18        \n====================================================================\n\n\"\"\"",
      "text/html": "<table class=\"simpletable\">\n<tr>\n        <td>Model:</td>               <td>OLS</td>         <td>Adj. R-squared:</td>      <td>0.884</td>   \n</tr>\n<tr>\n  <td>Dependent Variable:</td>       <td>price</td>             <td>AIC:</td>         <td>744777.3304</td>\n</tr>\n<tr>\n         <td>Date:</td>        <td>2022-10-16 17:58</td>        <td>BIC:</td>         <td>744864.0552</td>\n</tr>\n<tr>\n   <td>No. Observations:</td>        <td>43152</td>        <td>Log-Likelihood:</td>   <td>-3.7238e+05</td>\n</tr>\n<tr>\n       <td>Df Model:</td>              <td>9</td>           <td>F-statistic:</td>      <td>3.666e+04</td> \n</tr>\n<tr>\n     <td>Df Residuals:</td>          <td>43142</td>      <td>Prob (F-statistic):</td>    <td>0.00</td>    \n</tr>\n<tr>\n      <td>R-squared:</td>            <td>0.884</td>            <td>Scale:</td>        <td>1.8327e+06</td> \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n     <td></td>       <th>Coef.</th>   <th>Std.Err.</th>     <th>t</th>     <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th>   <td>3913.0232</td>  <td>6.5169</td>  <td>600.4416</td> <td>0.0000</td> <td>3900.2499</td> <td>3925.7965</td>\n</tr>\n<tr>\n  <th>carat</th>   <td>4675.8946</td>  <td>31.2003</td> <td>149.8669</td> <td>0.0000</td> <td>4614.7414</td> <td>4737.0478</td>\n</tr>\n<tr>\n  <th>cut</th>      <td>79.7319</td>   <td>6.6867</td>   <td>11.9240</td> <td>0.0000</td>  <td>66.6258</td>   <td>92.8379</td> \n</tr>\n<tr>\n  <th>color</th>   <td>-440.5288</td>  <td>6.8383</td>  <td>-64.4208</td> <td>0.0000</td> <td>-453.9320</td> <td>-427.1256</td>\n</tr>\n<tr>\n  <th>clarity</th> <td>496.4432</td>   <td>6.7301</td>   <td>73.7649</td> <td>0.0000</td> <td>483.2521</td>  <td>509.6342</td> \n</tr>\n<tr>\n  <th>depth</th>   <td>-190.1759</td>  <td>7.9864</td>  <td>-23.8124</td> <td>0.0000</td> <td>-205.8294</td> <td>-174.5223</td>\n</tr>\n<tr>\n  <th>table</th>   <td>-209.3948</td>  <td>7.0214</td>  <td>-29.8225</td> <td>0.0000</td> <td>-223.1568</td> <td>-195.6328</td>\n</tr>\n<tr>\n  <th>x</th>       <td>-766.0692</td>  <td>48.5550</td> <td>-15.7774</td> <td>0.0000</td> <td>-861.2378</td> <td>-670.9005</td>\n</tr>\n<tr>\n  <th>y</th>        <td>20.1115</td>   <td>29.6365</td>  <td>0.6786</td>  <td>0.4974</td> <td>-37.9767</td>   <td>78.1996</td> \n</tr>\n<tr>\n  <th>z</th>       <td>-26.8075</td>   <td>29.7693</td>  <td>-0.9005</td> <td>0.3679</td> <td>-85.1558</td>   <td>31.5408</td> \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n     <td>Omnibus:</td>    <td>12406.161</td>  <td>Durbin-Watson:</td>      <td>1.999</td>  \n</tr>\n<tr>\n  <td>Prob(Omnibus):</td>   <td>0.000</td>   <td>Jarque-Bera (JB):</td> <td>244618.648</td>\n</tr>\n<tr>\n       <td>Skew:</td>       <td>0.894</td>       <td>Prob(JB):</td>        <td>0.000</td>  \n</tr>\n<tr>\n     <td>Kurtosis:</td>    <td>14.526</td>    <td>Condition No.:</td>       <td>18</td>    \n</tr>\n</table>"
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# elastic\n",
    "OLSResults(model, results_elastic.params, model.normalized_cov_params).summary2(xname=features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The values of the features are in \"coef\" column. There is no zero weights.\n",
    "Elastic Net Regression summary:\n",
    "    We consider significance level = 0.05.\n",
    "   1. P-values of \"y\" is higher than 0.05, so we can call \"y\" coefficient insignificant\n",
    "   2. P-values of \"z\" is higher than 0.05, so we can call \"z\" coefficient insignificant\n",
    "   3. R2_adj show that ~88.4% of the variance is explained, so the model is valuable\n",
    "   4. As F-statistics ia a pretty large number, we can say that the model is significant\n",
    "   5. Durbin-Watson value is nearly 2, so there is no autocorrelation\n",
    "   6. Large Jarque-Bera value indicates that errors are not normally distributed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<bold> Summary: <bold>\n",
    "1. As Linear Regression model has the smallest AIC value, it should be selected.\n",
    "2. Linear Regression model also has the smallest BIC value\n",
    "3. In all models \"y\" and \"z\" are insignificant\n",
    "4. The Ridge model is the most skewed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 4. [1 point] Implement one of the elimination algorithms that were described in the Seminar_4 (Elimination by P-value, Forward elimination, Backward elimination), make conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<bold>We consider significance level = 0.05.<bold>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start from model with all features:\n",
      "R2_adj = 0.8853021693\n",
      "RMSE = 1347.9932745208\n",
      "AIC = 744418.8263415196\n",
      "\n",
      "Feature \"z\" was eliminated.\n",
      "Current:\n",
      "R2_adj = 0.8853047938\n",
      "RMSE = 1347.9934744846\n",
      "AIC = 744416.8391440106\n",
      "\n",
      "Feature \"y\" was eliminated.\n",
      "Current:\n",
      "R2_adj = 0.8853018368\n",
      "RMSE = 1348.0264734788\n",
      "AIC = 744416.9518473670\n",
      "\n",
      "Features after p-value elimination algorithm:\n",
      "['const' 'carat' 'cut' 'color' 'clarity' 'depth' 'table' 'x']\n"
     ]
    }
   ],
   "source": [
    "# p-value elimination for linear regression model\n",
    "model = sm.OLS(y_train, X_train)\n",
    "results = model.fit()\n",
    "sign_level = 0.05\n",
    "\n",
    "features = list(X.iloc[:, :10].columns)\n",
    "features.insert(0, \"const\")\n",
    "features = np.array(features)\n",
    "\n",
    "print(\"Start from model with all features:\")\n",
    "print(\"R2_adj = %.10f\" % results.rsquared_adj)\n",
    "print(\"RMSE = %.10f\" % mean_squared_error(y_train, results.predict(X_train), squared=False))\n",
    "print(\"AIC = %.10f\\n\" % results.aic)\n",
    "p_values = np.array(results.pvalues)\n",
    "X_eliminated = X_train\n",
    "\n",
    "while np.max(p_values) > sign_level:\n",
    "    insign_feature_index = np.argmax(p_values)\n",
    "    X_eliminated = np.delete(X_eliminated, insign_feature_index, axis=1)\n",
    "    print(f'Feature \\\"{features[insign_feature_index]}\\\" was eliminated.')\n",
    "    features = np.delete(features, insign_feature_index)\n",
    "    model = sm.OLS(y_train, X_eliminated)\n",
    "    results = model.fit()\n",
    "    p_values = np.array(results.pvalues)\n",
    "    print(\"Current:\\nR2_adj = %.10f\" % results.rsquared_adj)\n",
    "    print(\"RMSE = %.10f\" % mean_squared_error(y_train, results.predict(X_eliminated), squared=False))\n",
    "    print(\"AIC = %.10f\\n\" % results.aic)\n",
    "\n",
    "print(\"Features after p-value elimination algorithm:\")\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Summary: the elimination of the \"z\" increased model's quality, so we can remove it."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 5. [1 point] Find the best (in terms of RMSE) $\\alpha$ for Lasso regression using cross-validation with 4 folds. You must select values from range $[10^{-4}, 10^{3}]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.682695795279722\n"
     ]
    }
   ],
   "source": [
    "parameters = {'alpha': np.logspace(-4, 3)}\n",
    "search = GridSearchCV(Lasso(), parameters, cv=4, scoring='neg_root_mean_squared_error')\n",
    "search.fit(X_train, y_train)\n",
    "print(search.best_params_['alpha'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Gradient descent\n",
    "\n",
    "#### 6. [3.5 points] Implement a Ridge regression model for the MSE loss function, trained by gradient descent.\n",
    "\n",
    "All calculations must be vectorized, and python loops can only be used for gradient descent iterations. As a stop criterion, you must use (simultaneously):\n",
    "\n",
    "* checking for the Absolute-value norm of the weight difference on two adjacent iterations (for example, less than some small number of the order of $10^{-6}$, set by the `tolerance` parameter);\n",
    "* reaching the maximum number of iterations (for example, 10000, set by the `max_iter` parameter).\n",
    "\n",
    "You need to implement:\n",
    "\n",
    "* Full gradient descent:\n",
    "\n",
    "$$\n",
    "w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} Q(w_{k}).\n",
    "$$\n",
    "\n",
    "* Stochastic Gradient Descent:\n",
    "\n",
    "$$\n",
    "w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} q_{i_{k}}(w_{k}).\n",
    "$$\n",
    "\n",
    "$\\nabla_{w} q_{i_{k}}(w_{k}) \\, $ is the estimate of the gradient over the batch of objects selected randomly.\n",
    "\n",
    "* Momentum method:\n",
    "\n",
    "$$\n",
    "h_0 = 0, \\\\\n",
    "h_{k + 1} = \\alpha h_{k} + \\eta_k \\nabla_{w} Q(w_{k}), \\\\\n",
    "w_{k + 1} = w_{k} - h_{k + 1}.\n",
    "$$\n",
    "\n",
    "* Adagrad method:\n",
    "\n",
    "$$\n",
    "G_0 = 0, \\\\\n",
    "G_{k + 1} = G_{k} + (\\nabla_{w} Q(w_{k+1}))^2, \\\\\n",
    "w_{k + 1} = w_{k} - \\eta * \\frac{\\nabla_{w} Q(w_{k+1})}{\\sqrt{G_{k+1} + \\epsilon}}.\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "To make sure that the optimization process really converges, we will use the `loss_history` class attribute. After calling the `fit` method, it should contain the values of the loss function for all iterations, starting from the first one (before the first step on the anti-gradient).\n",
    "\n",
    "You need to initialize the weights with a random vector from normal distribution. The following is a template class that needs to contain the code implementing all variations of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from numpy import linalg as la\n",
    "\n",
    "\n",
    "class LinReg(BaseEstimator):\n",
    "    def __init__(self, delta=1.0, gd_type='Momentum',\n",
    "                 tolerance=1e-4, max_iter=1000, w0=None, eta=1e-2, alpha=1e-3, reg_cf=1.0, epsilon=1e-8):\n",
    "        \"\"\"\n",
    "        gd_type: str\n",
    "            'GradientDescent', 'StochasticDescent', 'Momentum', 'Adagrad'\n",
    "        delta: float\n",
    "            proportion of object in a batch (for stochastic GD)\n",
    "        tolerance: float\n",
    "            for stopping gradient descent\n",
    "        max_iter: int\n",
    "            maximum number of steps in gradient descent\n",
    "        w0: np.array of shape (d)\n",
    "            init weights\n",
    "        eta: float\n",
    "            learning rate\n",
    "        alpha: float\n",
    "            momentum coefficient\n",
    "        reg_cf: float\n",
    "            regularization coefficient\n",
    "        epsilon: float\n",
    "            numerical stability\n",
    "        \"\"\"\n",
    "\n",
    "        self.delta = delta\n",
    "        self.gd_type = gd_type\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iter = max_iter\n",
    "        self.w0 = w0\n",
    "        self.alpha = alpha\n",
    "        self.w = None\n",
    "        self.eta = eta\n",
    "        self.loss_history = None  # list of loss function values at each training iteration\n",
    "        self.epsilon = epsilon\n",
    "        self.reg_cf = reg_cf\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (l, d)\n",
    "        y: np.array of shape (l)\n",
    "        ---\n",
    "        output: self\n",
    "        \"\"\"\n",
    "        self.loss_history = []\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "        if self.gd_type == 'GradientDescent':\n",
    "            self.w = self.full_grad_descent_calc(X, y)\n",
    "        if self.gd_type == 'Momentum':\n",
    "            self.w = self.momentum_descent_calc(X, y)\n",
    "        if self.gd_type == 'Adagrad':\n",
    "            self.w = self.adagrad_descent_calc(X, y)\n",
    "        if self.gd_type == 'StochasticDescent':\n",
    "            self.w = self.stochastic_grad_descent_calc(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.w is None:\n",
    "            raise Exception('Not trained yet')\n",
    "        return np.dot(np.insert(X, 0, 1, axis=1), self.w)\n",
    "\n",
    "    def calc_gradient(self, X, y, w):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (l, d) (l can be equal to 1 if stochastic)\n",
    "        y: np.array of shape (l)\n",
    "        ---\n",
    "        output: np.array of shape (d)\n",
    "        \"\"\"\n",
    "        return 2 * np.dot(X.T, np.dot(X, w) - y) / np.shape(X)[0] + 2 * self.reg_cf * w / np.shape(X)[0]\n",
    "\n",
    "    def full_grad_descent_calc(self, X, y):\n",
    "        w = np.random.normal(0, 1, np.shape(X)[1])\n",
    "        self.w = w.copy()\n",
    "        iter_num = 0\n",
    "        while iter_num < self.max_iter and abs(la.norm(w - self.w) - self.tolerance) > 0:\n",
    "            gradient = self.calc_gradient(X, y, w)\n",
    "            self.loss_history = np.append(self.loss_history, self.calc_loss(X, y))\n",
    "            self.w = w\n",
    "            w -= self.eta * gradient\n",
    "            iter_num += 1\n",
    "        return w\n",
    "\n",
    "    def momentum_descent_calc(self, X, y):\n",
    "        w = np.random.normal(0, 1, np.shape(X)[1])\n",
    "        self.w = w.copy()\n",
    "        iter_num = 0\n",
    "        h = np.zeros(np.shape(X)[1])\n",
    "        while iter_num < self.max_iter and abs(la.norm(w - self.w) - self.tolerance) > 0:\n",
    "            gradient = self.calc_gradient(X, y, w)\n",
    "            self.loss_history = np.append(self.loss_history, self.calc_loss(X, y))\n",
    "            h = self.alpha * h + self.eta * gradient\n",
    "            self.w = w\n",
    "            w -= h\n",
    "            iter_num += 1\n",
    "        return w\n",
    "\n",
    "    def adagrad_descent_calc(self, X, y):\n",
    "        w = np.random.normal(0, 1, np.shape(X)[1])\n",
    "        self.w = w.copy()\n",
    "        iter_num = 0\n",
    "        g = np.zeros(np.shape(X)[1])\n",
    "        while iter_num < self.max_iter and abs(la.norm(w - self.w) - self.tolerance) > 0:\n",
    "            gradient = self.calc_gradient(X, y, w)\n",
    "            self.loss_history = np.append(self.loss_history, self.calc_loss(X, y))\n",
    "            g += np.square(gradient)\n",
    "            self.w = w\n",
    "            w -= self.eta / (np.sqrt(g + self.epsilon)) * gradient\n",
    "            iter_num += 1\n",
    "        return w\n",
    "\n",
    "    def stochastic_grad_descent_calc(self, X, y):\n",
    "        np.random.seed(42)\n",
    "        w = np.random.normal(0, 1, np.shape(X)[1])\n",
    "        self.w = w.copy()\n",
    "        iter_num = 0\n",
    "        batch_size = int(np.round(np.shape(X)[0] * self.delta))\n",
    "        while iter_num < self.max_iter and abs(la.norm(w - self.w) - self.tolerance) > 0:\n",
    "            # get samples from data randomly\n",
    "            batch_indices = np.random.randint(X.shape[0], size=batch_size)\n",
    "            X_batch = X[batch_indices, :]\n",
    "            y_batch = y.iloc[batch_indices]\n",
    "            gradient = self.calc_gradient(X_batch, y_batch, w)\n",
    "            self.w = w\n",
    "            w -= self.eta * gradient\n",
    "\n",
    "            # if we want to run code with epochs (than max_iter = number of epochs)\n",
    "            # for start in range(0, np.shape(X)[0], batch_size):\n",
    "            #     stop = start + batch_size\n",
    "            #     X_batch = X[start:stop]\n",
    "            #     y_batch = y[start:stop]\n",
    "            #     gradient = self.calc_gradient(X_batch, y_batch, w)\n",
    "            #     self.w = w\n",
    "            #     w -= self.eta * gradient\n",
    "\n",
    "            self.loss_history = np.append(self.loss_history, self.calc_loss(X, y))\n",
    "            iter_num += 1\n",
    "        return w\n",
    "\n",
    "    def calc_loss(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (l, d)\n",
    "        y: np.array of shape (l)\n",
    "        ---\n",
    "        output: float \n",
    "        \"\"\"\n",
    "        return np.sum(np.square(np.dot(X, self.w) - y)) / np.shape(X)[0] + self.reg_cf * np.sum(np.square(self.w)) / np.shape(X)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 7. [1 points] Train and validate \"hand-written\" models on the same data, and compare the quality with the Sklearn or StatsModels methods. Investigate the effect of the `max_iter` and `alpha` parameters on the optimization process. Is it consistent with your expectations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full gradient descent (max 1000 iterations, learning rate = 0.01): \n",
      "Train RMSE = 1418.1646\n",
      "Train R2 = 0.8731\n",
      "Test RMSE = 1436.6539\n",
      "Test R2 = 0.8725\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(encoded_data, y, test_size=0.2, random_state=17)\n",
    "# scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "# Full gradient descent\n",
    "model_full_gr = LinReg(gd_type='GradientDescent')\n",
    "model_full_gr.fit(X_train, y_train)\n",
    "y_train_predicted = model_full_gr.predict(X_train)\n",
    "y_test_predicted = model_full_gr.predict(X_test)\n",
    "print(\"Full gradient descent (max 1000 iterations, learning rate = 0.01): \")\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_test_predicted, squared=False))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_test_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full gradient descent (max 500 iterations): \n",
      "Train RMSE = 1488.9497\n",
      "Train R2 = 0.8601\n",
      "Test RMSE = 1513.2966\n",
      "Test R2 = 0.8586\n"
     ]
    }
   ],
   "source": [
    "model_full_gr_iter_test = LinReg(gd_type='GradientDescent', max_iter=500)\n",
    "model_full_gr_iter_test.fit(X_train, y_train)\n",
    "y_train_predicted = model_full_gr_iter_test.predict(X_train)\n",
    "y_test_predicted = model_full_gr_iter_test.predict(X_test)\n",
    "print(\"Full gradient descent (max 500 iterations): \")\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_test_predicted, squared=False))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_test_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full gradient descent (max 1000 iterations, learning rate = 0.001): \n",
      "Train RMSE = 1698.9215\n",
      "Train R2 = 0.8178\n",
      "Test RMSE = 1736.5946\n",
      "Test R2 = 0.8138\n"
     ]
    }
   ],
   "source": [
    "model_full_gr_iter_test = LinReg(gd_type='GradientDescent', eta=0.001)\n",
    "model_full_gr_iter_test.fit(X_train, y_train)\n",
    "y_train_predicted = model_full_gr_iter_test.predict(X_train)\n",
    "y_test_predicted = model_full_gr_iter_test.predict(X_test)\n",
    "print(\"Full gradient descent (max 1000 iterations, learning rate = 0.001): \")\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_test_predicted, squared=False))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_test_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Summary:\n",
    "1.   max_iter   ,   max_iter   ,   ."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Momentum (max 1000 iterations, momentum coefficient = 0.001): \n",
      "Train RMSE = 1418.1270\n",
      "Train R2 = 0.8731\n",
      "Test RMSE = 1436.6138\n",
      "Test R2 = 0.8726\n"
     ]
    }
   ],
   "source": [
    "# Momentum\n",
    "model_momentum = LinReg(gd_type='Momentum')\n",
    "model_momentum.fit(X_train, y_train)\n",
    "y_train_predicted = model_momentum.predict(X_train)\n",
    "y_test_predicted = model_momentum.predict(X_test)\n",
    "print(\"Momentum (max 1000 iterations, momentum coefficient = 0.001): \")\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_test_predicted, squared=False))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_test_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Momentum (max 500 iterations): \n",
      "Train RMSE = 1488.7496\n",
      "Train R2 = 0.8601\n",
      "Test RMSE = 1513.0745\n",
      "Test R2 = 0.8586\n"
     ]
    }
   ],
   "source": [
    "model_momentum_test = LinReg(gd_type='Momentum', max_iter=500)\n",
    "model_momentum_test.fit(X_train, y_train)\n",
    "y_train_predicted = model_momentum_test.predict(X_train)\n",
    "y_test_predicted = model_momentum_test.predict(X_test)\n",
    "print(\"Momentum (max 500 iterations): \")\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_test_predicted, squared=False))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_test_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Momentum (max 1000 iterations, momentum coefficient = 0.1): \n",
      "Train RMSE = 1408.8322\n",
      "Train R2 = 0.8747\n",
      "Test RMSE = 1426.6467\n",
      "Test R2 = 0.8743\n"
     ]
    }
   ],
   "source": [
    "model_momentum_test = LinReg(gd_type='Momentum', alpha=0.1)\n",
    "model_momentum_test.fit(X_train, y_train)\n",
    "y_train_predicted = model_momentum_test.predict(X_train)\n",
    "y_test_predicted = model_momentum_test.predict(X_test)\n",
    "print(\"Momentum (max 1000 iterations, momentum coefficient = 0.1): \")\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_test_predicted, squared=False))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_test_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Summary:\n",
    "1.   max_iter   ,   max_iter   ,   .\n",
    "2.   momentum coefficient   ,   momentum coefficient   ."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic (max 1000 iterations, delta = 0.8): \n",
      "Train RMSE = 1418.4459\n",
      "Train R2 = 0.8730\n",
      "Test RMSE = 1437.0572\n",
      "Test R2 = 0.8725\n"
     ]
    }
   ],
   "source": [
    "# Stochastic\n",
    "model_stochastic = LinReg(gd_type='StochasticDescent', delta=0.8)\n",
    "model_stochastic.fit(X_train, y_train)\n",
    "y_train_predicted = model_stochastic.predict(X_train)\n",
    "y_test_predicted = model_stochastic.predict(X_test)\n",
    "print(\"Stochastic (max 1000 iterations, delta = 0.8): \")\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_test_predicted, squared=False))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_test_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic (max 1000 iterations, delta = 0.3: \n",
      "Train RMSE = 1418.5434\n",
      "Train R2 = 0.8730\n",
      "Test RMSE = 1437.1287\n",
      "Test R2 = 0.8725\n"
     ]
    }
   ],
   "source": [
    "model_stochastic_test = LinReg(gd_type='StochasticDescent', delta=0.3)\n",
    "model_stochastic_test.fit(X_train, y_train)\n",
    "y_train_predicted = model_stochastic_test.predict(X_train)\n",
    "y_test_predicted = model_stochastic_test.predict(X_test)\n",
    "print(\"Stochastic (max 1000 iterations, delta = 0.3: \")\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_test_predicted, squared=False))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_test_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic (max 500 iterations, delta = 0.8: \n",
      "Train RMSE = 1488.9311\n",
      "Train R2 = 0.8601\n",
      "Test RMSE = 1513.3074\n",
      "Test R2 = 0.8586\n"
     ]
    }
   ],
   "source": [
    "model_stochastic_test = LinReg(gd_type='StochasticDescent', delta=0.8, max_iter=500)\n",
    "model_stochastic_test.fit(X_train, y_train)\n",
    "y_train_predicted = model_stochastic_test.predict(X_train)\n",
    "y_test_predicted = model_stochastic_test.predict(X_test)\n",
    "print(\"Stochastic (max 500 iterations, delta = 0.8: \")\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_test_predicted, squared=False))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_test_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Summary:\n",
    "1.   max_iter   ,   max_iter   ,   .\n",
    "2.       ,       ,   ."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [148]\u001B[0m, in \u001B[0;36m<cell line: 5>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      3\u001B[0m parameters \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124meta\u001B[39m\u001B[38;5;124m'\u001B[39m: np\u001B[38;5;241m.\u001B[39mlogspace(\u001B[38;5;241m3\u001B[39m,\u001B[38;5;241m4\u001B[39m)}\n\u001B[1;32m      4\u001B[0m search \u001B[38;5;241m=\u001B[39m GridSearchCV(LinReg(gd_type\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAdagrad\u001B[39m\u001B[38;5;124m'\u001B[39m), parameters, cv\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, scoring\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mneg_root_mean_squared_error\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 5\u001B[0m \u001B[43msearch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:891\u001B[0m, in \u001B[0;36mBaseSearchCV.fit\u001B[0;34m(self, X, y, groups, **fit_params)\u001B[0m\n\u001B[1;32m    885\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_results(\n\u001B[1;32m    886\u001B[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001B[1;32m    887\u001B[0m     )\n\u001B[1;32m    889\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m results\n\u001B[0;32m--> 891\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_search\u001B[49m\u001B[43m(\u001B[49m\u001B[43mevaluate_candidates\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    893\u001B[0m \u001B[38;5;66;03m# multimetric is determined here because in the case of a callable\u001B[39;00m\n\u001B[1;32m    894\u001B[0m \u001B[38;5;66;03m# self.scoring the return type is only known after calling\u001B[39;00m\n\u001B[1;32m    895\u001B[0m first_test_score \u001B[38;5;241m=\u001B[39m all_out[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_scores\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1392\u001B[0m, in \u001B[0;36mGridSearchCV._run_search\u001B[0;34m(self, evaluate_candidates)\u001B[0m\n\u001B[1;32m   1390\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_run_search\u001B[39m(\u001B[38;5;28mself\u001B[39m, evaluate_candidates):\n\u001B[1;32m   1391\u001B[0m     \u001B[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001B[39;00m\n\u001B[0;32m-> 1392\u001B[0m     \u001B[43mevaluate_candidates\u001B[49m\u001B[43m(\u001B[49m\u001B[43mParameterGrid\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparam_grid\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:838\u001B[0m, in \u001B[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001B[0;34m(candidate_params, cv, more_results)\u001B[0m\n\u001B[1;32m    830\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    831\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\n\u001B[1;32m    832\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFitting \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m folds for each of \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;124m candidates,\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    833\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m totalling \u001B[39m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m fits\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m    834\u001B[0m             n_splits, n_candidates, n_candidates \u001B[38;5;241m*\u001B[39m n_splits\n\u001B[1;32m    835\u001B[0m         )\n\u001B[1;32m    836\u001B[0m     )\n\u001B[0;32m--> 838\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mparallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    839\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_fit_and_score\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    840\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase_estimator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    841\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    842\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    843\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    844\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtest\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    845\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparameters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparameters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    846\u001B[0m \u001B[43m        \u001B[49m\u001B[43msplit_progress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_splits\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    847\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcandidate_progress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_candidates\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    848\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_and_score_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    849\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    850\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mproduct\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    851\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcandidate_params\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    852\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    853\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    855\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(out) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    856\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    857\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo fits were performed. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    858\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWas the CV iterator empty? \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    859\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWere there no candidates?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    860\u001B[0m     )\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:1046\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m   1043\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdispatch_one_batch(iterator):\n\u001B[1;32m   1044\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterating \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_original_iterator \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1046\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdispatch_one_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m   1047\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[1;32m   1049\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m pre_dispatch \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mall\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m n_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   1050\u001B[0m     \u001B[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001B[39;00m\n\u001B[1;32m   1051\u001B[0m     \u001B[38;5;66;03m# No need to wait for async callbacks to trigger to\u001B[39;00m\n\u001B[1;32m   1052\u001B[0m     \u001B[38;5;66;03m# consumption.\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:861\u001B[0m, in \u001B[0;36mParallel.dispatch_one_batch\u001B[0;34m(self, iterator)\u001B[0m\n\u001B[1;32m    859\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    860\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 861\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dispatch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtasks\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    862\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:779\u001B[0m, in \u001B[0;36mParallel._dispatch\u001B[0;34m(self, batch)\u001B[0m\n\u001B[1;32m    777\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[1;32m    778\u001B[0m     job_idx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs)\n\u001B[0;32m--> 779\u001B[0m     job \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_backend\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_async\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    780\u001B[0m     \u001B[38;5;66;03m# A job can complete so quickly than its callback is\u001B[39;00m\n\u001B[1;32m    781\u001B[0m     \u001B[38;5;66;03m# called before we get here, causing self._jobs to\u001B[39;00m\n\u001B[1;32m    782\u001B[0m     \u001B[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001B[39;00m\n\u001B[1;32m    783\u001B[0m     \u001B[38;5;66;03m# used (rather than .append) in the following line\u001B[39;00m\n\u001B[1;32m    784\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs\u001B[38;5;241m.\u001B[39minsert(job_idx, job)\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py:208\u001B[0m, in \u001B[0;36mSequentialBackend.apply_async\u001B[0;34m(self, func, callback)\u001B[0m\n\u001B[1;32m    206\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply_async\u001B[39m(\u001B[38;5;28mself\u001B[39m, func, callback\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    207\u001B[0m     \u001B[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001B[39;00m\n\u001B[0;32m--> 208\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mImmediateResult\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    209\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m callback:\n\u001B[1;32m    210\u001B[0m         callback(result)\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py:572\u001B[0m, in \u001B[0;36mImmediateResult.__init__\u001B[0;34m(self, batch)\u001B[0m\n\u001B[1;32m    569\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, batch):\n\u001B[1;32m    570\u001B[0m     \u001B[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001B[39;00m\n\u001B[1;32m    571\u001B[0m     \u001B[38;5;66;03m# arguments in memory\u001B[39;00m\n\u001B[0;32m--> 572\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresults \u001B[38;5;241m=\u001B[39m \u001B[43mbatch\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:262\u001B[0m, in \u001B[0;36mBatchedCalls.__call__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    258\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    259\u001B[0m     \u001B[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001B[39;00m\n\u001B[1;32m    260\u001B[0m     \u001B[38;5;66;03m# change the default number of processes to -1\u001B[39;00m\n\u001B[1;32m    261\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m parallel_backend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend, n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_jobs):\n\u001B[0;32m--> 262\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    263\u001B[0m                 \u001B[38;5;28;01mfor\u001B[39;00m func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitems]\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:262\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    258\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    259\u001B[0m     \u001B[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001B[39;00m\n\u001B[1;32m    260\u001B[0m     \u001B[38;5;66;03m# change the default number of processes to -1\u001B[39;00m\n\u001B[1;32m    261\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m parallel_backend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend, n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_jobs):\n\u001B[0;32m--> 262\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    263\u001B[0m                 \u001B[38;5;28;01mfor\u001B[39;00m func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitems]\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/fixes.py:216\u001B[0m, in \u001B[0;36m_FuncWrapper.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    214\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    215\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig):\n\u001B[0;32m--> 216\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:680\u001B[0m, in \u001B[0;36m_fit_and_score\u001B[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001B[0m\n\u001B[1;32m    678\u001B[0m         estimator\u001B[38;5;241m.\u001B[39mfit(X_train, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params)\n\u001B[1;32m    679\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 680\u001B[0m         \u001B[43mestimator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    682\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[1;32m    683\u001B[0m     \u001B[38;5;66;03m# Note fit time as time until error\u001B[39;00m\n\u001B[1;32m    684\u001B[0m     fit_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m start_time\n",
      "Input \u001B[0;32mIn [103]\u001B[0m, in \u001B[0;36mLinReg.fit\u001B[0;34m(self, X, y)\u001B[0m\n\u001B[1;32m     53\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mw \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmomentum_descent_calc(X, y)\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgd_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAdagrad\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m---> 55\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mw \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madagrad_descent_calc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgd_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mStochasticDescent\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m     57\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mw \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstochastic_grad_descent_calc(X, y)\n",
      "Input \u001B[0;32mIn [103]\u001B[0m, in \u001B[0;36mLinReg.adagrad_descent_calc\u001B[0;34m(self, X, y)\u001B[0m\n\u001B[1;32m    105\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m iter_num \u001B[38;5;241m<\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_iter \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mabs\u001B[39m(la\u001B[38;5;241m.\u001B[39mnorm(w \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mw) \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtolerance) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    106\u001B[0m     gradient \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcalc_gradient(X, y, w)\n\u001B[0;32m--> 107\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss_history \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss_history, \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcalc_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    108\u001B[0m     g \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39msquare(gradient)\n\u001B[1;32m    109\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mw \u001B[38;5;241m=\u001B[39m w\n",
      "Input \u001B[0;32mIn [103]\u001B[0m, in \u001B[0;36mLinReg.calc_loss\u001B[0;34m(self, X, y)\u001B[0m\n\u001B[1;32m    142\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcalc_loss\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y):\n\u001B[1;32m    143\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    144\u001B[0m \u001B[38;5;124;03m    X: np.array of shape (l, d)\u001B[39;00m\n\u001B[1;32m    145\u001B[0m \u001B[38;5;124;03m    y: np.array of shape (l)\u001B[39;00m\n\u001B[1;32m    146\u001B[0m \u001B[38;5;124;03m    ---\u001B[39;00m\n\u001B[1;32m    147\u001B[0m \u001B[38;5;124;03m    output: float \u001B[39;00m\n\u001B[1;32m    148\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 149\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39msum(np\u001B[38;5;241m.\u001B[39msquare(\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mw\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m-\u001B[39m y)) \u001B[38;5;241m/\u001B[39m np\u001B[38;5;241m.\u001B[39mshape(X)[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreg_cf \u001B[38;5;241m*\u001B[39m np\u001B[38;5;241m.\u001B[39msum(np\u001B[38;5;241m.\u001B[39msquare(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mw)) \u001B[38;5;241m/\u001B[39m np\u001B[38;5;241m.\u001B[39mshape(X)[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[0;32m<__array_function__ internals>:5\u001B[0m, in \u001B[0;36mdot\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Adagrad\n",
    "# searching the best learning rate for adagrad modification\n",
    "parameters = {'eta': np.logspace(3,4)}\n",
    "search = GridSearchCV(LinReg(gd_type='Adagrad'), parameters, cv=2, scoring='neg_root_mean_squared_error')\n",
    "search.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adagrad: \n",
      "Train RMSE = 1347.9956\n",
      "Train R2 = 0.8853\n",
      "Test RMSE = 1370.8994\n",
      "Test R2 = 0.8839\n"
     ]
    }
   ],
   "source": [
    "# best_eta = search.best_params_['eta']\n",
    "best_eta = 1000\n",
    "model_adagrad = LinReg(gd_type='Adagrad', eta=best_eta)\n",
    "model_adagrad.fit(X_train, y_train)\n",
    "print(\"Adagrad: \")\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_test_predicted, squared=False))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_test_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For Adagrad modification we have to choose a higher learning rate, because of the dynamic learning rate calculations.\n",
    "This modification has the best equality, but this is mostly because of the GridSearchCV for the best learning rate."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Sklearn model\n",
    "model = Ridge()\n",
    "model.fit(X_train, y_train)\n",
    "y_train_predicted = model.predict(X_train)\n",
    "y_test_predicted = model.predict(X_test)\n",
    "print(\"Ridge from Sklearn: \")\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_test_predicted, squared=False))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_test_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<bold>Comparing hand-written models with model from Sklearn:<bold>\n",
    "Quality of Sklearn model is a little bit better than the hand-written one, because:\n",
    "   1. Sklearn model's RMSE is less than others.\n",
    "   2. R2 is higher than R2 of other models.\n",
    "\n",
    "But hand-written models still have good quality. Sklearn model explained ~89% of the data (R2). In the same time hand-written models explained ~87% of the data, which is very close to library model. Same about RMSE values.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 8. [1 points] Plot graphs (on the same picture) of the dependence of the loss function value on the iteration number for Full GD, SGD, Momentum and Adagrad. Draw conclusions about the rate of convergence of various modifications of gradient descent.\n",
    "\n",
    "Don't forget about what *beautiful* graphics should look like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Text(0.5, 1.0, 'Loss function value and iteration number dependence')"
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 720x720 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1, figsize=(10,10))\n",
    "plt.plot(model_full_gr.loss_history)\n",
    "plt.plot(model_momentum.loss_history)\n",
    "plt.plot(model_adagrad.loss_history)\n",
    "plt.plot(model_stochastic.loss_history)\n",
    "plt.legend([\"Full Gradient Descent\", \"Momentum\", \"Adagrad\", \"Stochastic GD\"])\n",
    "plt.xlabel(\"iteration number\")\n",
    "plt.ylabel(\"loss function value\")\n",
    "plt.title(\"Loss function value and iteration number dependence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "***    \n",
    "           .\n",
    "     0-100 ."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}