{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# HSE 2022: Mathematical Methods for Data Analysis\n",
    "\n",
    "## Homework 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-26T16:48:20.566549Z",
     "start_time": "2020-09-26T16:48:19.893995Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.regression.linear_model import OLSResults\n",
    "from math import sqrt\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "% matplotlib inline\n",
    "\n",
    "sns.set(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Data\n",
    "\n",
    "For this homework we use Dataset from seaborn on diamonds prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = sns.load_dataset('diamonds')\n",
    "y = data.price\n",
    "X = data.drop(['price'], axis=1)\n",
    "columns = data.drop(['price'], axis=1).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 0. [0.25 points] Encode categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "categories = (data.dtypes == \"category\")\n",
    "object_cols = list(categories[categories].index)\n",
    "\n",
    "encoded_data = X.copy()\n",
    "label_encoder = LabelEncoder()\n",
    "for col in object_cols:\n",
    "    encoded_data[col] = label_encoder.fit_transform(encoded_data[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 1. [0.25 points] Split the data into train and test sets with ratio 80:20 with random_state=17."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(encoded_data, y, test_size=0.2, random_state=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2. [1 point] Train models on train data using StatsModels library and apply it to the test set; use $RMSE$ and $R^2$ as the quality measure.\n",
    "\n",
    "* [`LinearRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html);\n",
    "* [`Ridge`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) with $\\alpha = 0.01$;\n",
    "* [`Lasso`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) with $\\alpha = 0.01$\n",
    "* [`ElasticNet`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html) with $\\alpha = 0.01$, $l_{1}$_$ratio = 0.6$\n",
    "\n",
    "Don't forget to scale the data before training the models with StandardScaler!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE = 1347.9933\n",
      "Test RMSE = 1370.9682\n",
      "Train R2 = 0.8853\n",
      "Test R2 = 0.8839\n"
     ]
    }
   ],
   "source": [
    "# for statsmodels\n",
    "X_train = sm.add_constant(X_train)\n",
    "X_test = sm.add_constant(X_test)\n",
    "model = sm.OLS(y_train, X_train)\n",
    "\n",
    "# Linear Regression from statsmodels\n",
    "results_lr = model.fit()\n",
    "y_test_predicted = results_lr.predict(X_test)\n",
    "y_train_predicted = results_lr.predict(X_train)\n",
    "\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_test_predicted, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_test_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE = 1365.9920\n",
      "Test RMSE = 1383.9541\n",
      "Train R2 = 0.8822\n",
      "Test R2 = 0.8817\n"
     ]
    }
   ],
   "source": [
    "# Ridge from statsmodels\n",
    "results_ridge = model.fit_regularized(L1_wt=0, alpha=0.01)\n",
    "y_test_predicted = results_ridge.predict(X_test)\n",
    "y_train_predicted = results_ridge.predict(X_train)\n",
    "\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_test_predicted, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_test_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE = 1348.6037\n",
      "Test RMSE = 1370.0240\n",
      "Train R2 = 0.8852\n",
      "Test R2 = 0.8841\n"
     ]
    }
   ],
   "source": [
    "# Lasso from statsmodels\n",
    "results_lasso = model.fit_regularized(L1_wt=1, alpha=0.01)\n",
    "y_test_predicted = results_lasso.predict(X_test)\n",
    "y_train_predicted = results_lasso.predict(X_train)\n",
    "\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_test_predicted, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_test_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE = 1353.6044\n",
      "Test RMSE = 1372.8616\n",
      "Train R2 = 0.8844\n",
      "Test R2 = 0.8836\n"
     ]
    }
   ],
   "source": [
    "# Elastic net from statsmodels\n",
    "results_elastic = model.fit_regularized(L1_wt=0.6, alpha=0.01)\n",
    "y_test_predicted = results_elastic.predict(X_test)\n",
    "y_train_predicted = results_elastic.predict(X_train)\n",
    "\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_test_predicted, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_test_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The best quality has Linear Regression model because it has the smallest RMSE and the highest R2 score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 3. [1 point] Explore the values of the parameters of the resulting models and compare the number of zero weights in them. Comment on the significance of the coefficients, overal model significance and other related factors from the results table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<class 'statsmodels.iolib.summary2.Summary'>\n\"\"\"\n                  Results: Ordinary least squares\n====================================================================\nModel:              OLS              Adj. R-squared:     0.885      \nDependent Variable: price            AIC:                744418.8263\nDate:               2022-10-16 00:46 BIC:                744505.5512\nNo. Observations:   43152            Log-Likelihood:     -3.7220e+05\nDf Model:           9                F-statistic:        3.701e+04  \nDf Residuals:       43142            Prob (F-statistic): 0.00       \nR-squared:          0.885            Scale:              1.8175e+06 \n--------------------------------------------------------------------\n             Coef.    Std.Err.    t     P>|t|    [0.025     0.975]  \n--------------------------------------------------------------------\nconst       3928.6813   6.4899 605.3537 0.0000  3915.9610  3941.4016\ncarat       5257.1453  31.0710 169.1979 0.0000  5196.2456  5318.0450\ncut           76.4610   6.6590  11.4824 0.0000    63.4093    89.5128\ncolor       -455.4350   6.8100 -66.8778 0.0000  -468.7826  -442.0874\nclarity      491.4240   6.7022  73.3231 0.0000   478.2876   504.5604\ndepth       -226.2704   7.9533 -28.4498 0.0000  -241.8590  -210.6818\ntable       -213.2612   6.9923 -30.4996 0.0000  -226.9662  -199.5562\nx          -1383.2878  48.3537 -28.6077 0.0000 -1478.0619 -1288.5137\ny             42.1665  29.5137   1.4287 0.1531   -15.6809   100.0138\nz              3.3540  29.6459   0.1131 0.9099   -54.7524    61.4604\n--------------------------------------------------------------------\nOmnibus:            11265.146      Durbin-Watson:         1.999     \nProb(Omnibus):      0.000          Jarque-Bera (JB):      360275.892\nSkew:               0.611          Prob(JB):              0.000     \nKurtosis:           17.103         Condition No.:         18        \n====================================================================\n\n\"\"\"",
      "text/html": "<table class=\"simpletable\">\n<tr>\n        <td>Model:</td>               <td>OLS</td>         <td>Adj. R-squared:</td>      <td>0.885</td>   \n</tr>\n<tr>\n  <td>Dependent Variable:</td>       <td>price</td>             <td>AIC:</td>         <td>744418.8263</td>\n</tr>\n<tr>\n         <td>Date:</td>        <td>2022-10-16 00:46</td>        <td>BIC:</td>         <td>744505.5512</td>\n</tr>\n<tr>\n   <td>No. Observations:</td>        <td>43152</td>        <td>Log-Likelihood:</td>   <td>-3.7220e+05</td>\n</tr>\n<tr>\n       <td>Df Model:</td>              <td>9</td>           <td>F-statistic:</td>      <td>3.701e+04</td> \n</tr>\n<tr>\n     <td>Df Residuals:</td>          <td>43142</td>      <td>Prob (F-statistic):</td>    <td>0.00</td>    \n</tr>\n<tr>\n      <td>R-squared:</td>            <td>0.885</td>            <td>Scale:</td>        <td>1.8175e+06</td> \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n     <td></td>        <th>Coef.</th>   <th>Std.Err.</th>     <th>t</th>     <th>P>|t|</th>   <th>[0.025</th>     <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th>    <td>3928.6813</td>  <td>6.4899</td>  <td>605.3537</td> <td>0.0000</td>  <td>3915.9610</td>  <td>3941.4016</td>\n</tr>\n<tr>\n  <th>carat</th>    <td>5257.1453</td>  <td>31.0710</td> <td>169.1979</td> <td>0.0000</td>  <td>5196.2456</td>  <td>5318.0450</td>\n</tr>\n<tr>\n  <th>cut</th>       <td>76.4610</td>   <td>6.6590</td>   <td>11.4824</td> <td>0.0000</td>   <td>63.4093</td>    <td>89.5128</td> \n</tr>\n<tr>\n  <th>color</th>    <td>-455.4350</td>  <td>6.8100</td>  <td>-66.8778</td> <td>0.0000</td>  <td>-468.7826</td>  <td>-442.0874</td>\n</tr>\n<tr>\n  <th>clarity</th>  <td>491.4240</td>   <td>6.7022</td>   <td>73.3231</td> <td>0.0000</td>  <td>478.2876</td>   <td>504.5604</td> \n</tr>\n<tr>\n  <th>depth</th>    <td>-226.2704</td>  <td>7.9533</td>  <td>-28.4498</td> <td>0.0000</td>  <td>-241.8590</td>  <td>-210.6818</td>\n</tr>\n<tr>\n  <th>table</th>    <td>-213.2612</td>  <td>6.9923</td>  <td>-30.4996</td> <td>0.0000</td>  <td>-226.9662</td>  <td>-199.5562</td>\n</tr>\n<tr>\n  <th>x</th>       <td>-1383.2878</td>  <td>48.3537</td> <td>-28.6077</td> <td>0.0000</td> <td>-1478.0619</td> <td>-1288.5137</td>\n</tr>\n<tr>\n  <th>y</th>         <td>42.1665</td>   <td>29.5137</td>  <td>1.4287</td>  <td>0.1531</td>  <td>-15.6809</td>   <td>100.0138</td> \n</tr>\n<tr>\n  <th>z</th>         <td>3.3540</td>    <td>29.6459</td>  <td>0.1131</td>  <td>0.9099</td>  <td>-54.7524</td>    <td>61.4604</td> \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n     <td>Omnibus:</td>    <td>11265.146</td>  <td>Durbin-Watson:</td>      <td>1.999</td>  \n</tr>\n<tr>\n  <td>Prob(Omnibus):</td>   <td>0.000</td>   <td>Jarque-Bera (JB):</td> <td>360275.892</td>\n</tr>\n<tr>\n       <td>Skew:</td>       <td>0.611</td>       <td>Prob(JB):</td>        <td>0.000</td>  \n</tr>\n<tr>\n     <td>Kurtosis:</td>    <td>17.103</td>    <td>Condition No.:</td>       <td>18</td>    \n</tr>\n</table>"
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = list(X.iloc[:, :10].columns)\n",
    "features.insert(0, \"const\")\n",
    "\n",
    "# linear regression\n",
    "results_lr.summary2(xname=features)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The values of the features are in \"coef\" column. There is no zero weights.\n",
    "Linear Regression summary:\n",
    "    We consider significance level = 0.05.\n",
    "    1. P-values of \"y\" is higher than 0.05, so we can call \"y\" coefficient insignificant\n",
    "    2. P-values of \"z\" is higher than 0.05, so we can call \"z\" coefficient insignificant\n",
    "    3. R2_adj show that ~88.5% of the variance is explained, so the model is valuable\n",
    "    4. As F-statistics ia a pretty large number, we can say that the model is significant\n",
    "    5. Durbin-Watson value is nearly 2, so there is no autocorrelation\n",
    "    6. Large Jarque-Bera value indicates that errors are not normally distributed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "outputs": [
    {
     "data": {
      "text/plain": "<class 'statsmodels.iolib.summary2.Summary'>\n\"\"\"\n                  Results: Ordinary least squares\n====================================================================\nModel:              OLS              Adj. R-squared:     0.882      \nDependent Variable: price            AIC:                745563.5541\nDate:               2022-10-16 00:46 BIC:                745650.2790\nNo. Observations:   43152            Log-Likelihood:     -3.7277e+05\nDf Model:           9                F-statistic:        3.591e+04  \nDf Residuals:       43142            Prob (F-statistic): 0.00       \nR-squared:          0.882            Scale:              1.8664e+06 \n---------------------------------------------------------------------\n            Coef.    Std.Err.     t      P>|t|     [0.025     0.975] \n---------------------------------------------------------------------\nconst     3889.7835    6.5765  591.4627  0.0000  3876.8933  3902.6736\ncarat     4219.7372   31.4858  134.0201  0.0000  4158.0243  4281.4500\ncut         81.8556    6.7479   12.1306  0.0000    68.6296    95.0816\ncolor     -424.7483    6.9009  -61.5498  0.0000  -438.2742  -411.2225\nclarity    496.6610    6.7917   73.1280  0.0000   483.3492   509.9728\ndepth     -162.6547    8.0595  -20.1817  0.0000  -178.4514  -146.8579\ntable     -203.6562    7.0856  -28.7421  0.0000  -217.5442  -189.7682\nx         -312.9137   48.9993   -6.3861  0.0000  -408.9533  -216.8741\ny           21.4162   29.9077    0.7161  0.4739   -37.2035    80.0359\nz          -39.9802   30.0417   -1.3308  0.1833   -98.8625    18.9021\n--------------------------------------------------------------------\nOmnibus:            13066.102      Durbin-Watson:         1.998     \nProb(Omnibus):      0.000          Jarque-Bera (JB):      183804.270\nSkew:               1.067          Prob(JB):              0.000     \nKurtosis:           12.883         Condition No.:         18        \n====================================================================\n\n\"\"\"",
      "text/html": "<table class=\"simpletable\">\n<tr>\n        <td>Model:</td>               <td>OLS</td>         <td>Adj. R-squared:</td>      <td>0.882</td>   \n</tr>\n<tr>\n  <td>Dependent Variable:</td>       <td>price</td>             <td>AIC:</td>         <td>745563.5541</td>\n</tr>\n<tr>\n         <td>Date:</td>        <td>2022-10-16 00:46</td>        <td>BIC:</td>         <td>745650.2790</td>\n</tr>\n<tr>\n   <td>No. Observations:</td>        <td>43152</td>        <td>Log-Likelihood:</td>   <td>-3.7277e+05</td>\n</tr>\n<tr>\n       <td>Df Model:</td>              <td>9</td>           <td>F-statistic:</td>      <td>3.591e+04</td> \n</tr>\n<tr>\n     <td>Df Residuals:</td>          <td>43142</td>      <td>Prob (F-statistic):</td>    <td>0.00</td>    \n</tr>\n<tr>\n      <td>R-squared:</td>            <td>0.882</td>            <td>Scale:</td>        <td>1.8664e+06</td> \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n     <td></td>       <th>Coef.</th>   <th>Std.Err.</th>     <th>t</th>     <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th>   <td>3889.7835</td>  <td>6.5765</td>  <td>591.4627</td> <td>0.0000</td> <td>3876.8933</td> <td>3902.6736</td>\n</tr>\n<tr>\n  <th>carat</th>   <td>4219.7372</td>  <td>31.4858</td> <td>134.0201</td> <td>0.0000</td> <td>4158.0243</td> <td>4281.4500</td>\n</tr>\n<tr>\n  <th>cut</th>      <td>81.8556</td>   <td>6.7479</td>   <td>12.1306</td> <td>0.0000</td>  <td>68.6296</td>   <td>95.0816</td> \n</tr>\n<tr>\n  <th>color</th>   <td>-424.7483</td>  <td>6.9009</td>  <td>-61.5498</td> <td>0.0000</td> <td>-438.2742</td> <td>-411.2225</td>\n</tr>\n<tr>\n  <th>clarity</th> <td>496.6610</td>   <td>6.7917</td>   <td>73.1280</td> <td>0.0000</td> <td>483.3492</td>  <td>509.9728</td> \n</tr>\n<tr>\n  <th>depth</th>   <td>-162.6547</td>  <td>8.0595</td>  <td>-20.1817</td> <td>0.0000</td> <td>-178.4514</td> <td>-146.8579</td>\n</tr>\n<tr>\n  <th>table</th>   <td>-203.6562</td>  <td>7.0856</td>  <td>-28.7421</td> <td>0.0000</td> <td>-217.5442</td> <td>-189.7682</td>\n</tr>\n<tr>\n  <th>x</th>       <td>-312.9137</td>  <td>48.9993</td>  <td>-6.3861</td> <td>0.0000</td> <td>-408.9533</td> <td>-216.8741</td>\n</tr>\n<tr>\n  <th>y</th>        <td>21.4162</td>   <td>29.9077</td>  <td>0.7161</td>  <td>0.4739</td> <td>-37.2035</td>   <td>80.0359</td> \n</tr>\n<tr>\n  <th>z</th>       <td>-39.9802</td>   <td>30.0417</td>  <td>-1.3308</td> <td>0.1833</td> <td>-98.8625</td>   <td>18.9021</td> \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n     <td>Omnibus:</td>    <td>13066.102</td>  <td>Durbin-Watson:</td>      <td>1.998</td>  \n</tr>\n<tr>\n  <td>Prob(Omnibus):</td>   <td>0.000</td>   <td>Jarque-Bera (JB):</td> <td>183804.270</td>\n</tr>\n<tr>\n       <td>Skew:</td>       <td>1.067</td>       <td>Prob(JB):</td>        <td>0.000</td>  \n</tr>\n<tr>\n     <td>Kurtosis:</td>    <td>12.883</td>    <td>Condition No.:</td>       <td>18</td>    \n</tr>\n</table>"
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ridge\n",
    "OLSResults(model, results_ridge.params, model.normalized_cov_params).summary2(xname=features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The values of the features are in \"coef\" column. There is no zero weights.\n",
    "Ridge Regression summary:\n",
    "    We consider significance level = 0.05.\n",
    "    1. P-values of \"y\" is higher than 0.05, so we can call \"y\" coefficient insignificant\n",
    "    2. P-values of \"z\" is higher than 0.05, so we can call \"z\" coefficient insignificant\n",
    "    3. R2_adj show that ~88.2% of the variance is explained, so the model is valuable\n",
    "    4. As F-statistics ia a pretty large number, we can say that the model is significant\n",
    "    5. Durbin-Watson value is nearly 2, so there is no autocorrelation\n",
    "    6. Large Jarque-Bera value indicates that errors are not normally distributed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "outputs": [
    {
     "data": {
      "text/plain": "<class 'statsmodels.iolib.summary2.Summary'>\n\"\"\"\n                  Results: Ordinary least squares\n====================================================================\nModel:              OLS              Adj. R-squared:     0.885      \nDependent Variable: price            AIC:                744457.9003\nDate:               2022-10-16 00:46 BIC:                744544.6252\nNo. Observations:   43152            Log-Likelihood:     -3.7222e+05\nDf Model:           9                F-statistic:        3.697e+04  \nDf Residuals:       43142            Prob (F-statistic): 0.00       \nR-squared:          0.885            Scale:              1.8192e+06 \n--------------------------------------------------------------------\n             Coef.    Std.Err.    t     P>|t|    [0.025     0.975]  \n--------------------------------------------------------------------\nconst       3928.6713   6.4928 605.0782 0.0000  3915.9452  3941.3974\ncarat       5063.8698  31.0850 162.9037 0.0000  5002.9425  5124.7970\ncut           77.5642   6.6620  11.6428 0.0000    64.5066    90.6218\ncolor       -453.1266   6.8130 -66.5087 0.0000  -466.4802  -439.7729\nclarity      495.2762   6.7052  73.8644 0.0000   482.1339   508.4186\ndepth       -214.9442   7.9569 -27.0135 0.0000  -230.5399  -199.3485\ntable       -213.4343   6.9954 -30.5105 0.0000  -227.1455  -199.7231\nx          -1201.4586  48.3756 -24.8361 0.0000 -1296.2757 -1106.6416\ny             54.7441  29.5270   1.8540 0.0637    -3.1294   112.6176\nz             -1.4625  29.6593  -0.0493 0.9607   -59.5953    56.6702\n--------------------------------------------------------------------\nOmnibus:            11656.263      Durbin-Watson:         1.999     \nProb(Omnibus):      0.000          Jarque-Bera (JB):      315053.588\nSkew:               0.713          Prob(JB):              0.000     \nKurtosis:           16.160         Condition No.:         18        \n====================================================================\n\n\"\"\"",
      "text/html": "<table class=\"simpletable\">\n<tr>\n        <td>Model:</td>               <td>OLS</td>         <td>Adj. R-squared:</td>      <td>0.885</td>   \n</tr>\n<tr>\n  <td>Dependent Variable:</td>       <td>price</td>             <td>AIC:</td>         <td>744457.9003</td>\n</tr>\n<tr>\n         <td>Date:</td>        <td>2022-10-16 00:46</td>        <td>BIC:</td>         <td>744544.6252</td>\n</tr>\n<tr>\n   <td>No. Observations:</td>        <td>43152</td>        <td>Log-Likelihood:</td>   <td>-3.7222e+05</td>\n</tr>\n<tr>\n       <td>Df Model:</td>              <td>9</td>           <td>F-statistic:</td>      <td>3.697e+04</td> \n</tr>\n<tr>\n     <td>Df Residuals:</td>          <td>43142</td>      <td>Prob (F-statistic):</td>    <td>0.00</td>    \n</tr>\n<tr>\n      <td>R-squared:</td>            <td>0.885</td>            <td>Scale:</td>        <td>1.8192e+06</td> \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n     <td></td>        <th>Coef.</th>   <th>Std.Err.</th>     <th>t</th>     <th>P>|t|</th>   <th>[0.025</th>     <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th>    <td>3928.6713</td>  <td>6.4928</td>  <td>605.0782</td> <td>0.0000</td>  <td>3915.9452</td>  <td>3941.3974</td>\n</tr>\n<tr>\n  <th>carat</th>    <td>5063.8698</td>  <td>31.0850</td> <td>162.9037</td> <td>0.0000</td>  <td>5002.9425</td>  <td>5124.7970</td>\n</tr>\n<tr>\n  <th>cut</th>       <td>77.5642</td>   <td>6.6620</td>   <td>11.6428</td> <td>0.0000</td>   <td>64.5066</td>    <td>90.6218</td> \n</tr>\n<tr>\n  <th>color</th>    <td>-453.1266</td>  <td>6.8130</td>  <td>-66.5087</td> <td>0.0000</td>  <td>-466.4802</td>  <td>-439.7729</td>\n</tr>\n<tr>\n  <th>clarity</th>  <td>495.2762</td>   <td>6.7052</td>   <td>73.8644</td> <td>0.0000</td>  <td>482.1339</td>   <td>508.4186</td> \n</tr>\n<tr>\n  <th>depth</th>    <td>-214.9442</td>  <td>7.9569</td>  <td>-27.0135</td> <td>0.0000</td>  <td>-230.5399</td>  <td>-199.3485</td>\n</tr>\n<tr>\n  <th>table</th>    <td>-213.4343</td>  <td>6.9954</td>  <td>-30.5105</td> <td>0.0000</td>  <td>-227.1455</td>  <td>-199.7231</td>\n</tr>\n<tr>\n  <th>x</th>       <td>-1201.4586</td>  <td>48.3756</td> <td>-24.8361</td> <td>0.0000</td> <td>-1296.2757</td> <td>-1106.6416</td>\n</tr>\n<tr>\n  <th>y</th>         <td>54.7441</td>   <td>29.5270</td>  <td>1.8540</td>  <td>0.0637</td>   <td>-3.1294</td>   <td>112.6176</td> \n</tr>\n<tr>\n  <th>z</th>         <td>-1.4625</td>   <td>29.6593</td>  <td>-0.0493</td> <td>0.9607</td>  <td>-59.5953</td>    <td>56.6702</td> \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n     <td>Omnibus:</td>    <td>11656.263</td>  <td>Durbin-Watson:</td>      <td>1.999</td>  \n</tr>\n<tr>\n  <td>Prob(Omnibus):</td>   <td>0.000</td>   <td>Jarque-Bera (JB):</td> <td>315053.588</td>\n</tr>\n<tr>\n       <td>Skew:</td>       <td>0.713</td>       <td>Prob(JB):</td>        <td>0.000</td>  \n</tr>\n<tr>\n     <td>Kurtosis:</td>    <td>16.160</td>    <td>Condition No.:</td>       <td>18</td>    \n</tr>\n</table>"
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lasso\n",
    "OLSResults(model, results_lasso.params, model.normalized_cov_params).summary2(xname=features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The values of the features are in \"coef\" column. There is no zero weights.\n",
    "Lasso Regression summary:\n",
    "    We consider significance level = 0.05.\n",
    "    1. P-values of \"y\" is higher than 0.05, so we can call \"y\" coefficient insignificant\n",
    "    2. P-values of \"z\" is higher than 0.05, so we can call \"z\" coefficient insignificant\n",
    "    3. R2_adj show that ~88.5% of the variance is explained, so the model is valuable\n",
    "    4. As F-statistics ia a pretty large number, we can say that the model is significant\n",
    "    5. Durbin-Watson value is nearly 2, so there is no autocorrelation\n",
    "    6. Large Jarque-Bera value indicates that errors are not normally distributed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "outputs": [
    {
     "data": {
      "text/plain": "<class 'statsmodels.iolib.summary2.Summary'>\n\"\"\"\n                  Results: Ordinary least squares\n====================================================================\nModel:              OLS              Adj. R-squared:     0.884      \nDependent Variable: price            AIC:                744777.3304\nDate:               2022-10-16 00:46 BIC:                744864.0552\nNo. Observations:   43152            Log-Likelihood:     -3.7238e+05\nDf Model:           9                F-statistic:        3.666e+04  \nDf Residuals:       43142            Prob (F-statistic): 0.00       \nR-squared:          0.884            Scale:              1.8327e+06 \n---------------------------------------------------------------------\n            Coef.    Std.Err.     t      P>|t|     [0.025     0.975] \n---------------------------------------------------------------------\nconst     3913.0232    6.5169  600.4416  0.0000  3900.2499  3925.7965\ncarat     4675.8946   31.2003  149.8669  0.0000  4614.7414  4737.0478\ncut         79.7319    6.6867   11.9240  0.0000    66.6258    92.8379\ncolor     -440.5288    6.8383  -64.4208  0.0000  -453.9320  -427.1256\nclarity    496.4432    6.7301   73.7649  0.0000   483.2521   509.6342\ndepth     -190.1759    7.9864  -23.8124  0.0000  -205.8294  -174.5223\ntable     -209.3948    7.0214  -29.8225  0.0000  -223.1568  -195.6328\nx         -766.0692   48.5550  -15.7774  0.0000  -861.2378  -670.9005\ny           20.1115   29.6365    0.6786  0.4974   -37.9767    78.1996\nz          -26.8075   29.7693   -0.9005  0.3679   -85.1558    31.5408\n--------------------------------------------------------------------\nOmnibus:            12406.161      Durbin-Watson:         1.999     \nProb(Omnibus):      0.000          Jarque-Bera (JB):      244618.648\nSkew:               0.894          Prob(JB):              0.000     \nKurtosis:           14.526         Condition No.:         18        \n====================================================================\n\n\"\"\"",
      "text/html": "<table class=\"simpletable\">\n<tr>\n        <td>Model:</td>               <td>OLS</td>         <td>Adj. R-squared:</td>      <td>0.884</td>   \n</tr>\n<tr>\n  <td>Dependent Variable:</td>       <td>price</td>             <td>AIC:</td>         <td>744777.3304</td>\n</tr>\n<tr>\n         <td>Date:</td>        <td>2022-10-16 00:46</td>        <td>BIC:</td>         <td>744864.0552</td>\n</tr>\n<tr>\n   <td>No. Observations:</td>        <td>43152</td>        <td>Log-Likelihood:</td>   <td>-3.7238e+05</td>\n</tr>\n<tr>\n       <td>Df Model:</td>              <td>9</td>           <td>F-statistic:</td>      <td>3.666e+04</td> \n</tr>\n<tr>\n     <td>Df Residuals:</td>          <td>43142</td>      <td>Prob (F-statistic):</td>    <td>0.00</td>    \n</tr>\n<tr>\n      <td>R-squared:</td>            <td>0.884</td>            <td>Scale:</td>        <td>1.8327e+06</td> \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n     <td></td>       <th>Coef.</th>   <th>Std.Err.</th>     <th>t</th>     <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th>   <td>3913.0232</td>  <td>6.5169</td>  <td>600.4416</td> <td>0.0000</td> <td>3900.2499</td> <td>3925.7965</td>\n</tr>\n<tr>\n  <th>carat</th>   <td>4675.8946</td>  <td>31.2003</td> <td>149.8669</td> <td>0.0000</td> <td>4614.7414</td> <td>4737.0478</td>\n</tr>\n<tr>\n  <th>cut</th>      <td>79.7319</td>   <td>6.6867</td>   <td>11.9240</td> <td>0.0000</td>  <td>66.6258</td>   <td>92.8379</td> \n</tr>\n<tr>\n  <th>color</th>   <td>-440.5288</td>  <td>6.8383</td>  <td>-64.4208</td> <td>0.0000</td> <td>-453.9320</td> <td>-427.1256</td>\n</tr>\n<tr>\n  <th>clarity</th> <td>496.4432</td>   <td>6.7301</td>   <td>73.7649</td> <td>0.0000</td> <td>483.2521</td>  <td>509.6342</td> \n</tr>\n<tr>\n  <th>depth</th>   <td>-190.1759</td>  <td>7.9864</td>  <td>-23.8124</td> <td>0.0000</td> <td>-205.8294</td> <td>-174.5223</td>\n</tr>\n<tr>\n  <th>table</th>   <td>-209.3948</td>  <td>7.0214</td>  <td>-29.8225</td> <td>0.0000</td> <td>-223.1568</td> <td>-195.6328</td>\n</tr>\n<tr>\n  <th>x</th>       <td>-766.0692</td>  <td>48.5550</td> <td>-15.7774</td> <td>0.0000</td> <td>-861.2378</td> <td>-670.9005</td>\n</tr>\n<tr>\n  <th>y</th>        <td>20.1115</td>   <td>29.6365</td>  <td>0.6786</td>  <td>0.4974</td> <td>-37.9767</td>   <td>78.1996</td> \n</tr>\n<tr>\n  <th>z</th>       <td>-26.8075</td>   <td>29.7693</td>  <td>-0.9005</td> <td>0.3679</td> <td>-85.1558</td>   <td>31.5408</td> \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n     <td>Omnibus:</td>    <td>12406.161</td>  <td>Durbin-Watson:</td>      <td>1.999</td>  \n</tr>\n<tr>\n  <td>Prob(Omnibus):</td>   <td>0.000</td>   <td>Jarque-Bera (JB):</td> <td>244618.648</td>\n</tr>\n<tr>\n       <td>Skew:</td>       <td>0.894</td>       <td>Prob(JB):</td>        <td>0.000</td>  \n</tr>\n<tr>\n     <td>Kurtosis:</td>    <td>14.526</td>    <td>Condition No.:</td>       <td>18</td>    \n</tr>\n</table>"
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# elastic\n",
    "OLSResults(model, results_elastic.params, model.normalized_cov_params).summary2(xname=features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The values of the features are in \"coef\" column. There is no zero weights.\n",
    "Elastic Net Regression summary:\n",
    "    We consider significance level = 0.05.\n",
    "    1. P-values of \"y\" is higher than 0.05, so we can call \"y\" coefficient insignificant\n",
    "    2. P-values of \"z\" is higher than 0.05, so we can call \"z\" coefficient insignificant\n",
    "    3. R2_adj show that ~88.4% of the variance is explained, so the model is valuable\n",
    "    4. As F-statistics ia a pretty large number, we can say that the model is significant\n",
    "    5. Durbin-Watson value is nearly 2, so there is no autocorrelation\n",
    "    6. Large Jarque-Bera value indicates that errors are not normally distributed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<bold> Summary: <bold>\n",
    "    1. As Linear Regression model has the smallest AIC value, it should be selected.\n",
    "    2. Linear Regression model also has the smallest BIC value\n",
    "    3. In all models \"y\" and \"z\" are insignificant\n",
    "    4. The Ridge model is the most skewed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 4. [1 point] Implement one of the elimination algorithms that were described in the Seminar_4 (Elimination by P-value, Forward elimination, Backward elimination), make conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<bold>We consider significance level = 0.05.<bold>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start from model with all features:\n",
      "R2_adj = 0.8853021693\n",
      "RMSE = 1347.9932745208\n",
      "AIC = 744418.8263415196\n",
      "\n",
      "Feature \"z\" was eliminated.\n",
      "Current:\n",
      "R2_adj = 0.8853047938\n",
      "RMSE = 1347.9934744846\n",
      "AIC = 744416.8391440106\n",
      "\n",
      "Feature \"y\" was eliminated.\n",
      "Current:\n",
      "R2_adj = 0.8853018368\n",
      "RMSE = 1348.0264734788\n",
      "AIC = 744416.9518473670\n",
      "\n",
      "Features after p-value elimination algorithm:\n",
      "['const' 'carat' 'cut' 'color' 'clarity' 'depth' 'table' 'x']\n"
     ]
    }
   ],
   "source": [
    "# p-value elimination for linear regression model\n",
    "model = sm.OLS(y_train, X_train)\n",
    "results = model.fit()\n",
    "sign_level = 0.05\n",
    "\n",
    "features = list(X.iloc[:, :10].columns)\n",
    "features.insert(0, \"const\")\n",
    "features = np.array(features)\n",
    "\n",
    "print(\"Start from model with all features:\")\n",
    "print(\"R2_adj = %.10f\" % results.rsquared_adj)\n",
    "print(\"RMSE = %.10f\" % mean_squared_error(y_train, results.predict(X_train), squared=False))\n",
    "print(\"AIC = %.10f\\n\" % results.aic)\n",
    "p_values = np.array(results.pvalues)\n",
    "X_eliminated = X_train\n",
    "\n",
    "while np.max(p_values) > sign_level:\n",
    "    insign_feature_index = np.argmax(p_values)\n",
    "    X_eliminated = np.delete(X_eliminated, insign_feature_index, axis=1)\n",
    "    print(f'Feature \\\"{features[insign_feature_index]}\\\" was eliminated.')\n",
    "    features = np.delete(features, insign_feature_index)\n",
    "    model = sm.OLS(y_train, X_eliminated)\n",
    "    results = model.fit()\n",
    "    p_values = np.array(results.pvalues)\n",
    "    print(\"Current:\\nR2_adj = %.10f\" % results.rsquared_adj)\n",
    "    print(\"RMSE = %.10f\" % mean_squared_error(y_train, results.predict(X_eliminated), squared=False))\n",
    "    print(\"AIC = %.10f\\n\" % results.aic)\n",
    "\n",
    "print(\"Features after p-value elimination algorithm:\")\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Summary: the elimination of the \"z\" increased model's quality, so we can remove it."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 5. [1 point] Find the best (in terms of RMSE) $\\alpha$ for Lasso regression using cross-validation with 4 folds. You must select values from range $[10^{-4}, 10^{3}]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "parameters = {'alpha': np.logspace(-4, 3)}\n",
    "search = GridSearchCV(Lasso(), parameters, cv=4, scoring='neg_root_mean_squared_error')\n",
    "search.fit(X_train, y_train)\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Gradient descent\n",
    "\n",
    "#### 6. [3.5 points] Implement a Ridge regression model for the MSE loss function, trained by gradient descent.\n",
    "\n",
    "All calculations must be vectorized, and python loops can only be used for gradient descent iterations. As a stop criterion, you must use (simultaneously):\n",
    "\n",
    "* checking for the Absolute-value norm of the weight difference on two adjacent iterations (for example, less than some small number of the order of $10^{-6}$, set by the `tolerance` parameter);\n",
    "* reaching the maximum number of iterations (for example, 10000, set by the `max_iter` parameter).\n",
    "\n",
    "You need to implement:\n",
    "\n",
    "* Full gradient descent:\n",
    "\n",
    "$$\n",
    "w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} Q(w_{k}).\n",
    "$$\n",
    "\n",
    "* Stochastic Gradient Descent:\n",
    "\n",
    "$$\n",
    "w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} q_{i_{k}}(w_{k}).\n",
    "$$\n",
    "\n",
    "$\\nabla_{w} q_{i_{k}}(w_{k}) \\, $ is the estimate of the gradient over the batch of objects selected randomly.\n",
    "\n",
    "* Momentum method:\n",
    "\n",
    "$$\n",
    "h_0 = 0, \\\\\n",
    "h_{k + 1} = \\alpha h_{k} + \\eta_k \\nabla_{w} Q(w_{k}), \\\\\n",
    "w_{k + 1} = w_{k} - h_{k + 1}.\n",
    "$$\n",
    "\n",
    "* Adagrad method:\n",
    "\n",
    "$$\n",
    "G_0 = 0, \\\\\n",
    "G_{k + 1} = G_{k} + (\\nabla_{w} Q(w_{k+1}))^2, \\\\\n",
    "w_{k + 1} = w_{k} - \\eta * \\frac{\\nabla_{w} Q(w_{k+1})}{\\sqrt{G_{k+1} + \\epsilon}}.\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "To make sure that the optimization process really converges, we will use the `loss_history` class attribute. After calling the `fit` method, it should contain the values of the loss function for all iterations, starting from the first one (before the first step on the anti-gradient).\n",
    "\n",
    "You need to initialize the weights with a random vector from normal distribution. The following is a template class that needs to contain the code implementing all variations of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from numpy import linalg as la\n",
    "\n",
    "\n",
    "class LinReg(BaseEstimator):\n",
    "    def __init__(self, delta=1.0, gd_type='Momentum',\n",
    "                 tolerance=1e-4, max_iter=1000, w0=None, eta=1e-2, alpha=1e-3, reg_cf=1.0, epsilon=10e-8):\n",
    "        \"\"\"\n",
    "        gd_type: str\n",
    "            'GradientDescent', 'StochasticDescent', 'Momentum', 'Adagrad'\n",
    "        delta: float\n",
    "            proportion of object in a batch (for stochastic GD)\n",
    "        tolerance: float\n",
    "            for stopping gradient descent\n",
    "        max_iter: int\n",
    "            maximum number of steps in gradient descent\n",
    "        w0: np.array of shape (d)\n",
    "            init weights\n",
    "        eta: float\n",
    "            learning rate\n",
    "        alpha: float\n",
    "            momentum coefficient\n",
    "        reg_cf: float\n",
    "            regularization coefficient\n",
    "        epsilon: float\n",
    "            numerical stability\n",
    "        \"\"\"\n",
    "\n",
    "        self.delta = delta\n",
    "        self.gd_type = gd_type\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iter = max_iter\n",
    "        self.w0 = w0\n",
    "        self.alpha = alpha\n",
    "        self.w = None\n",
    "        self.eta = eta\n",
    "        self.loss_history = None  # list of loss function values at each training iteration\n",
    "        self.epsilon = epsilon\n",
    "        self.reg_cf = reg_cf\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (l, d)\n",
    "        y: np.array of shape (l)\n",
    "        ---\n",
    "        output: self\n",
    "        \"\"\"\n",
    "        self.loss_history = []\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "        if self.gd_type == 'GradientDescent':\n",
    "            self.w = self.full_grad_descent_calc(X, y)\n",
    "        if self.gd_type == 'Momentum':\n",
    "            self.w = self.momentum_descent_calc(X, y)\n",
    "        if self.gd_type == 'Adagrad':\n",
    "            self.w = self.adagrad_descent_calc(X, y)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.w is None:\n",
    "            raise Exception('Not trained yet')\n",
    "        return np.dot(np.insert(X, 0, 1, axis=1), self.w)\n",
    "\n",
    "    def calc_gradient(self, X, y, w):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (l, d) (l can be equal to 1 if stochastic)\n",
    "        y: np.array of shape (l)\n",
    "        ---\n",
    "        output: np.array of shape (d)\n",
    "        \"\"\"\n",
    "        return 2 * np.dot(X.T, np.dot(X, w) - y) / np.shape(X)[0] + 2 * self.reg_cf * w / np.shape(X)[0]\n",
    "\n",
    "    def full_grad_descent_calc(self, X, y):\n",
    "        w = np.random.normal(0, 1, np.shape(X)[1])\n",
    "        self.w = w.copy()\n",
    "        iter_num = 0\n",
    "        while iter_num < self.max_iter and abs(la.norm(w - self.w) - self.tolerance) > 10e-6:\n",
    "                #and abs(la.norm(w - self.w) - self.tolerance) > 10e-6:\n",
    "            gradient = self.calc_gradient(X, y, w)\n",
    "            self.loss_history = np.append(self.loss_history, self.calc_loss(X, y))\n",
    "            self.w = w\n",
    "            w -= self.eta * gradient\n",
    "            iter_num += 1\n",
    "        return w\n",
    "\n",
    "    def momentum_descent_calc(self, X, y):\n",
    "        w = np.random.normal(0, 1, np.shape(X)[1])\n",
    "        self.w = w.copy()\n",
    "        iter_num = 0\n",
    "        h = np.zeros(np.shape(X)[1])\n",
    "        while iter_num < self.max_iter and abs(la.norm(w - self.w) - self.tolerance) > 0:\n",
    "            gradient = self.calc_gradient(X, y, w)\n",
    "            self.loss_history = np.append(self.loss_history, self.calc_loss(X, y))\n",
    "            h = self.alpha * h + self.eta * gradient\n",
    "            self.w = w\n",
    "            w -= h\n",
    "            iter_num += 1\n",
    "        return w\n",
    "\n",
    "    def adagrad_descent_calc(self, X, y):\n",
    "        w = np.random.normal(0, 1, np.shape(X)[1])\n",
    "        self.w = w.copy()\n",
    "        iter_num = 0\n",
    "        g = np.zeros(np.shape(X)[1])\n",
    "        while iter_num < self.max_iter and abs(la.norm(w - self.w) - self.tolerance) > 10e-6:\n",
    "            gradient = self.calc_gradient(X, y, w)\n",
    "            np.append(self.loss_history, self.calc_loss(X, y))\n",
    "            g += np.square(gradient)\n",
    "            self.w = w\n",
    "            w -= self.eta * np.divide(gradient, (np.sqrt(g + self.epsilon)))\n",
    "            iter_num += 1\n",
    "        return w\n",
    "\n",
    "    # def stochastic_grad_descent_calc(self, X, y):\n",
    "    #     w = np.random.normal(0, 1, np.shape(X)[1])\n",
    "    #     w_prev = w.copy()\n",
    "    #     iter_num = 0\n",
    "    #     while iter_num < self.max_iter and abs(w - w_prev) < self.tolerance:\n",
    "    #\n",
    "    #     return w\n",
    "\n",
    "    def calc_loss(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (l, d)\n",
    "        y: np.array of shape (l)\n",
    "        ---\n",
    "        output: float \n",
    "        \"\"\"\n",
    "        return np.sum(np.square(np.dot(X, self.w) - y)) / np.shape(X)[0] + self.reg_cf * np.sum(np.square(self.w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 7. [1 points] Train and validate \"hand-written\" models on the same data, and compare the quality with the Sklearn or StatsModels methods. Investigate the effect of the `max_iter` and `alpha` parameters on the optimization process. Is it consistent with your expectations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO условие по выходу из цикла в модели\n",
    "# your code here\n",
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "X_train, X_test, y_train, y_test = train_test_split(encoded_data, y, test_size=0.2, random_state=17)\n",
    "# scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "# Full gradient descent\n",
    "model = LinReg(gd_type='GradientDescent')\n",
    "model.fit(X_train, y_train)\n",
    "y_train_predicted = model.predict(X_train)\n",
    "y_test_predicted = model.predict(X_test)\n",
    "print(\"Full gradient descent: \")\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_test, y_test_predicted, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_test, y_test_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Momentum: \n",
      "Train RMSE = 1418.1395\n",
      "Train R2 = 0.8731\n",
      "Train RMSE = 1436.6256\n",
      "Train R2 = 0.8725\n",
      "[31272297.98160338 28163401.21300612 25501876.94114036 23226105.28166647\n",
      " 21280493.71949341 19617702.09419871 18197389.29261866 16985150.62769159\n",
      " 15951619.23681988 15071705.95374813 14323956.10306663 13690005.03779136\n",
      " 13154117.08304106 12702794.94566768 12324448.67079219 12009114.93054645\n",
      " 11748218.86765017 11534371.92866996 11361200.14424394 11223198.17598767\n",
      " 11115605.17732585 11034299.12927676 10975706.83008922 10936727.15462852\n",
      " 10914665.56820312 10907178.19080239 10912223.97048477 10928023.74650928\n",
      " 10953025.17014413 10985872.60931228 11025381.29689624 11070515.09546212\n",
      " 11120367.3466111  11174144.35385483 11231151.11614158 11290778.98687276\n",
      " 11352494.98208284 11415832.50279027 11480383.27152789 11545790.31271232\n",
      " 11611741.83164078 11677965.86821276 11744225.6195511  11810315.34104389\n",
      " 11876056.74836317 11941295.85409638 12005900.18205076 12069756.31031337\n",
      " 12132767.70098437 12194852.78032913 12255943.2380695  12315982.51878442\n",
      " 12374924.48202521 12432732.21086367 12489376.95125925 12544837.16692258\n",
      " 12599097.69632093 12652149.00016638 12703986.489188   12754609.92325065\n",
      " 12804022.87397338 12852232.24394534 12899247.83645639 12945081.9703719\n",
      " 12989749.13540164 13033265.6835535  13075649.55303579 13116920.02128574\n",
      " 13157097.48416524 13196203.25868393 13234259.40689116 13271288.5788262\n",
      " 13307313.87263584 13342358.71016225 13376446.72647699 13409601.67198982\n",
      " 13441847.32589784 13473207.41986206 13503705.57090745 13533365.22263976\n",
      " 13562209.59395989 13590261.63453502 13617543.98635579 13644078.95077286\n",
      " 13669888.46046278 13694994.05582515 13719416.86535933 13743177.58961133\n",
      " 13766296.48831959 13788793.37042283 13810687.58662476 13831998.02423847\n",
      " 13852743.10405955 13872940.77904025 13892608.53455811 13911763.39009239\n",
      " 13930421.90213875 13948600.16820888 13966313.83177657 13983578.08804463\n",
      " 14000407.69041946 14016816.95759109 14032819.78112634 14048429.63349217\n",
      " 14063659.57643433 14078522.26964434 14093029.97965443 14107194.58890646\n",
      " 14121027.60494665 14134540.169703   14147743.06880716 14160646.74092663\n",
      " 14173261.2870772  14185596.47988915 14197661.77280369 14209466.30917935\n",
      " 14221018.93129039 14232328.18920191 14243402.34950841 14254249.40392444\n",
      " 14264877.0777179  14275292.83797793 14285503.90171106 14295517.24376024\n",
      " 14305339.60454269 14314977.49760374 14324437.21698418 14333724.84440023\n",
      " 14342846.25623529 14351807.13034368 14360612.95266709 14369269.02366476\n",
      " 14377780.46455919 14386152.22339929 14394389.08094328 14402495.65636414\n",
      " 14410476.41278033 14418335.66261492 14426077.57278654 14433706.16973536\n",
      " 14441225.34428782 14448638.85636366 14455950.33952899 14463163.30539915\n",
      " 14470281.14789517 14477307.14735767 14484244.47452192 14491096.194358\n",
      " 14497865.26977983 14504554.5652267  14511166.85012124 14517704.80220736\n",
      " 14524171.0107718  14530567.97975298 14536898.13074056 14543163.80586915\n",
      " 14549367.27060965 14555510.71646143 14561596.26354866 14567625.96312391\n",
      " 14573601.79998208 14579525.69478774 14585399.5063188  14591225.03362929\n",
      " 14597004.01813406 14602738.1456182  14608429.04817358 14614078.3060653\n",
      " 14619687.44953034 14625257.96051089 14630791.27432462 14636288.78127423\n",
      " 14641751.82819835 14647181.71996594 14652579.72091633 14657947.0562466\n",
      " 14663284.91334861 14668594.44309705 14673876.76109079 14679132.94884875\n",
      " 14684364.0549624  14689571.09620618 14694755.05860756 14699916.8984781\n",
      " 14705057.54340714 14710177.89321928 14715278.82089721 14720361.17347098\n",
      " 14725425.77287518 14730473.416775   14735504.87936248 14740520.91212403\n",
      " 14745522.24458033 14750509.58499946 14755483.62108461 14760445.02063691\n",
      " 14765394.43219469 14770332.48564977 14775259.79284183 14780176.94813154\n",
      " 14785084.52895341 14789983.0963489  14794873.19548079 14799755.35612928\n",
      " 14804630.09317066 14809497.90703914 14814359.28417245 14819214.69744191\n",
      " 14824064.60656739 14828909.45851799 14833749.68789859 14838585.71732322\n",
      " 14843417.95777541 14848246.80895622 14853072.65962025 14857895.88790019\n",
      " 14862716.8616203  14867535.93859923 14872353.46694258 14877169.7853255\n",
      " 14881985.22326594 14886800.10138853 14891614.73167978 14896429.41773472\n",
      " 14901244.45499533 14906060.13098116 14910876.72551222 14915694.51092469\n",
      " 14920513.75227946 14925334.70756393 14930157.62788729 14934982.75766935\n",
      " 14939810.3348235  14944640.5909336  14949473.75142533 14954310.03573209\n",
      " 14959149.65745559 14963992.82452144 14968839.73932981 14973690.59890138\n",
      " 14978545.5950188  14983404.9143637  14988268.73864954 14993137.24475032\n",
      " 14998010.60482539 15002888.98644047 15007772.55268502 15012661.46228601\n",
      " 15017555.86971834 15022455.92531196 15027361.77535579 15032273.56219858\n",
      " 15037191.42434678 15042115.4965596  15047045.90994127 15051982.79203061\n",
      " 15056926.26688806 15061876.45518018 15066833.47426176 15071797.43825561\n",
      " 15076768.45813007 15081746.64177435 15086732.0940718  15091724.91697108\n",
      " 15096725.20955541 15101733.06810993 15106748.58618714 15111771.85467066\n",
      " 15116802.96183719 15121841.99341681 15126889.03265171 15131944.1603533\n",
      " 15137007.45495787 15142078.99258068 15147158.84706876 15152247.09005221\n",
      " 15157343.79099428 15162449.01724006 15167562.83406399 15172685.30471611\n",
      " 15177816.49046718 15182956.45065258 15188105.24271523 15193262.9222473\n",
      " 15198429.54303101 15203605.15707828 15208789.81466955 15213983.56439154\n",
      " 15219186.45317413 15224398.52632635 15229619.82757149 15234850.39908136\n",
      " 15240090.28150977 15245339.51402514 15250598.1343424  15255866.17875416\n",
      " 15261143.68216103 15266430.67810138 15271727.19878031 15277033.27509797\n",
      " 15282348.93667728 15287674.21189095 15293009.12788787 15298353.71061902\n",
      " 15303707.98486265 15309071.97424898 15314445.70128433 15319829.18737469\n",
      " 15325222.45284878 15330625.51698066 15336038.39801169 15341461.11317218\n",
      " 15346893.67870244 15352336.10987348 15357788.42100715 15363250.62549592\n",
      " 15368722.73582222 15374204.76357738 15379696.71948007 15385198.61339454\n",
      " 15390710.45434825 15396232.25054929 15401764.0094034  15407305.73753057\n",
      " 15412857.44078134 15418419.12425279 15423990.79230414 15429572.44857207\n",
      " 15435164.09598569 15440765.73678124 15446377.37251644 15451999.00408462\n",
      " 15457630.63172849 15463272.25505366 15468923.8730419  15474585.48406409\n",
      " 15480257.08589298 15485938.67571563 15491630.25014561 15497331.80523499\n",
      " 15503043.33648607 15508764.83886286 15514496.30680238 15520237.73422567\n",
      " 15525989.11454865 15531750.44069272 15537521.70509519 15543302.89971946\n",
      " 15549094.01606503 15554895.04517733 15560705.97765731 15566526.80367089\n",
      " 15572357.51295821 15578198.09484271 15584048.53823998 15589908.83166656\n",
      " 15595778.96324841 15601658.92072938 15607548.6914794  15613448.26250255\n",
      " 15619357.62044499 15625276.75160274 15631205.64192928 15637144.27704304\n",
      " 15643092.64223472 15649050.72247449 15655018.50241906 15660995.96641858\n",
      " 15666983.09852346 15672979.882491   15678986.30179199 15685002.33961704\n",
      " 15691027.97888296 15697063.20223889 15703107.99207236 15709162.33051525\n",
      " 15715226.19944967 15721299.58051357 15727382.4551065  15733474.80439503\n",
      " 15739576.60931818 15745687.85059275 15751808.50871855 15757938.56398344\n",
      " 15764077.99646842 15770226.78605256 15776384.91241777 15782552.3550536\n",
      " 15788729.09326189 15794915.1061613  15801110.37269186 15807314.87161933\n",
      " 15813528.58153951 15819751.48088251 15825983.54791692 15832224.76075385\n",
      " 15838475.09735098 15844734.53551647 15851003.05291285 15857280.62706077\n",
      " 15863567.23534275 15869862.85500686 15876167.46317021 15882481.03682259\n",
      " 15888803.55282981 15895134.98793716 15901475.31877273 15907824.52185064\n",
      " 15914182.57357428 15920549.45023943 15926925.12803737 15933309.58305789\n",
      " 15939702.79129227 15946104.7286362  15952515.37089266 15958934.69377472\n",
      " 15965362.67290825 15971799.28383473 15978244.50201384 15984698.30282607\n",
      " 15991160.6615753  15997631.55349132 16004110.95373227 16010598.83738708\n",
      " 16017095.17947784 16023599.95496213 16030113.13873531 16036634.70563279\n",
      " 16043164.63043217 16049702.88785549 16056249.45257126 16062804.29919663\n",
      " 16069367.40229938 16075938.73639992 16082518.27597331 16089105.99545116\n",
      " 16095701.86922348 16102305.87164066 16108917.97701516 16115538.15962339\n",
      " 16122166.39370744 16128802.6534768  16135446.91311005 16142099.14675654\n",
      " 16148759.32853801 16155427.43255016 16162103.43286427 16168787.30352868\n",
      " 16175479.01857036 16182178.55199633 16188885.87779517 16195600.9699384\n",
      " 16202323.80238191 16209054.34906728 16215792.58392322 16222538.48086677\n",
      " 16229292.0138047  16236053.15663471 16242821.8832467  16249598.16752402\n",
      " 16256381.98334459 16263173.30458216 16269972.10510742 16276778.35878911\n",
      " 16283592.03949519 16290413.12109387 16297241.57745467 16304077.38244955\n",
      " 16310920.50995381 16317770.9338472  16324628.62801484 16331493.56634822\n",
      " 16338365.72274613 16345245.0711156  16352131.58537278 16359025.23944389\n",
      " 16365926.00726603 16372833.86278807 16379748.77997148 16386670.73279118\n",
      " 16393599.69523629 16400535.64131099 16407478.54503524 16414428.38044555\n",
      " 16421385.12159574 16428348.74255769 16435319.21742197 16442296.52029864\n",
      " 16449280.62531786 16456271.50663061 16463269.13840933 16470273.49484855\n",
      " 16477284.55016553 16484302.27860091 16491326.65441927 16498357.65190972\n",
      " 16505395.24538652 16512439.40918965 16519490.11768529 16526547.34526646\n",
      " 16533611.06635351 16540681.25539462 16547757.88686636 16554840.93527414\n",
      " 16561930.37515273 16569026.18106675 16576128.3276111  16583236.78941145\n",
      " 16590351.54112465 16597472.55743924 16604599.81307579 16611733.28278739\n",
      " 16618872.94136002 16626018.76361298 16633170.72439923 16640328.79860587\n",
      " 16647492.96115439 16654663.18700115 16661839.45113766 16669021.72859096\n",
      " 16676209.99442398 16683404.22373584 16690604.3916622  16697810.47337554\n",
      " 16705022.44408554 16712240.27903932 16719463.95352176 16726693.44285581\n",
      " 16733928.72240272 16741169.76756237 16748416.55377352 16755669.05651406\n",
      " 16762927.25130127 16770191.11369209 16777460.61928334 16784735.74371198\n",
      " 16792016.46265529 16799302.75183117 16806594.58699831 16813891.94395642\n",
      " 16821194.79854641 16828503.12665065 16835816.90419311 16843136.1071396\n",
      " 16850460.71149791 16857790.69331803 16865126.0286923  16872466.69375561\n",
      " 16879812.66468553 16887163.91770251 16894520.42907003 16901882.17509471\n",
      " 16909249.13212651 16916621.27655887 16923998.58482882 16931381.03341713\n",
      " 16938768.59884845 16946161.25769144 16953558.98655885 16960961.76210773\n",
      " 16968369.56103942 16975782.36009978 16983200.13607923 16990622.86581288\n",
      " 16998050.5261806  17005483.09410717 17012920.54656233 17020362.86056086\n",
      " 17027810.01316274 17035261.98147313 17042718.74264254 17050180.27386687\n",
      " 17057646.55238746 17065117.55549122 17072593.26051062 17080073.64482383\n",
      " 17087558.68585475 17095048.36107306 17102542.64799429 17110041.52417988\n",
      " 17117544.9672372  17125052.95481963 17132565.46462661 17140082.47440366\n",
      " 17147603.96194241 17155129.90508069 17162660.28170251 17170195.06973813\n",
      " 17177734.24716408 17185277.7920032  17192825.68232461 17200377.89624384\n",
      " 17207934.41192275 17215495.2075696  17223060.26143909 17230629.55183231\n",
      " 17238203.05709682 17245780.75562662 17253362.62586217 17260948.64629043\n",
      " 17268538.79544481 17276133.05190523 17283731.39429809 17291333.80129627\n",
      " 17298940.25161916 17306550.72403263 17314165.19734904 17321783.65042721\n",
      " 17329406.06217247 17337032.41153658 17344662.67751778 17352296.83916074\n",
      " 17359934.87555657 17367576.7658428  17375222.48920336 17382872.02486857\n",
      " 17390525.35211511 17398182.45026601 17405843.29869062 17413507.87680463\n",
      " 17421176.16406997 17428848.13999485 17436523.78413372 17444203.0760872\n",
      " 17451885.99550213 17459572.52207147 17467262.63553432 17474956.31567584\n",
      " 17482653.54232727 17490354.29536587 17498058.55471485 17505766.30034343\n",
      " 17513477.5122667  17521192.17054564 17528910.25528708 17536631.74664365\n",
      " 17544356.62481372 17552084.8700414  17559816.46261647 17567551.38287436\n",
      " 17575289.61119606 17583031.12800814 17590775.91378266 17598523.94903712\n",
      " 17606275.21433447 17614029.69028298 17621787.35753627 17629548.1967932\n",
      " 17637312.18879787 17645079.31433952 17652849.55425253 17660622.88941634\n",
      " 17668399.30075539 17676178.76923909 17683961.27588175 17691746.80174255\n",
      " 17699535.32792545 17707326.83557916 17715121.30589708 17722918.72011723\n",
      " 17730719.05952222 17738522.30543917 17746328.43923966 17754137.44233967\n",
      " 17761949.29619955 17769763.98232388 17777581.48226152 17785401.77760547\n",
      " 17793224.84999282 17801050.68110472 17808879.2526663  17816710.54644663\n",
      " 17824544.5442586  17832381.22795892 17840220.57944803 17848062.58067003\n",
      " 17855907.21361263 17863754.46030709 17871604.30282814 17879456.72329393\n",
      " 17887311.70386594 17895169.22674895 17903029.27419095 17910891.82848309\n",
      " 17918756.87195959 17926624.3869977  17934494.35601761 17942366.76148241\n",
      " 17950241.58589799 17958118.81181301 17965998.42181879 17973880.39854928\n",
      " 17981764.72468097 17989651.38293283 17997540.35606622 18005431.62688487\n",
      " 18013325.17823476 18021220.99300407 18029119.05412312 18037019.34456429\n",
      " 18044921.84734195 18052826.54551239 18060733.42217375 18068642.46046596\n",
      " 18076553.64357065 18084466.95471109 18092382.37715213 18100299.8942001\n",
      " 18108219.48920277 18116141.14554926 18124064.84666996 18131990.57603652\n",
      " 18139918.31716167 18147848.05359923 18155779.76894405 18163713.44683185\n",
      " 18171649.07093924 18179586.6249836  18187526.092723   18195467.45795619\n",
      " 18203410.70452243 18211355.8163015  18219302.77721361 18227251.57121929\n",
      " 18235202.18231934 18243154.59455478 18251108.79200673 18259064.75879642\n",
      " 18267022.47908498 18274981.9370735  18282943.1170029  18290906.00315385\n",
      " 18298870.57984671 18306836.83144145 18314804.7423376  18322774.29697414\n",
      " 18330745.47982946 18338718.27542124 18346692.66830645 18354668.64308121\n",
      " 18362646.18438075 18370625.27687931 18378605.90529012 18386588.05436524\n",
      " 18394571.7088956  18402556.8537108  18410543.47367916 18418531.55370754\n",
      " 18426521.07874134 18434512.03376439 18442504.40379889 18450498.17390533\n",
      " 18458493.32918242 18466489.85476704 18474487.7358341  18482486.95759654\n",
      " 18490487.50530522 18498489.36424885 18506492.51975392 18514496.95718464\n",
      " 18522502.66194284 18530509.6194679  18538517.81523671 18546527.23476357\n",
      " 18554537.8636001  18562549.68733522 18570562.69159501 18578576.8620427\n",
      " 18586592.18437856 18594608.64433984 18602626.22770068 18610644.92027206\n",
      " 18618664.70790173 18626685.5764741  18634707.5119102  18642730.50016762\n",
      " 18650754.5272404  18658779.57915897 18666805.6419901  18674832.70183679\n",
      " 18682860.74483825 18690889.75716976 18698919.72504266 18706950.63470423\n",
      " 18714982.47243766 18723015.22456195 18731048.87743184 18739083.41743775\n",
      " 18747118.83100571 18755155.10459728 18763192.22470945 18771230.17787464\n",
      " 18779268.95066056 18787308.52967018 18795348.90154162 18803390.05294813\n",
      " 18811431.97059798 18819474.64123441 18827518.05163551 18835562.18861426\n",
      " 18843607.03901832 18851652.58973006 18859698.82766646 18867745.73977901\n",
      " 18875793.3130537  18883841.53451089 18891890.39120527 18899939.87022579\n",
      " 18907989.95869559 18916040.64377192 18924091.91264605 18932143.75254326\n",
      " 18940196.15072273 18948249.09447746 18956302.57113422 18964356.5680535\n",
      " 18972411.07262937 18980466.0722895  18988521.55449504 18996577.50674054\n",
      " 19004633.91655393 19012690.7714964  19020748.05916236 19028805.76717936\n",
      " 19036863.88320804 19044922.39494203 19052981.29010791 19061040.55646513\n",
      " 19069100.18180593 19077160.1539553  19085220.46077088 19093281.09014294\n",
      " 19101342.02999423 19109403.26828002 19117464.79298794 19125526.59213793\n",
      " 19133588.65378225 19141650.9660053  19149713.51692363 19157776.29468584\n",
      " 19165839.28747253 19173902.48349622 19181965.87100128 19190029.4382639\n",
      " 19198093.17359195 19206157.065325   19214221.1018342  19222285.27152221\n",
      " 19230349.56282317 19238413.96420259 19246478.46415736 19254543.05121557\n",
      " 19262607.71393653 19270672.4409107  19278737.22075958 19286802.04213568\n",
      " 19294866.89372244 19302931.76423418 19310996.64241601 19319061.51704379\n",
      " 19327126.37692405 19335191.21089394 19343256.00782115 19351320.75660382\n",
      " 19359385.44617056 19367450.0654803  19375514.60352225 19383579.04931588\n",
      " 19391643.39191079 19399707.62038668 19407771.72385327 19415835.69145029\n",
      " 19423899.51234733 19431963.17574385 19440026.67086908 19448089.98698195\n",
      " 19456153.11337107 19464216.03935461 19472278.75428029 19480341.24752528\n",
      " 19488403.50849615 19496465.52662882 19504527.29138847 19512588.7922695\n",
      " 19520650.01879546 19528710.960519   19536771.60702178 19544831.94791443\n",
      " 19552891.97283649 19560951.67145633 19569011.03347112 19577070.04860671\n",
      " 19585128.70661767 19593186.99728709 19601244.91042664 19609302.43587648\n",
      " 19617359.56350514 19625416.28320952 19633472.58491481 19641528.45857445\n",
      " 19649583.89417002 19657638.88171124 19665693.41123584 19673747.47280958\n",
      " 19681801.05652612 19689854.15250702 19697906.75090161 19705958.84188701\n",
      " 19714010.415668   19722061.46247701 19730111.97257404 19738161.93624658\n",
      " 19746211.34380962 19754260.18560547 19762308.45200387 19770356.13340176\n",
      " 19778403.22022333 19786449.70291992 19794495.57196997 19802540.81787899\n",
      " 19810585.43117942 19818629.40243067 19826672.72221901 19834715.38115751\n",
      " 19842757.36988598 19850798.67907096 19858839.29940559 19866879.22160963\n",
      " 19874918.43642931 19882956.93463736 19890994.70703291 19899031.74444143\n",
      " 19907068.03771471 19915103.57773075 19923138.35539372 19931172.36163395\n",
      " 19939205.5874078  19947238.02369766 19955269.66151187 19963300.49188467\n",
      " 19971330.50587613 19979359.69457211 19987388.0490842  19995415.56054967\n",
      " 20003442.22013139 20011468.01901782 20019492.94842289 20027516.99958601\n",
      " 20035540.16377197 20043562.43227093 20051583.79639827 20059604.24749467\n",
      " 20067623.77692595 20075642.37608308 20083660.03638205 20091676.74926389\n",
      " 20099692.50619458 20107707.29866503 20115721.11819096 20123733.95631288\n",
      " 20131745.8045961  20139756.65463056 20147766.49803084 20155775.32643612\n",
      " 20163783.13151009 20171789.90494092 20179795.6384412  20187800.32374787\n",
      " 20195803.9526222  20203806.5168497  20211808.00824011 20219808.41862731\n",
      " 20227807.73986927 20235805.96384802 20243803.0824696  20251799.08766395\n",
      " 20259793.97138495 20267787.72561027 20275780.34234142 20283771.81360358\n",
      " 20291762.13144565 20299751.28794017 20307739.27518324 20315726.08529449\n",
      " 20323711.71041701 20331696.14271733 20339679.37438536 20347661.3976343\n",
      " 20355642.20470066 20363621.78784413 20371600.13934759 20379577.25151704\n",
      " 20387553.11668153 20395527.72719312 20403501.07542686 20411473.15378069\n",
      " 20419443.95467544 20427413.47055473 20435381.69388495 20443348.61715519]\n"
     ]
    }
   ],
   "source": [
    "model = LinReg(gd_type='Momentum')\n",
    "model.fit(X_train, y_train)\n",
    "y_train_predicted = model.predict(X_train)\n",
    "y_test_predicted = model.predict(X_test)\n",
    "print(\"Momentum: \")\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_test, y_test_predicted, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_test, y_test_predicted))\n",
    "print(model.loss_history)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# model = LinReg(gd_type='Stochastic')\n",
    "# model.fit(X_train, y_train)\n",
    "# y_train_predicted = model.predict(X_train)\n",
    "# y_test_predicted = model.predict(X_test)\n",
    "# print(\"Stochastic: \")\n",
    "# print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "# print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "# print(\"Train RMSE = %.4f\" % mean_squared_error(y_test, y_test_predicted, squared=False))\n",
    "# print(\"Train R2 = %.4f\" % r2_score(y_test, y_test_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# model = LinReg(gd_type='Adagrad')\n",
    "# model.fit(X_train, y_train)\n",
    "# y_train_predicted = model.predict(X_train)\n",
    "# y_test_predicted = model.predict(X_test)\n",
    "# print(\"Adagrad: \")\n",
    "# print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "# print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "# print(\"Train RMSE = %.4f\" % mean_squared_error(y_test, y_test_predicted, squared=False))\n",
    "# print(\"Train R2 = %.4f\" % r2_score(y_test, y_test_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = Ridge()\n",
    "model.fit(X_train, y_train)\n",
    "y_train_predicted = model.predict(X_train)\n",
    "y_test_predicted = model.predict(X_test)\n",
    "print(\"Ridge from Sklearn: \")\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_predicted, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_predicted))\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_test, y_test_predicted, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_test, y_test_predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 8. [1 points] Plot graphs (on the same picture) of the dependence of the loss function value on the iteration number for Full GD, SGD, Momentum and Adagrad. Draw conclusions about the rate of convergence of various modifications of gradient descent.\n",
    "\n",
    "Don't forget about what *beautiful* graphics should look like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# your code here \n",
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}